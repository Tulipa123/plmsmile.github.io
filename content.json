[{"title":"剑指offer算法题(11-20)","date":"2017-12-29T03:57:48.000Z","path":"2017/12/29/aim2offer2/","text":"剑指offer算法题(03-10) 旋转数组中的最小值-11 排序算法 我的排序算法总结 旋转数组最小值 原有序数组：1,2,3,4,5,6,7，旋转一下，把一部分放到后面去：4,5,6,7, 1,2,3。 求：在\\(O(logn)\\)内找到数组中最小的元素 leetcode旋转数组 思路 特点：左边序列全部大于等于右边序列。 最小的值在中间，顺序查找肯定不行，那就只能二分查找了。 设两个指针，left从左边向中间靠拢，right从右边向中间靠拢。 循环条件：a[l] &gt;= a[r] 直到l+1==r 为止，那么a[r]就是我们要的最小值。 计算中间值 a[m] a[m] &gt;= a[l] ：m在左边序列，l = m， l向右走 a[m] &lt;= a[r] ： m在右边序列，r == m， r向左走 陷阱 可能一个数字都不旋转，即a为有序序列，直接返回a[0] 有重复的数字，a[l]==a[m] &amp;&amp; a[m]==[r]， 则只能顺序查找了 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * 查找旋转数组中的最小值。两个指针指向前后两个递增序列，向中间靠拢 */int minum_rotate_array(const vector&lt;int&gt;&amp; a) &#123; if (a.size() == 0) &#123; return 0; &#125; int l = 0; int r = a.size() - 1; // 特殊情况，旋转0个，原数组，小于不能等于 if (a[l] &lt; a[r]) &#123; return a[l]; &#125; int res = -1; while (a[l] &gt;= a[r]) &#123; // 两个指针已经相邻 if (l + 1 == r) &#123; res = a[r]; break; &#125; // 中间指针 int m = (l + r) / 2; // 三个数相等，无法确定中间数在前后那个序列 if (a[l] == a[m] &amp;&amp; a[m] == a[r]) &#123; res = get_min(a, l, r); break; &#125; if (a[m] &gt;= a[l]) &#123; l = m; &#125; else if (a[m] &lt;= a[r]) &#123; r = m; &#125; &#125; return res;&#125;int get_min(const vector&lt;int&gt;&amp;a, int start, int end) &#123; int min = a[start]; for (int i = start + 1; i &lt;= end; i++) if (a[i] &lt; min) &#123; min = a[i]; &#125; return min;&#125;","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"}]},{"title":"排序算法总结","date":"2017-12-26T08:06:53.000Z","path":"2017/12/26/sort-algorithms/","text":"插入排序 直接插入 前面的已经有序，把后面的插入到前面有序的元素中。 步骤 找到待插入位置 给插入元素腾出空间，边比较边移动 如对4 5 1 2 6 3 12345678910# 5插入44 5 1 2 6 3# 1插入 4 51 4 5 2 6 3# 2插入 1 4 51 2 4 5 6 3# 6插入 1 2 4 51 2 4 5 6 3# 3插入到前面1 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 直接插入排序，先比较找位置，再移动 **/ void insert_sort(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; // 最大，追加在末尾即可 if (a[i] &gt; a[i-1]) &#123; continue; &#125; // 找到待插入的位置 int k = -1; for (int j = 0; j &lt; i; j++) &#123; if (a[i] &lt; a[j]) &#123; k = j; break; &#125; &#125; int t = a[i]; // 先挪动元素，向后移动 for (int j = i; j &gt; k; j--) &#123; a[j] = a[j-1]; &#125; a[k] = t; &#125;&#125;/** * 直接插入排序，边比较边移动 **/void insert_sort2(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; if (a[i] &lt; a[i-1]) &#123; int t = a[i]; int j = i - 1; while (a[j] &gt; t &amp;&amp; j &gt;= 0) &#123; a[j+1] = a[j]; j--; &#125; a[j+1] = t; &#125; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 适用性 直接插入 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 顺序存储和链式存储的线性表 折半插入 先折半查找出位置，再统一的移动。 若m为1个数字，则\\(a[i]&gt;a[m]\\)，该插入位置为\\(l=m+1\\)。 仅仅减少了元素的比较次数，元素的移动次数依然没有改变。时间复杂度仍然为\\(\\mathrm{O}(n)\\)。 关键代码 123456789101112131415161718192021222324252627282930/** * 插入排序，折半查找出位置，再统一移动 **/ void insert_sort_bisearch(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; if (a[i] &gt; a[i-1]) &#123; continue; &#125; // 折半查找，a[i]要插入的位置为l int l = 0, r = i - 1; while (l &lt;= r) &#123; int m = (l + r) / 2; if (a[i] &gt; a[m]) &#123; // 查找右边 l = m + 1; &#125; else if (a[i] &lt; a[m])&#123; // 查找左边 r = m - 1; &#125; else &#123; l = m + 1; break; &#125; &#125; int t = a[i]; for (int j = i; j &gt; l; j--) &#123; a[j] = a[j-1]; &#125; a[l] = t; &#125;&#125; 希尔排序 希尔排序又称为缩小增量排序， 把整个列表，分成多个\\(\\rm{L[i, i+d,i+2d,\\cdots, i+kd]}\\)这样的列表，每个进行直接插入排序。每一轮不断缩小d的值，直到全部有序。 实际例子 4 5 1 2 6 3 1234567891011121314151617# d = 34 _ _ 2 _ __ 5 _ _ 6 __ _ 1 _ _ 3# 化为3个列表，分别进行直接插入排序，得到2 5 1 4 6 3# d = 22 _ 1 _ 6 __ 5 _ 4 _ 3# 排序，得到1 3 2 4 6 5# d = 11 3 2 4 6 5# 直接插入排序，得到1 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031323334/* * 希尔排序，按照步长，去划分为多个组。对这些组分别进行插入排序 */void shell_sort(vector&lt;int&gt;&amp; a) &#123; // 步长gap==组的个数 for (int gap = a.size() / 2; gap &gt; 0; gap = gap / 2) &#123; // 对各个组进行排序 for (int i = 0; i &lt; gap; i++) &#123; group_sort(a, i, gap); &#125; &#125;&#125;/* * 对希尔排序中的单个组进行排序，直接插入 * Args: * a -- 数组 * start -- 该组的起始地址 * gap -- 组的步长，也是组的个数 */void group_sort(vector&lt;int&gt; &amp;a, int start, int gap) &#123; for (int i = start + gap; i &lt; a.size(); i += gap) &#123; if (a[i] &lt; a[i - gap]) &#123; int t = a[i]; int j = i - gap; // 从后向前比较，边比较，边移动 while (a[j] &gt; t &amp;&amp; j &gt;= start) &#123; a[j + gap] = a[j]; j -= gap; &#125; a[j + gap] = t; &#125; &#125;&#125; 总结分析 最好的增量\\(d_1 = \\frac{n}{2}, d_{i+1} = \\lfloor \\frac{d_i}{2}\\rfloor\\) 名称 时间 最好 最差 空间 稳定 适用性 希尔排序 \\({O}(1)\\) 不稳定 线性存储的线性表 交换排序 冒泡排序 执行n-1轮，每一轮把\\(a[0, \\ldots, i]\\)的最大的向下沉。 123456789101112131415162 4 3 1 6 5# 第一趟2 4 _ _ _ _2 3 4 _ _ _ # 4-3 to 3-42 3 1 4 _ _ # 4-1 to 1-4 2 3 1 4 6 _ 2 3 1 4 5 6 # 6-5 to 5-6# 第二趟2 3 _ _ _ 6 2 1 3 _ _ 6 # 3-1 to 1-32 1 3 4 _ 6 2 1 3 4 5 6# 第三趟1 2 _ _ 5 6 # 2-1 to 1-21 2 3 _ 5 61 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031void bubble_sort1(int* a, int n) &#123; for (int i = n-1; i &gt; 0; i--) &#123; // 每一轮把a[0,...,i]中最大的向下沉 for (int j = 0; j &lt; i; j++) &#123; if (a[j] &gt; a[j+1]) &#123; int t = a[j]; a[j] = a[j+1]; a[j+1] = t; &#125; &#125; &#125; &#125;// 若当前轮，已经没有发生交换，说明已经全部有序void bubble_sort2(int* a, int n) &#123; int swapped = 0; for (int i = n - 1; i &gt; 0; i--) &#123; swapped = 0; for (int j = 0; j &lt; i; j++) &#123; if (a[j] &gt; a[j+1]) &#123; int t = a[j]; a[j] = a[j+1]; a[j+1] = t; swapped = 1; &#125; &#125; if (swapped = 0) &#123; break; &#125; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 冒泡排序 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 每一轮都有一个元素到最终位置上 快速排序 思想 把一个序列，选一个数（第一个数），进行划分。左边小于x，中间x，右边大于x。再依次递归划分左右两边。 关键代码 划分 123456789101112131415161718192021222324252627282930313233343536/** * 划分，左边小于x，中间x，右边大于x * Args: * a: * l: * r: * Returns: * i: x=a[l]的最终所在位置 **/int partition(int* a, int l, int r) &#123; int x = a[l]; int i = l; int j = r; // 划分 while (i &lt; j) &#123; // 从右到左，找到第一个小于x的a[j]，放到a[i]上 while (a[j] &gt;= x &amp;&amp; j &gt; i) &#123; j--; &#125; // 把a[j]放到左边i上 if (j &gt; i) &#123; a[i++] = a[j]; &#125; // 从左到右，找到一个大于x的[i] while (a[i] &lt;= x &amp;&amp; i &lt; j) &#123; i++; &#125; // 把a[i]放到右边j上 if (i &lt; j) &#123; a[j--] = a[i]; &#125; &#125; // x放在中间 a[i] = x; return i;&#125; 递归快排 1234567891011void quick_sort(int* a, int l, int r) &#123; // 1. 递归终止 if (l &gt;= r) &#123; return; &#125; // 2. 划分，左边小于x，中间x，右边大于x int k = partition(a, l, r); // 3. 递归快排左右两边 quick_sort(a, l, k - 1); quick_sort(a, k + 1, r);&#125; 非递归快排 123456789101112131415161718192021222324252627void quick_sort_stack(int* a, int l, int r) &#123; int i, j; stack&lt;int&gt; st; // 注意进栈和出栈的顺序 st.push(r); st.push(l); while (st.empty() == false) &#123; // 每次出栈一组 i = st.top(); st.pop(); j = st.top(); st.pop(); if (i &lt; j) &#123; int k = partition(a, i, j); // 左边的 if (k &gt; i) &#123; st.push(k - 1); st.push(i); &#125; // 右边的 if (k &lt; j) &#123; st.push(j); st.push(k + 1); &#125; &#125; &#125;&#125; 总结分析 第i趟完成后，最少有i个元素在最终位置。 名称 时间 最好 最差 空间 稳定 备注 快排 \\({O}(n\\log n)\\) \\(O(n\\log n)\\) \\({O}(n^2)\\) \\({O}(\\log_2n)\\)栈的深度 不稳定 基本有序或者逆序，效果最差 选择排序 简单选择 前面已经有序，从后面选择最小的与前面末尾最大的进行交换。 12345678910114 5 1 2 6 3# 选择1与4交换1 5 4 2 6 3# 选择2与5交换1 2 4 5 6 3# 选择3与4交换1 2 3 5 6 4# 选择4与5交换1 2 3 4 6 5# 选择5与6交换1 2 3 4 5 6 总结分析 名称 时间 最好 最差 空间 稳定 备注 简单选择 \\({O}(n^2)\\) \\({O}(n^2)\\) \\({O}(n^2)\\) \\(O(1)\\) 不稳定 关键代码 1234567891011/** * 简单选择排序，选择后面最小的来与当前有序的最后一个（最大的）交换 **/void select_sort(int *a, int n) &#123; for (int i = 1; i &lt; n; i++) &#123; int k = min_index(a, i, n-1); if (a[i-1] &gt; a[k]) &#123; swap(a, i-1, k); &#125; &#125;&#125; 堆 堆 堆是一颗完全二叉树。 有n个节点，堆的索引从0开始，节点i的 左孩子：\\(2\\cdot i+1\\) 右孩子：\\(2\\cdot i +2\\) 父亲：\\(\\lceil \\frac{i-1}{2}\\rceil\\) 最后一个节点是：下标为\\(\\lfloor n/2\\rfloor-1\\) 的节点的孩子，即第\\(\\lfloor n/2\\rfloor\\)个节点的孩子。 大根堆 大根堆：最大元素在根节点。小根堆：最小元素在根节点。 90 70 80 60 10 40 50 30 20 是一个大根堆，10 20 70 30 50 90 80 60 40 是一个小根堆，如下 堆添加 先把元素添加到末尾，再依次向上面调整，若大于父节点，就和父节点交换。 堆删除 删除堆顶元素：把末尾元素，放到堆顶，再依次向下调整。 删除堆中元素：把末尾元素，放入空白处，再调整堆即可。 堆排序 思路 初始化堆。把\\(a_1, \\cdots, a_n\\)构造成为最大堆 取出最大值。每次出堆根最大元素。出\\(a_1\\)，把\\(a_n\\)放到\\(a_1\\)上，再把\\(a_1, \\cdots. a_{n-1}\\)调整为最大堆。再出元素 重复2 建立堆 n个元素，最后一个父亲节点\\(\\lfloor n/2\\rfloor\\)， 下标为\\(k=\\lfloor n/2\\rfloor-1\\)。 对这些节点\\(a_k, \\cdots, a_0\\)， 依次调整它们的子树，使子树成堆。即，若根节点小于左右节点的较大值，则交换。 建立堆实例 对于数据{20,30,90,40,70,110,60,10,100,50,80}，建立为最大堆{110,100,90,40,80,20,60,10,30,50,70} 取出最大值 把根节点（最大值）和当前堆的末尾值进行交换，最大值放到最后。再对剩余的数据进行成堆，再依次取最大值交换。 每一次取出最大值重新恢复堆，要\\(O(\\log n)\\)，有n个数，一共是\\(O(n \\log n)\\)。 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 保证以start为根节点的子树是一个最大堆，末尾元素为end * * Args: * a: 存放堆的数组 * start: 根节点 * end: 子树的末尾元素 * Returns: * None **/ void max_heap_down(vector&lt;int&gt;&amp; a, int start, int end) &#123; // 当前节点 int c = start; // 左孩子 int l = 2 * c + 1; int t = a[c]; for (; l &lt;= end; c = l, l = 2*c + 1) &#123; // 选择较大的孩子 if (l + 1 &lt;= end &amp;&amp; a[l] &lt; a[l + 1]) &#123; // 选择右孩子 l++; &#125; if (t &gt;= a[l]) &#123; // 父亲大于孩子 break; &#125; else &#123; // 交换 a[c] = a[l]; a[l] = t; &#125; &#125;&#125;/** * 堆排序，升序 **/void heap_sort_asc(vector&lt;int&gt;&amp; a) &#123; int n = a.size(); // 初始化一个最大堆 for (int i = n / 2 - 1; i &gt;= 0; i--) &#123; max_heap_down(a, i, n - 1); &#125; cout &lt;&lt; \"inited\" &lt;&lt; endl; // 依次取堆顶元素放到末尾 for (int i = n - 1; i &gt;= 0; i--) &#123; // max放到a[i] int t = a[i]; a[i] = a[0]; a[0] = t; // 保证a[0...i-1]依然是个最大堆 max_heap_down(a, 0, i-1); &#125; return;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 堆排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}( \\log n)\\) \\(O(n)\\) 不稳定 归并排序 从上往下-递归 思路 分解 -- 将区间一分为二，求分裂点\\(\\rm{mid} = \\frac{\\rm{start +end}}{2}\\) 递归求解 -- 递归对两个子区间 \\(a_s,\\cdots , a_m\\)和\\(a_{m+1}, \\cdots, a_{e}\\)进行归并排序。终结条件是子区间长度为1 合并 -- 把两个有序的子区间 \\(a_s,\\cdots , a_m\\)和\\(a_{m+1}, \\cdots, a_{e}\\) 合并为一个完整的有序区间\\(a_s, \\cdots, a_e\\) 分解 分解&amp;递归&amp;合并 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 归并排序，从上到下 **/void merge_sort_up2down(vector&lt;int&gt; &amp;a, int start, int end) &#123; if (start &gt;= end) &#123; return; &#125; int mid = (start + end) / 2; // 递归排序a[start...mid] merge_sort_up2down(a, start, mid); // 递归排序a[mid+1...end] merge_sort_up2down(a, mid + 1, end); // 两个有序序列merge在一起 merge(a, start, mid, end);&#125;/** * 将a中前后两个有序序列合并在一起 **/ void merge(vector&lt;int&gt; &amp;a, int start, int mid, int end) &#123; // 把有序序列临时存放到t中 int * t = new int [end - start + 1]; int i = start; int j = mid + 1; int k = 0; // 依次合并 while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; if (a[i] &lt; a[j]) &#123; t[k++] = a[i++]; &#125; else &#123; t[k++] = a[j++]; &#125; &#125; while (i &lt;= mid) &#123; t[k++] = a[i++]; &#125; while (j &lt;= end) &#123; t[k++] = a[j++]; &#125; // 把新的有序列表复制回a中 for (int i = 0; i &lt; k; i++) &#123; a[start + i] = t[i]; &#125; delete [] t;&#125; 从下往上-非递归 思想 把数组分成若干个长度为1的子数组，再两两合并；得到长度为2的数组，再两两合并；依次反复，直到形成一个数组。 关键代码 123456789101112131415161718192021222324252627282930/** * 归并排序，从下到上 **/void merge_sort_down2up(vector&lt;int&gt; &amp;a) &#123; if (a.size() &lt;= 0) return; for (int i = 1; i &lt; a.size(); i = i * 2) merge_groups(a, i);&#125;/* * 对a做若干次合并，分为若干个gap。对每相邻的两个gap进行合并排序 * Args: * a: 数组 * gap: 一个子数组的长度 */void merge_groups(vector&lt;int&gt; &amp;a, int gap) &#123; int twolen = 2 * gap; int i; for (i = 0; i + twolen - 1 &lt; a.size(); i += twolen) &#123; int start = i; int mid = i + gap - 1; int end = i + twolen - 1; merge(a, start, mid, end); &#125; // 最后还有一个gap if (i + gap - 1 &lt; a.size() - 1) &#123; merge(a, i, i + gap - 1, a.size() - 1); &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 归并排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}(n \\log n)\\) \\(O(n)\\)， merge占用 稳定 归并排序的形式是一颗二叉树，遍历的次数就是二叉树的深度\\(O(\\log n)\\)， 而一共n个数。 桶排序 桶排序很简单。数组a有n个数，最大值为max。则，建立一个长度为max的数组b，初始化为0。 遍历a，遇到\\(a_i = k\\)， 则\\(b_k += 1\\) 。即在对应的桶里计数加1。 基数排序 基数排序分为最高位优先和最低位优先。 基数排序是桶排序的扩展。把所有的数，统一位数。然后，按照每一位进行，从低位到高位跑排序。 关键是找到output和buckets的对应关系。每个bucket存储前面累积的元素的数量。 123456buckets[2] = 1;// 说明排序数是2的元素有1个buckets[3] = 4;// 说明排序数是3的元素有 4-1=3个buckets[4] = 7;// 说明排序数是4的元素有 7-4=3个 后面在进行根据排序数找到当前数的最终所在位置的时候，就会利用这个关系。 1234567// 比如排序数是3的数字，会出现3个// 则id就为 buckets[3]-1// 每出现一个，则buckets[3]--// 举个例子// 初始buckets[2]=1，则这1个数字的最终序号是：0// 初始buckets[3]=4，则这3个数字的最终序号是：3,2,1 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/* * 基数排序 */void radix_sort(vector&lt;int&gt; &amp;a) &#123; if (a.size() &lt;= 1) &#123; return; &#125; int max = *max_element(a.begin(), a.end()); // exp=1, 10, 100, 1000... for (int exp = 1; max / exp &gt; 0; exp *= 10) &#123; count_sort(a, exp); &#125;&#125;/* * 对数组按照某个位数进行排序 * Args: * a -- 数组 * exp -- 指数，1, 10, 100... 分别按照个位、十位、百位排序 * Returns: * None */void count_sort(vector&lt;int&gt;&amp; a, int exp) &#123; // 存储被排序数据的临时数组 int output [a.size()]; // 桶 数据的出现次数 int buckets[10] = &#123;0&#125;; for (int i = 0; i &lt; a.size(); i++) &#123; int t = (a[i] / exp) % 10; buckets[t]++; &#125; // 根据前面的出现次数，推算出当前数字在原数组中的index for (int i = 1; i &lt; 10; i++) buckets[i] += buckets[i - 1]; // 将数据存储到output中 for (int i = a.size() - 1; i &gt;= 0; i--) &#123; int j = (a[i] / exp) % 10; int k = buckets[j]; output[k - 1] = a[i]; buckets[j]--; &#125; // 赋值给a for (int i = 0; i &lt; a.size(); i++) &#123; a[i] = output[i]; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 基数排序 \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\(O(r)\\)， r个队列 稳定 总结比较 总结 思想总结 名称 一句话描述 直接插入 前面有序，为新来的，在前面找到合适的位置，进行插入 折半插入 前面有序，为新来的，使用折半查找到插入位置，进行插入 希尔排序 gap个间隔为gap的子序列，每个进行直接插入排序；减小gap，依次排序，直至为1 冒泡排序 交换n-1趟，\\(a_{i-1}&gt;a_i\\)，则进行交换，每一趟都有个最大的沉到末尾 快排 第一个数x，先划分，左边小于x，中间x，右边大于x。再依次递归排序左右两边 简单选择 前面有序，从后面选择最小的与前面末尾的（最大的）进行交换 堆排序 初始化大根堆，堆顶和末尾元素交换，再调整使剩下的元素成堆， 重复 归并排序 分解为左右两个序列，对左右两个序列进行递归归并排序，再合并。即sort,sort,merge 基数排序 位数一样，从低位到高位，分别按照每一位进行排序 时空复杂度总结 名称 时间 最好 最差 空间 稳定 直插 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 希尔 \\({O}(1)\\) 不稳定 冒泡 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 快排 \\({O}(n\\log n)\\) \\(O(n\\log n)\\) \\({O}(n^2)\\) \\({O}(\\log_2n)\\)，栈的深度 不稳定 简选 \\({O}(n^2)\\) \\({O}(n^2)\\) \\({O}(n^2)\\) \\(O(1)\\) 不稳定 堆排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}( \\log n)\\) \\(O(n)\\) 不稳定 归并排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}(n \\log n)\\) \\(O(n)\\)， merge占用 稳定 基数排序 \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\(O(r)\\)， r个队列 稳定 稳定的：插、冒、归、基 较快的：快、些、归、堆，插得好、冒得好 比较次数与初始状态无关：选择、归并 排序趟数与初始状态无关：选择、插入、基数 冒、选、堆：每趟都有1个在最终的位置，最大or最小 直接插入：第\\(i\\)趟，前\\(i+1\\)个有序，但不是最终序列 快排： \\(i\\)趟后， 至少有\\(i\\)个在最终位置 时间复杂度：快些归堆\\({O(n\\log n)}\\)，基数排序\\(O(d(n+r))\\) ， 其余\\(O(n^2)\\) ， 空间复杂度：归\\(O(n)\\)， 快\\(O(\\log n)\\)， 基\\(O(r)\\)， 其余\\(O(1)\\) 算法选择 条件 可选算法 n较小， \\(n \\le 50\\) 直接插入、简单选择 基本有序 直接插入、冒泡 n较大，要\\(O(n \\log n)\\) 快排、堆排（不稳定），归并排序（稳定） n较大，要快，要稳定 归并排序，与直插结合的改进的归并排序 n很大，位数很少，可以分解 基数排序 记录本身信息量太大 为了避免移动，可以使用链表作为存储结构","tags":[{"name":"排序","slug":"排序","permalink":"http://plmsmile.github.io/tags/排序/"},{"name":"插入","slug":"插入","permalink":"http://plmsmile.github.io/tags/插入/"},{"name":"快排，快速排序","slug":"快排，快速排序","permalink":"http://plmsmile.github.io/tags/快排，快速排序/"},{"name":"堆排序","slug":"堆排序","permalink":"http://plmsmile.github.io/tags/堆排序/"},{"name":"归并排序","slug":"归并排序","permalink":"http://plmsmile.github.io/tags/归并排序/"},{"name":"基数排序","slug":"基数排序","permalink":"http://plmsmile.github.io/tags/基数排序/"},{"name":"冒泡排序","slug":"冒泡排序","permalink":"http://plmsmile.github.io/tags/冒泡排序/"}]},{"title":"cpp-pointer-object-reference","date":"2017-12-25T13:17:36.000Z","path":"2017/12/25/cpp-pointer-object-reference/","text":"类对象和指针的区别 类对象和指针 代码 类 1234567class Test&#123; public: int a; Test()&#123; a = 1; &#125;&#125;; 类指针 12345678910111213141516171819202122void test1() &#123; // 1. 两个类的指针，使用了new Test* t1 = new Test(); t1-&gt;a = 10; Test* t2 = new Test(); t2-&gt;a = 5; // 2. 两个指针的所指向的地址不一样 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl; // 3. 指针t1的值赋值给了t2，两个指针指向的地址相同，都是t1的地址 t2 = t1; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl; t1-&gt;a = 111; t2-&gt;a = 222; // 4. 修改了同样的地方，输出为222 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl;&#125; 类对象 1234567891011121314151617181920212223void test2() &#123; // 1. 两个类对象 Test t1; t1.a = 10; Test t2; t2.a = 5; // 2. 对象的内容不一样 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; // 3. 把t1对象的内容赋值给t2。t2的内容和t1相同 t2 = t1; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; // 4. 再分别修改两个不同的对象，输出分别为111， 222 t1.a = 111; t2.a = 222; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; &#125; 不同点 类指针 类对象 内存分配 定义类指针，不分配内存 定义类对象，分配内存 关系 内存地址，指向类对象。利用构造函数分配一块内存 访问 间接访问 直接访问 多态 实现多态，父类指针调用子类对象 不能实现多态，声明即调用构造函数，已分配内存 堆栈 内存堆，永久变量，手动释放，new-delete搭配 内存栈，局部临时变量 new的对象在堆中 在栈中 使用 new： S* s = new S() , s-&gt;name 声明即可： S s;，s.name 生命期 需要delete 类的析构函数来释放空间 传参 指针4个字节 对象，参数传递资源占用太大 虚函数f 调用分配给它空间时那种类的func 调用自己的func 其它 父类指针指向子类对象 推荐使用const &amp;引用，安全系数高。","tags":[{"name":"cpp","slug":"cpp","permalink":"http://plmsmile.github.io/tags/cpp/"},{"name":"指针，对象，引用","slug":"指针，对象，引用","permalink":"http://plmsmile.github.io/tags/指针，对象，引用/"}]},{"title":"cs224n-assignment-1","date":"2017-12-17T04:47:06.000Z","path":"2017/12/17/cs224n-assignment-1/","text":"Softmax Softmax常数不变性 \\[ \\rm{softmax}(\\mathbf{x})_i = \\frac{e^{\\mathbf x_i}}{\\sum_{j}e^{\\mathbf{x}_j}} \\] 一般在计算softmax的时候，避免太大的数，要加一个常数。 一般是减去最大的数。 \\[ \\rm{softmax}(x) = \\rm{softmax}(x+c) \\] 关键代码 12345678def softmax(x): exp_func = lambda x: np.exp(x - np.max(x)) sum_func = lambda x: 1.0 / np.sum(x) x = np.apply_along_axis(exp_func, -1, x) denom = np.apply_along_axis(sum_func, -1, x) denom = denom[..., np.newaxis] x = x * denom return x 神经网络基础 Sigmoid实现 我的sigmoid笔记 \\[ \\begin{align} &amp; \\sigma (z) = \\frac {1} {1 + \\exp(-z)}, \\; \\sigma(z) \\in (0,1) \\\\ \\\\ &amp; \\sigma^\\prime (z) = \\sigma(z) (1 - \\sigma(z)) \\\\ \\end{align} \\] 关键代码 12345678910def sigmoid(x): s = 1.0 / (1 + np.exp(-x)) return sdef sigmoid_grad(s): \"\"\" 对sigmoid的函数值，求梯度 \"\"\" ds = s * (1 - s) return ds Softmax求梯度 交叉熵和softmax如下，记softmax的输入为\\(\\theta\\) ，\\(y\\)是真实one-hot向量。 \\[ \\begin{align} &amp; \\rm{CE}(y, \\hat y) = - \\sum_{i} y_i \\times \\log (\\hat y_i) \\\\ \\\\ &amp; \\hat y = \\rm{softmax} (\\theta)\\\\ \\end{align} \\] softmax求导 引入记号： \\[ \\begin{align} &amp; f_i = e^{\\theta_i} &amp; \\text{分子} \\\\ &amp; g_i = \\sum_{k=1}^{K}e^{\\theta_k} &amp; \\text{分母，与i无关} \\\\ &amp; \\hat y_i = S_i = \\frac{f_i}{g_i} &amp; \\text{softmax}\\\\ \\end{align} \\] 则有\\(S_i​\\)对其中的一个数据\\(\\theta_j​\\) 求梯度： \\[ \\frac{\\partial S_i}{\\partial \\theta_j} = \\frac{f_i^{\\prime} g_i - f_i g_i^{\\prime}}{g_i^2} \\] 其中两个导数 \\[ f^{\\prime}_i(\\theta_j) = \\begin{cases} &amp; e^{\\theta_j}, &amp; i = j\\\\ &amp; 0, &amp; i \\ne j \\\\ \\end{cases} \\] \\[ g^{\\prime}_i(\\theta_j) = e^{\\theta_j} \\] \\(i=j\\)时 \\[ \\begin{align} \\frac{\\partial S_i}{\\partial \\theta_j} &amp; = \\frac{e^{\\theta_j} \\cdot \\sum_{k}e^{\\theta_k}- e^{\\theta_i} \\cdot e^{\\theta_j}}{\\left( \\sum_ke^{\\theta_k}\\right)^2} \\\\ \\\\ &amp; = \\frac{e^{\\theta_j}}{\\sum_ke^{\\theta_k}} \\cdot \\left( 1 - \\frac{e^{\\theta_j}}{\\sum_k e^{\\theta_k}} \\right) \\\\ \\\\ &amp; = S_i \\cdot (1 - S_i) \\end{align} \\] \\(i \\ne j\\)时 \\[ \\begin{align} \\frac{\\partial S_i}{\\partial \\theta_j} &amp; = \\frac{ - e^{\\theta_i} \\cdot e^{\\theta_j}}{\\left( \\sum_ke^{\\theta_k}\\right)^2} = - S_i \\cdot S_j \\end{align} \\] 交叉熵求梯度 \\[ \\begin{align} &amp; \\rm{CE}(y, \\hat y) = - \\sum_{i} y_i \\times \\log (\\hat y_i) \\\\ \\\\ &amp; \\hat y = \\rm{S} (\\theta)\\\\ \\end{align} \\] 只关注有关系的部分，带入\\(y_i =1\\) ： \\[ \\begin{align} \\frac{\\partial CE}{\\partial \\theta_i} &amp; = -\\frac{\\partial \\log \\hat y_i}{\\partial \\theta_i} = - \\frac{1}{\\hat y_i} \\cdot \\frac{\\partial \\hat y_i}{\\partial \\theta_i} \\\\ \\\\ &amp; = - \\frac{1}{S_i} \\cdot \\frac{\\partial S_i}{\\partial \\theta_i} = S_i - 1 \\\\ \\\\ &amp; = \\hat y_i - y_i \\end{align} \\] 不带入求导 \\[ \\begin{align} \\frac{\\partial CE}{\\partial \\theta_i} &amp; = - \\sum_{k}y_k \\times \\frac{\\partial \\log S_k}{\\partial \\theta_i} \\\\ &amp; = - \\sum_{k}y_k \\times \\frac{1}{S_k}\\times \\frac{\\partial S_k}{\\partial \\theta_i} \\\\ &amp; = - y_i (1 - S_i) - \\sum_{k \\ne i} y_k \\cdot \\frac{1}{S_k} \\cdot (- S_i \\cdot S_k) \\\\ &amp; = - y_i (1 - S_i) + \\sum_{k \\ne i} y_k \\cdot S_i \\\\ &amp; = S_i - y_i \\end{align} \\] 所以，交叉熵的导数是 \\[ \\frac{\\partial CE}{\\partial \\theta_i} = \\hat y_i - y_i, \\quad \\quad \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta} = \\hat y - y \\] 即 \\[ \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta_i} = \\begin{cases} &amp; \\hat y_i - 1, &amp; \\text{i是label} \\\\ &amp;\\hat y_i, &amp; \\text{其它}\\\\ \\end{cases} \\] 简单网络 前向计算 \\[ \\begin{align} &amp; z_1 = xW_1 + b_1 \\\\ \\\\ &amp; h = \\rm{sigmoid}(z1) \\\\ \\\\ &amp; z_2 = hW_2 + b_2 \\\\ \\\\ &amp; \\hat y = \\rm{softmax}(z_2) \\end{align} \\] 关键代码： 123def forward_backward_prop(data, labels, params, dimensions): h = sigmoid(np.dot(data, W1) + b1) yhat = softmax(np.dot(h, W2) + b2) loss函数 \\[ J = \\rm{CE}(y, \\hat y) \\] 关键代码： 123def forward_backward_prop(data, labels, params, dimensions): # yhat[labels==1]实际上是boolean索引，见我的numpy_api.ipynb cost = np.sum(-np.log(yhat[labels == 1])) / data.shape[0] 反向传播 \\[ \\begin {align} &amp; \\delta_2 = \\frac{\\partial J}{\\partial z_2} = \\hat y - y \\\\ \\\\ &amp; \\frac{\\partial J}{\\partial h} = \\delta_2 \\cdot \\frac{\\partial z_2}{\\partial h} = \\delta_2 W_2^T \\\\ \\\\ &amp; \\delta_1 = \\frac{\\partial J}{\\partial z_1} = \\frac{\\partial J}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z_1} = \\delta_2 W_2^T \\circ \\sigma^{\\prime}(z_1) \\\\ \\\\ &amp; \\frac{\\partial J}{\\partial x} = \\delta_1 W_1^T \\end{align} \\] 一共有\\((d_x + 1) \\cdot d_h + (d_h +1) \\cdot d_y\\) 个参数。 关键代码： 123456789101112131415def forward_backward_prop(data, labels, params, dimensions): # 前面推导的softmax梯度公式 gradyhat = (yhat - labels) / data.shape[0] # 链式法则 gradW2 = np.dot(h.T, gradyhat) # 本地导数是1，把第1维的所有加起来 gradb2 = np.sum(gradyhat, axis=0, keepdims=True) gradh = np.dot(gradyhat, W2.T) gradz1 = gradh * sigmoid_grad(h) gradW1 = np.dot(data.T, gradz1) gradb1 = np.sum(gradz1, axis=0, keepdims=True) grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten())) return cost, grad 梯度检查 我的梯度检查 123456789101112131415161718192021222324252627def gradcheck_naive(f, x): fx, grad = f(x) # Evaluate function value at original point h = 1e-4 # Do not change this! # Iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: ix = it.multi_index # 关键代码 x[ix] += h random.setstate(rndstate) new_f1 = f(x)[0] x[ix] -= 2 * h random.setstate(rndstate) new_f2 = f(x)[0] x[ix] += h numgrad = (new_f1 - new_f2) / (2 * h) # Compare gradients reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix])) if reldiff &gt; 1e-5: print (\"Gradient check failed.\") print (\"First gradient error found at index %s\" % str(ix)) print (\"Your gradient: %f \\t Numerical gradient: %f\" % ( grad[ix], numgrad)) return it.iternext() # Step to next dimension Word2Vec 我的word2vec笔记 词向量的梯度 符号定义 \\(v_c\\) 中心词向量，输入词向量，\\(V\\)， \\(\\mathbb{R}^{W\\times d}\\) \\(u_o\\) 上下文词向量，输出词向量，\\(U=[u_1, u_2, \\cdots, u_w]\\) , \\(\\mathbb{R}^{d\\times W}\\) 前向 预测o是c的上下文概率，o为正确单词 \\[ \\hat y_o = p(o \\mid c) = \\rm{softmax}(o) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w} \\exp(u_w^T v_c)} \\] 得分向量： \\[ z=U^T \\cdot v_c, \\quad [W,d] \\times[ d] \\in ,\\mathbb{R}^{W } \\] loss及梯度 \\[ J_{\\rm{softmax-CE}}(v_c, o, U) = CE(y, \\hat y), \\quad \\text{其中} \\; \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta} = \\hat y - y \\] 梯度 中文 计算 维数 \\(\\frac{\\partial J}{\\partial z}\\) softmax \\(\\hat y - y\\) \\(W\\) \\(\\frac{\\partial J}{\\partial v_c}\\) 中心词 \\(\\frac{\\partial J}{\\partial z} \\cdot \\frac{\\partial z}{\\partial v_c} = (\\hat y - y) \\cdot U^T\\) \\(d\\) \\(\\frac{\\partial J}{\\partial U}\\) 上下文 \\(\\frac{\\partial J}{\\partial z} \\cdot \\frac{\\partial z}{\\partial U^T}= (\\hat y - y) \\cdot v_c\\) \\(d \\times W\\) 关键代码 123456789101112131415161718192021222324def softmaxCostAndGradient(predicted, target, outputVectors, dataset): \"\"\" Softmax cost function for word2vec models Args: predicted: 中心词vc target: 上下文uo, index outputVectors: 输出，上下文矩阵U，W*d，未转置 dataset: Returns: cost: 交叉熵loss gradv: 一维向量 gradU: W*d \"\"\" vhat = predicted z = np.dot(outputVectors,vhat) preds = softmax(z) # Calculate the cost: cost = -np.log(preds[target]) # Gradients gradz = preds.copy() gradz[target] -= 1.0 gradU = np.outer(z, vhat) gradv = np.dot(outputVectors.T, z) ### END YOUR CODE return cost, gradv, gradU","tags":[{"name":"cs224n","slug":"cs224n","permalink":"http://plmsmile.github.io/tags/cs224n/"},{"name":"assignment","slug":"assignment","permalink":"http://plmsmile.github.io/tags/assignment/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"},{"name":"cbow","slug":"cbow","permalink":"http://plmsmile.github.io/tags/cbow/"},{"name":"skip-gram","slug":"skip-gram","permalink":"http://plmsmile.github.io/tags/skip-gram/"}]},{"title":"cs224n-notes-5-rnn","date":"2017-12-12T03:29:13.000Z","path":"2017/12/12/cs224n-notes-5-rnn/","text":"公式 \\[ P(s) = P(小猫) \\cdot P(爱 \\mid 小猫) \\cdot P(吃 \\mid 小猫,爱) \\cdot P(\\text{鱼} \\mid 小猫,爱,吃 ) \\] \\[ P(s) = P(小猫) \\cdot P(爱 \\mid 小猫) \\cdot P(吃 \\mid 爱) \\cdot P(\\text{鱼} \\mid 吃 ) \\] 具体推导 \\[ p(爱\\mid 小猫) = \\frac {\\rm{count(小猫, \\,爱)}} {\\rm{count}(小猫)} \\] 设hidden_size是 \\(d_h\\) ，词向量的维度是\\(d\\) 符号 意义 维数 \\(x_t\\) 时刻\\(t\\)的输入词向量 \\(\\mathbb R^{d}\\) \\(W^{(hx)}\\) 调整输入\\(x\\) \\(\\mathbb R^{d_h \\times d}\\) \\(h_{t-1}\\) 时刻t-1的隐状态 \\(\\mathbb R^{d_h}\\) \\(W^{(hh)}\\) 调整上一时刻的隐状态\\(h_{t-1}\\) \\(\\mathbb R^{d_h \\times d_h}\\) \\(W^{(hy)}\\) 把隐状态转h换为输出单词 \\(\\mathbb R^{\\vert V\\vert \\times d_h}\\) \\(y_{t}\\) 结合\\(x_t\\)与历史信息，预测的t+1时刻的单词 \\(\\mathbb R^{\\vert V\\vert}\\) \\(a\\) 激活函数，这里是sigmoid 无 RNN语言模型的计算过程 \\[ \\begin {align} &amp; h_t = a (W^{(hh)} h_{t-1} + W^{(hx)} x_t) \\\\ \\\\ &amp; \\hat y_t = \\rm{softmax} (W^{(hy)} h_t) \\\\ \\\\ &amp; \\hat p(x_{t+1}=w_j \\mid x_1, \\cdots, x_t) = \\hat y_{t,j} \\end{align} \\] 时刻t的损失 \\[ J_t(\\theta) = - \\sum_{j=1}^{\\vert V\\vert} y_{t, j} \\times \\log (\\hat y_{t,j}) \\] 总损失 \\[ J = \\frac{1}{T} \\sum_{t=1}^{T} J_t(\\theta) = - \\frac{1}{T} \\sum_{t=1}^{T}\\sum_{j=1}^{\\vert V\\vert} y_{t, j} \\times \\log (\\hat y_{t,j}) \\] 模型困惑度 \\[ \\rm{Perplexity} = 2 ^{\\text{交叉熵}} = 2^{J} \\] 梯度 简单点 \\[ \\begin {align} &amp; h_t = Wh_{t-1} + W^{(hx)} x_t \\\\ \\\\ &amp; \\hat y_t =W^{(s)} f(h_t) \\\\ \\\\ \\end{align} \\] 总的误差是之前每个时刻的误差之和 \\[ \\frac{\\partial E}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial E_t}{\\partial W} \\] 每一时刻的误差又是之前每个时刻的误差之和，应用链式法则 \\[ \\frac{\\partial E_t}{\\partial W} = \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\sum_{k=1}^t \\frac{\\partial h_t}{\\partial h_k} \\frac{\\partial h_k}{\\partial W} \\] \\[ \\frac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\color{blue} {\\frac{\\partial h_t}{\\partial h_k}} \\frac{\\partial h_k}{\\partial W} \\] \\[ \\frac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t \\frac{\\partial E_k}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k}\\frac{\\partial h_k}{\\partial h_{k-1}} \\frac{\\partial h_{k-1}}{\\partial W} \\] 而中间的 \\[ \\frac{\\partial h_t}{\\partial h_k} = \\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}} = \\prod_{j=k+1}^t W^T \\times \\rm{diag}[f^{\\prime}(j_{j-1})] \\] 而导数矩阵雅克比矩阵 \\[ \\frac{\\partial h_j}{\\partial h_{j-1}} = [ \\frac{\\partial h_{j}}{\\partial h_{j-1,1}}, \\cdots , \\frac{\\partial h_{j}}{\\partial h_{j-1,d_h}}] = \\begin{bmatrix} \\frac{\\partial h_{j,1}}{\\partial h_{j-1,1}} &amp; \\cdots &amp; \\frac{\\partial h_{j,1}}{\\partial h_{j-1,d_h}} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial h_{j,d_h}}{\\partial h_{j-1,1}} &amp; \\cdots &amp; \\frac{\\partial h_{j,d_h}}{\\partial h_{j-1,d_h}} \\\\ \\end{bmatrix} \\] 合并起来，得到最终的 \\[ \\frac{\\partial E}{\\partial W} = \\sum_{t=1}^T\\sum_{k=1}^t \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} (\\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}}) \\frac{\\partial h_k}{\\partial W} \\] 两个不等式 \\[ \\| \\frac{\\partial h_j}{\\partial h_{j-1}}\\| \\le \\| W^T\\| \\cdot \\|\\rm{diag}[f^{\\prime}(h_{j-1})] \\| \\le \\beta_W \\beta_h \\] 所以有 \\[ \\| \\frac{\\partial h_t}{\\partial h_k} \\| =\\| \\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}} \\| \\le \\color{blue}{ (\\beta_W \\beta_h)^{t-k}} \\] 防止梯度爆炸 \\[ \\hat g =\\frac{\\partial E}{\\partial W} \\\\ \\\\ \\\\ \\hat g = \\frac{\\text{threshold}}{\\|\\hat g\\|} \\hat g \\] softmax \\[ p(w_t \\mid h_{t-1}) = p(c_t \\mid h_{t-1}) \\cdot p(w_t \\mid c_t) \\] Quasi-RNN 输入单词序列，\\(T\\)个d维的词向量 \\[ X \\in \\mathbb {R}^{T\\times d} \\] m维的filters，产生一个新的序列，产生m维的候选向量\\(\\mathbf{z}_t\\) \\[ Z \\in \\mathbb {R}^{T \\times m} \\] 宽度为k的filter，每个\\(z_t\\)只依赖于\\(\\mathbf{x}_{t-k+1}\\)到 \\(\\mathbf{x}_{t}\\) 。masked convolution \\[ \\mathbf{x}_{t-k+1} \\quad \\mathbf{x}_{t} \\quad \\mathbb{R}^{k \\times d \\times m} \\] 参数维数都是\\(\\mathbb{R}^{k \\times d \\times m}\\) ,*是masked convolution \\[ \\begin{align} &amp; Z = \\tanh (W_z * X) \\\\ \\\\ &amp; F = \\sigma(W_f * X) \\\\ \\\\ &amp; O = \\sigma(W_o * X) \\end{align} \\] fo-pooling \\[ \\begin{align} &amp; c_t = f_t \\cdot c_{t-1} + (1 - f_t) \\cdot z_t \\\\ \\\\ &amp; h_t = o_t \\cdot c_t \\end{align} \\] LSTM梯度消失 本质上维护一个状态\\(h_t\\)， 传统RNN，覆写 \\[ h_t = f(h_{t-1}, x_t) \\] 现代RNN，LSTM&amp;GRU，累加 \\[ h_t = \\sum_{k=1}^t \\Delta h_k \\] 和Residual Net相似，利用一个独立的加法通道，把梯度保持下去。","tags":[{"name":"cs224n","slug":"cs224n","permalink":"http://plmsmile.github.io/tags/cs224n/"}]},{"title":"nlp-labels","date":"2017-12-03T07:31:23.000Z","path":"2017/12/03/nlp-labels/","text":"词性标注和句法依存的符号 词性标注 IBM英语和中文标注集 书上的 斯坦福Stanford coreNLP 中 宾州树库 中的汉语词性标注规范， 如下 词性标记 英文名称 中文名称 示例 AD adverbs 副词 还 AS aspect marker 体表词 了，着，过，的（我是去年来过的） BA in ba-const 把，将 把，将 CC coordinating conjunction 并列连词 和，与，或，或者 CD cardinal conj 数词，基数词 一百 CS subordinating conj 从数连词 若，如果，如 DEC for relative-clause etc 标句词，关系从句的 我买的书 DEG associative 所有格、连接作用的 我的书 DT determiner 限定词 这 ETC tag for words in coordination phrase 等，等等 科技文教等领域，等，等等 IJ Interjection 感叹词 啊 JJ noun-modifier other than nouns 其他名词修饰语 共同的/DEG目的/NN他/PN是/VC男的/DEG LB in long bei-construction 长被 被他打了 LC localizer 方位词 桌子上 M measure word（including classifiers） 量词 一块糖 MSP some particles 其他结构助词 他/PN 所 需要/VV 的/DEC 所，而，以 NN common nouns 普通名词 桌子 NR proper nouns 专有名词 天安门 NT temporal nouns 时间名词 清朝，一月 OD ordinal numbers 序数词 第一 ON onomatopoeia 拟声词 哗啦啦 P prepositions 介词 在 PN pronouns 代词 你，我，他 PU punctuations 标点 ， 。 SB In long bei-consturction 短被 他/PN 被/SB 训了/AS SP Sentence-final particle 句末助词 你好吧、SP吧 呢 啊 吗 VA Predicative adjective 谓词形容词 太阳 红彤彤/VA 雪白 丰富 VC Copula 系动词 是 为 非 VE as the main verb “有”作为主要动词 有，无 VV verbs 普通动词 喜欢，走 自己总结的 标记 英文 中文 NP noun phrase 名词短语 PP prepositional phrase 介词短语 VP verb phrase 动词短语 NNS 名词（复数） NNP 专有名词（单数） NNPS 专有名词（复数） PRP 人称代词 JJ 形容词 JJS 形容词（最高级） JJR 形容词（比较级） MD 情态动词 VB 动词 VBP 动词，现在时，非第三人称单数 VBZ 动词，现在时，第三人称单数 VBG 动词，动名词，现在分词 英语标记 复杂版 词性标记 描述 UNKNOW 未知词 DT 限定词 QT 量词 CD 基数 NN 名词，单数 NNS 名词，复数 NNP 专有名词，单数 NNPS 专有名词，复数 EX 存在性的There PRP 人称代词，PP PRP$ 物主代词，PP$ POS 所有格结束词 RB 副词 RBS 副词，最高级 RBR 副词，比较级 句法依存 中心语为谓语 符号 意义 英语 备注 subj 主语 subject nsubj 名词性主语 nominal subject 同步、建设 top 主题 topic 是，建筑 npsubj 被动型主语 nominal passive subject 被句子 中的主语 csubj 从句主语 clausal subject 中文里无 xsubj x主语 一般一个主语下含多个从句 中心语为谓语或介词 符号 意义 英语 备注 obj 宾语 object dobj 直接宾语 颁布，文件 iobj 间接宾语 indirect object 基本不存在 range 间接宾语为数量词，格 成交，元 pobj 介词宾语 根据，要求 lobj 时间介词 来，今年 中心语为谓词 符号 意义 英语 备注 comp 补语 ccomp 从句补语 一般由两个动词组成， xcomp x从句补语 xclausal complement 不存在 acomp 形容词补语 adjectival complement tcomp 时间补语 temporal complement 遇到，以前 lccomp 位置补语 localizer complement 占，以上 rscomp 结果补语 resultative complement 中心语为名词 符号 意义 英语 备注 mod 修饰语 modifier pass 被动修饰 passive tmod 时间修饰 temporal modifier remod 关系从句修饰 relative clause modifier 问题，遇到 numod 数量修饰 numeric modifier 规定，若干 ornmod 序数修饰 numeric modifier clf 类别修饰 classifier modifier 文件，件 nmod 符合名词修饰 noun compound modifier 浦东，上海 amod 形容词修饰 adjective modifier 情况，新 advmod 副词修饰 adverbial modifier 做到，基本 vmod 动词修饰 verb modifier, participle modifier prnmod 插入词修饰 parenthetical modifier neg 不定修饰 negative modifier 遇到，不 det 限定词修饰 determiner modifier 活动，这些 possm 所属标记 possessive maker NP poss 所属修饰 possessive modifier NP dvpm DVP标记 DVP maker DVP(简单，的) dvpmod DVP修饰 DVP modifier DVP(采取，简单) assm 关联标记 associative marker DNP(开发，的) assmod 关联修饰 associative modifier NP|QP (教训，特区) prep 介词修饰 prepositional modifier NP|VP|IP (采取，对) clmod 从句修饰 clause modifier 因为，开始 plmod 介词性地点修饰 prepositional localizer modifier 在，上 asp 时态修饰 aspect marker 做到，了 partmod 分词修饰 participial modifier 中文不存在 etc 等关系 办法，等 中心语为实词 符号 意义 英语 备注 conj 联合 conjunct cop 系动双指助动词 copula cc 连接 coordination 指中心词与连词 其他 符号 意义 英语 备注 attr 属性关系 是，过程 cordmod 并列联合动词 coordinated verb compound 颁布，实行 mmod 清潭洞次 modal verb 得到，能 ba 把字关系 tclaus 时间从句 以后，积累 补语化成分 complementizer 一般指","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://plmsmile.github.io/tags/词性标注/"},{"name":"句法依存","slug":"句法依存","permalink":"http://plmsmile.github.io/tags/句法依存/"}]},{"title":"cs231n-linear-notes","date":"2017-11-27T03:24:40.000Z","path":"2017/11/27/cs231n-linear-notes/","text":"线性分类器，svm和交叉熵损失函数 线性分类器 得分函数 图片是三维数组，元素在0-255的整数。宽度-高度-3 ，3代表RGB颜色通道。 对图像进行零中心化。 输入图片\\(D=32 \\times 32 \\times 3 = 3072\\)个像素，压缩成1维向量，一共有\\(K=10\\)个类别。 \\[ f(x_i, W, b) = Wx_i + b \\] 分析 \\(W\\)的每一行都是一个类别的分类器，一共10个 得到分为每个类的score 改变\\(W, b\\) ，使得分类准确，正确的score高，错误的score低 为x添加一维，\\(x \\in \\mathbb R^{D+1}\\) ， 写为： \\[ f (x_i, W) = Wx_i \\] 理解 权重 输入4个像素，函数会根据权重对某些位置的某些颜色表现出喜好或者厌恶（正负）。 比如船类别，一般周围有很多蓝色的水，那么蓝色通道的权值就会很大（正）。绿色和红色就比较低（负）。那么如果出现绿色和红色的像素，就会降低是船的概率。 权重解释2 \\(W\\)的每一行对应于一个分类的模板(原型)， 用图像和模板去比较，计算得分（点积），找到最相似的模板。 线性函数 实际上，每个输入\\(x_i\\)就是3072维空间中的一个点，线性函数就是对这些点进行边界决策分类。 与线的距离越大，得分越高。 损失函数 最常用两个分类器：SVM和Softmax。分别使用SVM loss和交叉熵loss。 SVM 多类支持向量积损失（Multiclass Support Vector Machine）。正确分类比错误分类的得分高出一个边界值\\(\\Delta (一般= 1)\\) 。 记\\(x_i\\)分为第j个类别的得分为\\(s_j = f(x_i, W)_j\\) ，单个折叶损失 hinge loss， 也称作max margin loss ，如下： \\[ \\begin {align} loss &amp; = \\max(0, s_j - s_{y_i}+ \\Delta)= \\begin {cases} &amp; 0, &amp; s_{y_i} - s_j &gt; \\Delta \\\\ &amp; s_j - s_{y_i}+ \\Delta, &amp; 其它 \\\\ \\end{cases} \\\\ \\\\ &amp; =\\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\\\ \\end{align} \\] 如果错误分类进入红色区域，就开始计算loss。 第\\(i\\)个数据的loss就是把所有错误类别的loss加起来： \\[ L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i}+ \\Delta) \\] 正则化 plmsmile的L2正则化 使用L2正则化，对\\(W\\)的元素进行平方惩罚， 抑制大数值的权值。正则化loss（正则化惩罚）如下： \\[ R(W) = \\sum_k\\sum_l W_{kl}^2 \\] 最终loss就是数据损失+正则化损失， \\(\\lambda\\) 是正则化强度。 \\[ L = \\underbrace {\\frac{1}{N}\\sum_{i}L_i}_{\\text{data loss}} + \\underbrace {\\lambda R(W)}_{\\text{正则化 loss}} \\] 引入正则化以后，SVM就有了最大边界max margin 这一个良好性质。（不是很懂，后面再解决） Softmax plmsmile的交叉熵 每个类别的得分 \\[ s_j = f(x_i, W)_j = f_j \\] Softmax函数求得\\(x_i\\)分为第\\(j\\) 类的概率，这样会求得所有类别的概率，即预测的结果。 \\[ p(j \\mid x_i) = \\frac {\\exp(f_j)} {\\sum_{k} \\exp(f_k)} \\] 单个数据的loss，就是取其概率的负对数 ： \\[ L_i = - \\log p(y_i \\mid x_i) = - \\log \\left( \\frac{e^{f_{y_i}}}{\\sum e^{f_k}}\\right) = -f_{y_i} + \\log \\sum_{k}e^{f_k} \\] 从直观上看，最小化loss就是要最大化正确的概率（最小化正确分类的负对数概率），最小化其它分类的概率。 交叉熵的体现 程序会预测一个所有类别的概率分布\\(q = (p(1 \\mid x_i), \\cdots, p(K \\mid x_i))\\) 。真实label概率\\(p = (0, \\cdots, 1, 0,\\cdots, 0)\\) ，交叉熵： \\[ \\begin{align} H(p, q) &amp; = - \\sum_{x} p(x) \\log q(x) \\\\ &amp; = - (p(y_i) \\cdot \\log q(y_i)) = - (1 \\cdot \\log p(y_i \\mid x_i) ) = - \\log p(y_i \\mid x_i) \\end{align} \\] 由于\\(H(p) = 0\\)， 唯一确定，熵为0。交叉熵就等于真实和预测的分布的KL距离 。也就是说想要两个概率分布一样，即预测的所有概率密度都在正确类别上面。 \\[ H(p, q) = H(p) + D_{KL}(p || q) = D_{KL}(p || q) \\] 结合正则化 结合正则化 \\[ L = \\underbrace {\\frac{1}{N}\\sum_{i}L_i}_{\\text{data loss}} + \\underbrace {\\lambda R(W)}_{\\text{正则化 loss}} \\] 最小化正确概率分类的负对数概率，就是在进行最大似然估计。正则化部分就是对W的高斯先验，这里进行的是最大后验估计。（不懂） SVM和Softmax比较 SVM loss：希望正确分类比其他分类的得分高出一个边界值。 Softmax 交叉熵loss：希望正确分类概率大，其它分类概率小。","tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://plmsmile.github.io/tags/cs231n/"},{"name":"线性分类","slug":"线性分类","permalink":"http://plmsmile.github.io/tags/线性分类/"},{"name":"svm","slug":"svm","permalink":"http://plmsmile.github.io/tags/svm/"}]},{"title":"cs224n-notes3-神经网络2","date":"2017-11-26T08:21:23.000Z","path":"2017/11/26/cs224n-notes3-neural-networks-2/","text":"过拟合 过拟合 训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种： early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及到所有的数据点，意味着拟合函数波动很大。 看到，在某些很小的区间内里，函数值的变化很剧烈。意味着这些小区间的导数值（绝对值）非常大。由于自变量值可大可小，所以只有系数足够大，才能保证导数值足够大。 所以：过拟合时，参数一般都很大。参数较小时，意味着模型复杂度更低，对数据的拟合刚刚好， 这也是奥卡姆剃刀法则。 范数 向量范数 \\(x \\in \\mathbb {R}^d\\) 范数 定义 1-范数 \\(\\left \\| x\\right\\|_1 = \\sum_i^d \\|x_i\\|\\)， 绝对值之和 2-范数 \\(\\left \\| x\\right\\|_2 = \\left(\\sum_i^d \\|x_i\\|^2\\right)^{\\frac{1}{2}}\\)， 绝对值之和再开方 p-范数 \\(\\left \\| x\\right\\|_p = \\left(\\sum_i^d \\|x_i\\|^p\\right)^{\\frac{1}{p}}\\)， 绝对值的p次方之和的\\(\\frac{1}{p}\\)次幂 \\(\\infty\\)-范数 \\(\\left \\| x\\right\\|_\\infty = \\max_\\limits i \\|x_i\\|\\) ，绝对值的最大值 -\\(\\infty\\)-范数 \\(\\left \\| x\\right\\|_{-\\infty} = \\min_\\limits i \\|x_i\\|\\) ，绝对值的最小值 矩阵范数 \\(A \\in \\mathbb R^{m \\times n}\\) 范数 定义 1-范数 \\(\\left \\| A\\right\\|_1 = \\max \\limits_{j}\\sum_i^m \\|a_{ij}\\|\\)，列和范数，矩阵列向量绝对值之和的最大值。 \\(\\infty\\)-范数 \\(\\left \\| A\\right\\|_\\infty = \\max_\\limits i \\sum_{j}^{n}\\|a_{ij}\\|\\) ，行和范数，所有行向量绝对值之和的最大值。 2-范数 \\(\\left \\| A\\right\\|_2 = \\sqrt{\\lambda_{m}}\\) ， 其中\\(\\lambda_m\\)是\\(A^TA\\)的最大特征值。 F-范数 \\(\\left \\| A\\right\\|_F = \\left(\\sum_i^m \\sum_j^n a_{ij}^2\\right)^{\\frac{1}{2}}\\)，所有元素的平方之和，再开方。或者不开方， L2正则化就直接平方，不开方。 L2正则化权重衰减 为了避免过拟合，使用L2正则化参数。\\(\\lambda\\)是正则项系数，用来权衡正则项和默认损失的比重。\\(\\lambda\\) 的选取很重要。 \\[ J_R = J + \\lambda \\sum_{i=1}^L \\left \\| W^{(i)}\\right \\|_F \\] L2惩罚更倾向于更小更分散的权重向量，鼓励使用所有维度的特征，而不是只依赖其中的几个，这也避免了过拟合。 标准L2正则化 \\(\\lambda\\) 是正则项系数，\\(n\\)是数据数量，\\(w\\)是模型的参数。 \\[ C = C_0 + \\frac {\\lambda} {2n} \\sum_w w^2 \\] \\(C\\)对参数\\(w\\)和\\(b\\)的偏导： \\[ \\begin {align} &amp; \\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\\\ &amp; \\frac{\\partial C}{\\partial b} = \\frac{\\partial C_0}{\\partial b} \\\\ \\end{align} \\] 更新参数 ：可以看出，正则化\\(C\\)对\\(w\\)有影响，对\\(b\\)无影响。 \\[ \\begin{align} w &amp;= w - \\alpha \\cdot \\frac{\\partial C}{\\partial w} \\\\ &amp;= (1 - \\frac{\\alpha \\lambda}{n})w - \\alpha \\frac{\\partial C_0}{\\partial w} \\\\ \\end{align} \\] 从上式可以看出： 不使用正则化时，\\(w\\)的系数是1 使用正则化时 \\(w\\)的系数是\\(1 - \\frac{\\alpha \\lambda}{n} &lt; 1\\) ，效果是减小\\(w\\)， 所以是权重衰减 weight decay 当然，\\(w\\)具体增大或减小，还取决于后面的导数项 mini-batch随机梯度下降 设\\(m\\) 是这个batch的样本个数，有更新参数如下，即求batch个C对w的平均偏导值 \\[ \\begin {align} &amp; w = (1 - \\frac{\\alpha \\lambda}{n})w - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}\\frac{\\partial C_i}{\\partial w} \\\\ &amp; b = b - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}\\frac{\\partial C_i}{\\partial b} \\\\ \\end{align} \\] 所以，权重衰减后一般可以减小过拟合。 L2正则化比L1正则化更加发散，权值也会被限制的更小。 一般使用L2正则化。 还有一种方法是最大范数限制：给范数一个上界\\(\\left \\| w \\right \\| &lt; c\\) ， 可以在学习率太高的时候网络不会爆炸，因为更新总是有界的。 实例说明 增加网络的层的数量和尺寸时，网络的容量上升，多个神经元一起合作，可以表达各种复杂的函数。 如下图，2分类问题，有噪声数据。 一个隐藏层。神经元数量分别是3、6、20。很明显20过拟合了，拟合了所有的数据。正则化就是处理过拟合的非常好的办法。 对20个神经元的网络，使用正则化，解决过拟合问题。正则化强度\\(\\lambda\\)很重要。 L1正则化 正则化loss如下： \\[ C = C_0 + \\frac {\\lambda} {n} \\sum_w |w| \\] 对\\(w\\)的偏导， 其中\\(\\rm{sgn}(w)\\)是符号函数： \\[ \\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} \\cdot \\rm{sgn}(w) \\] 更新参数： \\[ w = w - \\frac{\\alpha \\lambda}{n} \\cdot \\rm{sgn}(w) - \\alpha \\frac{\\partial C_0}{\\partial w} \\] 分析：\\(w\\)为正，减小；\\(w\\)为负，增大。所以L1正则化就是使参数向0靠近，是权重尽可能为0，减小网络复杂度，防止过拟合。 特别地：当\\(w=0\\)时，不可导，就不要正则化项了。L1正则化更加稀疏。 随机失活Dropout Dropout是非常有用的正则化的办法，它改变了网络结构。一般采用L2正则化+Dropout来防止过拟合。 训练的时候，输出不变，随机以概率\\(p\\)保留神经元，\\(1-p\\)删除神经元（置位0）。每次迭代删除的神经元都不一样。 BP的时候，置位0的神经元的参数就不再更新， 只更新前向时alive的神经元。 预测的时候，要保留所有的神经元，即不使用Dropout。 相当于训练了很多个（指数级数量）小网络（半数网络），在预测的时候综合它们的结果。随着训练的进行，大部分的半数网络都可以给出正确的分类结果。 数据预处理 用的很多的是0中心化。CNN中很少用PCA和白化。 应该：线划分训练、验证、测试集，只是从训练集中求平均值！然后各个集再减去这个平均值。 中心化 也称作均值减法， 把数据所有维度变成0均值，其实就是减去均值。就是将数据迁移到原点。 \\[ x = x - \\rm{avg}(x) = x - \\bar x \\] 标准化 也称作归一化， 数据所有维度都归一化，使其数值变化范围都近似相等。 除以标准差 最大值和最小值按照比例缩放到\\((-1 ,1)\\) 之间 方差\\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar x)^2\\) ，标准差就是\\(s\\) 。数据除以标准差，接近标准高斯分布。 \\[ x = \\frac{x}{s} \\] PCA 斯坦福PCA ，CSDNPCA和SVD的区别和联系 协方差 协方差就是乘积的期望-期望的乘积。 \\[ \\rm{Cov}(X, Y) = E(XY) - E(X)E(Y) \\] 协方差的性质如下： \\[ \\begin{align} &amp; \\rm{Cov}(X, Y) = \\rm{Conv}(Y, X) \\\\ \\\\ &amp; \\rm{Cov}(aX, bY) = ab \\cdot \\rm{Conv}(Y, X) \\\\ \\\\ &amp; \\rm{Cov}(X, X) = E(X^2) - E^2(X) = D(X) , \\quad \\text{三方公式}\\\\ \\\\ &amp; \\rm{Cov}(X, C) = 0 \\\\ \\\\ &amp; \\rm{Cov}(X, Y) = 0 \\leftrightarrow X与Y独立 \\end{align} \\] 还有别的性质就看考研笔记吧。 奇异值分解 \\[ A_{m \\times n} = U_{m \\times m} \\Sigma_{m \\times n} V^T_{n \\times n} \\] \\(V_{n \\times n}\\) ：\\(V\\)的列，一组对A正交输入或分析的基向量（线性无关）。这些向量是\\(M^TM\\) 的特征向量。 \\(U_{m \\times m}\\) ：\\(U\\)的列，一组对A正交输出的基向量 。是\\(MM^T\\)的特征向量。 \\(\\Sigma_{m \\times n}\\)：对角矩阵。对角元素按照从小到大排列，这些对角元素称为奇异值。 是\\(M^TM, MM^T\\) 的特征值的非负平方根，并且与U和V的行向量对应。 记\\(r\\)是非0奇异值的个数，则A中仅有\\(r\\)个重要特征，其余特征都是噪声和冗余特征。 奇异值的物理意义 利用SVD进行PCA 先将数据中心化。输入是\\(X \\in \\mathbb R^ {N \\times D}\\) ，则协方差矩阵 如下： \\[ \\mathrm{Cov}(X) = \\frac{X^TX}{N} \\; \\in \\mathbb R^{D \\times D} \\] 比如X有a和b两维，均值均是0。那么\\(\\rm{Cov}(ab)=E(ab)-0=(a_0b_0+a_1b_1+\\cdots + a_nb_n) /n\\) ，就得到了协方差值。 中心化 计算\\(x\\)的协方差矩阵cov 对协方差矩阵cov进行svd分解，得到u, s, v 去除x的相关性，旋转，\\(xrot = x \\cdot u\\) ，此时xrot的协方差矩阵只有对角线才有值，其余均为0 选出大于0的奇异值 数据降维 12345678910111213141516171819202122def test_pca(): x = np.random.randn(5, 10) # 中心化 x -= np.mean(x, axis=0) print (x.shape) # 协方差 conv = np.dot(x.T, x) / x.shape[0] print (conv.shape) print (conv) u, s, v = np.linalg.svd(conv) print (s) print (u.shape, s.shape, v.shape) # 大于0的奇异值 n_sv = np.where(s &gt; 1e-5)[0].shape[0] print(n_sv) # 对数据去除相关性 xrot = np.dot(x, u) print (xrot.shape) # 数据降维 xrot_reduced = np.dot(x, u[:, :n_sv]) # 降到了4维 print (xrot_reduced.shape) 白化 斯坦福白化 白化希望特征之间的相关性较低，所有特征具有相同的协方差。白化后，得到均值为0，协方差相等的矩阵。对\\(xrot\\)除以特征值。 \\[ x_{white} = \\frac{x_{rot}}{\\sqrt{\\lambda + \\epsilon}} \\] 1x_white = xrot / np.sqrt(s + 1e-5) 缺陷是：可能会夸大数据中的早上，因为把所有维度都拉伸到了相同的数值范围。可能有一些极少差异性（方差小）但大多数是噪声的维度。可以使用平滑来解决。 权重初始化 如果数据恰当归一化以后，可以假设所有权重数值中大约一半为正数，一半为负数。所以期望参数值是0。 千万不能够全零初始化。因为每个神经元的输出相同，BP时梯度也相同，参数更新也相同。神经元之间就失去了不对称性的源头。 小随机数初始化 如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并成为网络的不同部分。 参数接近于0单不等于0。使用零均值和标准差的高斯分布来生成随机数初始化参数，这样就打破了对称性。 1W = 0.01 * np.random.randn(D,H) 注意：不是参数值初始越小就一定好。参数小，意味着会减小BP中的梯度信号，在深度网络中，就会有问题。 校准方差 随着数据集的增长，随机初始化的神经元的输出数据分布的方差也会增大。可以使用\\(\\frac{1}{\\sqrt{n}}\\) 校准方差。n是数据的数量。这样就能保证网络中所有神经元起始时有近似同样的输出分布。这样也能够提高收敛的速度。 感觉实际上就是做了一个归一化。 数学详细推导见cs231n ， \\(s = \\sum_{i}^nw_ix_i\\) ，假设w和x都服从同样的分布。想要输出s和输入x有同样的方差。 \\[ \\begin {align} &amp; \\because D(s) = n \\cdot D(w)D(x), \\; D(s) = D(x) \\\\ &amp; \\therefore D(w) = \\frac{1}{n} \\\\ &amp; \\because D(w_{old}) = 1, \\; D(aX) = a^2 D(X) \\\\ &amp; \\therefore D(w) = \\frac{1}{n}D(w_{old}) = D(\\frac{1}{\\sqrt n} w_{old}) \\\\ &amp; \\therefore w = \\frac{1}{\\sqrt n} w_{old} \\end{align} \\] 所以要使用\\(\\frac{1}{\\sqrt{n}}\\)来标准化参数： 1W = 0.01 * np.random.randn(D,H)/ sqrt(n) 经验公式 对于某一层的方差，应该取决于两层的输入和输出神经元的数量，如下： \\[ \\rm{D}(w) = \\frac{2}{n_{in} + n_{out}} \\] ReLU来说，方差应该是\\(\\frac{2}{n}\\) 1W = 0.01 * np.random.randn(D,H) * sqrt(2.0 / n) 稀疏和偏置初始化 一般稀疏初始化用的比较少。一般偏置都初始化为0。 Batch Normalization 莫凡python BN讲解 和 CSDN-BN论文介绍 。Batch Normalization和普通数据标准化类似，是将分散的数据标准化。 Batch Normalization在神经网络非常流行，已经成为一个标准了。 训练速度分析 网络训练的时候，每一层网络参数更新，会导致下一层输入数据分布的变化。这个称为Internal Convariate Shift。 需要对数据归一化的原因 ： 神经网络的本质是学习数据分布。如果训练数据与测试数据的分布不同，那么泛化能力也大大降低 如果每个batch数据分布不同（batch 梯度下降），每次迭代都要去学习适应不同的分布，会大大降低训练速度 深度网络，前几层数据微小变化，后面几层数据差距会积累放大。 一旦某一层网络输入数据发生改变，这层网络就需要去适应学习这个新的数据分布。如果训练数据的分布一直变化，那么就会影响网络的训练速度。 敏感度问题 神经网络中，如果使用tanh激活函数，初始权值是0.1。 输入\\(x=1\\)， 正常更新： \\[ z = wx = 0.1, \\quad a(z_1) = 0.1 \\quad \\to \\quad a^\\prime(z) = 0.99 \\] 但是如果一开始输入 \\(x=20\\) ，会导致梯度消失，不更新参数。 \\[ z = wx = 2 ,\\quad a(z) \\approx 1 \\quad \\to \\quad a^\\prime(z) = 0 \\] 同样地，如果再输入\\(x=100\\) ，神经元的输出依然是接近于1，不更新参数。 \\[ z = wx = 10 ,\\quad a(z) \\approx 1 \\quad \\to \\quad a^\\prime(z) = 0 \\] 对于一个变化范围比较大特征维度，神经网络在初始阶段对它已经不敏感没有区分度了！ 这样的问题，在神经网络的输入层和中间层都存在。 BN算法 BN算法在每一次迭代中，对每一层的输入都进行归一化。把数据转换为均值为0、方差为1的高斯分布。 \\[ \\hat x = \\frac{x - E(x)} {\\sqrt{D(x) + \\epsilon}} \\] 非常大的缺陷：强行归一化会破坏掉刚刚学习到的特征。 把每层的数据分布都固定了，但不一定是前面一层学习到的数据分布。 牛逼的地方 ：设置两个可以学习的变量扩展参数\\(\\gamma\\) ，和平移参数 \\(\\beta\\) ，用这两个变量去还原上一层应该学习到的数据分布。（但是芳芳说，这一步其实可能没那么重要，要不要都行，CNN的本身会处理得更好）。 \\[ y = \\gamma \\hat x+ \\beta \\] 这样理解：用这两个参数，让神经网络自己去学习琢磨是前面的标准化是否有优化作用，如果没有优化效果，就用\\(\\gamma, \\beta\\)来抵消标准化的操作。 这样，BN就把原来不固定的数据分布，全部转换为固定的数据分布，而这种数据分布恰恰就是要学习到的分布。从而加速了网络的训练。 对一个mini-batch进行更新， 输入一个\\(batchsize=m\\)的数据，学习两个参数，输出\\(y\\) \\[ \\begin{align} &amp; \\mu = \\frac{1}{m} \\sum_{i=1}^m x_i &amp; \\text{求均值} \\\\ &amp; \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2 &amp; \\text{求方差} \\\\ &amp; \\hat x = \\frac{x - E(x)} {\\sqrt{\\sigma^2 + \\epsilon}} &amp; \\text{标准化} \\\\ &amp; y = \\gamma \\hat x+ \\beta &amp; \\text{scale and shfit} \\end{align} \\] 其实就是对输入数据做个归一化： \\[ z = wx+b \\to z = \\rm{BN}(wx + b) \\to a = f(z) \\] 一般在全连接层和激活函数之间添加BN层。 在测试的时候，由于是没有batch，所以使用固定的均值和标准差，也就是对训练的各个batch的均值和标准差做批处理。 \\[ E(x) = E(\\mu), \\quad D(x) = \\frac{b}{b-1} E(\\sigma^2) \\] BN的优点 1 训练速度快 2 选择大的初始学习率 初始大学习率，学习率的衰减也很快。快速训练收敛。小的学习率也可以。 3 不再需要Dropout BN本身就可以提高网络泛化能力，可以不需要Dropout和L2正则化。源神说，现在主流的网络都没有dropout了。但是会使用L2正则化，比较小的正则化。 4 不再需要局部相应归一化 5 可以把训练数据彻底打乱 效果图片展示 对所有数据标准化到一个范围，这样大部分的激活值都不会饱和，都不是-1或者1。 大部分的激活值在各个分布区间都有值。再传递到后面，数据更有价值。","tags":[{"name":"CS224N","slug":"CS224N","permalink":"http://plmsmile.github.io/tags/CS224N/"},{"name":"正则化","slug":"正则化","permalink":"http://plmsmile.github.io/tags/正则化/"},{"name":"范数","slug":"范数","permalink":"http://plmsmile.github.io/tags/范数/"},{"name":"Dropout","slug":"Dropout","permalink":"http://plmsmile.github.io/tags/Dropout/"},{"name":"数据预处理","slug":"数据预处理","permalink":"http://plmsmile.github.io/tags/数据预处理/"},{"name":"PCA","slug":"PCA","permalink":"http://plmsmile.github.io/tags/PCA/"},{"name":"白化","slug":"白化","permalink":"http://plmsmile.github.io/tags/白化/"},{"name":"BatchNorm","slug":"BatchNorm","permalink":"http://plmsmile.github.io/tags/BatchNorm/"}]},{"title":"cs224n-notes3-神经网络","date":"2017-11-23T04:01:08.000Z","path":"2017/11/23/cs224n-notes3-neural-networks/","text":"神经网络基础 很多数据都是非线性分割的，所以需要一种非线性non-linear决策边界 来分类。神经网络包含很多这样的非线性的决策函数。 神经元 神经元其实就是一个计算单元。 输入向量 \\(x \\in \\mathbb R^n\\) \\(z = w^T x + b\\) \\(a = f(z)\\) 激活函数，sigmoid, relu等，后文有讲。 Sigmoid神经元 传统用sigmoid多，但是现在一定不要使用啦。大多使用Relu作为激活函数。 \\[ z = \\mathbf{w}^T \\mathbf{x} + b , \\; a = \\frac {1}{1 + \\exp (-z)} \\] 网络层 一个网络层有很多个神经元。输入\\(\\mathbf x\\)向量，会传递到多个神经元。如 输入是\\(n\\)维，隐层是\\(m\\)维，有\\(m\\)个神经元。则有 \\[ \\begin{align} &amp; z = W x + b , &amp; W \\in \\mathbb{R}^{m \\times n}, x \\in \\mathbb R^n, b \\in \\mathbb R^m\\\\ &amp; a = f (z) &amp; a \\in \\mathbb R^m\\\\ &amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\\\ \\end{align} \\] 激活函数的意义 每个神经元 输入\\(z = w^Tx+b\\) ：对特征进行加权组合的结果 激活\\(a = f(z)\\)： 对\\(z\\)是否继续保留 最后会把所有的神经元的所有\\(z\\)的激活信息\\(a\\)综合起来，得到最终的分类结果。比如\\(s = U^T a\\)。 前向计算 输入\\(x \\in \\mathbb R^n\\)， 激活信息\\(a \\in \\mathbb R^m\\)。一般前向计算如下： \\[ \\begin{align} &amp; z = W x + b , &amp; W \\in \\mathbb{R}^{m \\times n}, x \\in \\mathbb R^n, b \\in \\mathbb R^m\\\\\\\\ &amp; a = f (z) &amp; a \\in \\mathbb R^m\\\\\\\\ &amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\\\ \\end{align} \\] 下面是一个简单的全连接，最后的圆圈里的1代表等价输出。 NER例子 NER(named-entity recognition)，命名实体识别。对于一个句子Museums in Paris are amazing。 要判断中心单词Paris是否是个命名实体。 既要看window里的所有词向量，也要看这些词的交互关系。比如：Paris出现在in的后面。 因为可能有Paris和Paris Hilton。这就需要non-linear decisions。 如果直接把input给到softmax，是很难获取到非线性决策的。所以需要添加中间层使用神经网络。如上图所示。 维数分析 每个单词4维，输入整个窗口就是20维。在隐层使用8个神经元。计算过程如下，最终得到一个分类的得分。 \\[ \\begin {align} &amp; z = Wx + b \\\\ &amp; a = f(z) \\\\ &amp; s = U^T a \\\\ \\end{align} \\] 维数如下： \\[ x \\in \\mathbb R^{20}, \\; W \\in \\mathbb R^{8\\times20}, \\; U \\in \\mathbb R^{8\\times1}, s \\in R \\] Max magin目标函数 正样本\\(s\\) ：Museums in Paris are amazing ，负样本\\(s_c\\)： Not all museums in Paris 。 只关心：正样本的得分高于负样本的得分， 其它的不关注。即要\\(s - s_c &gt; 0\\)： \\[ \\mathrm{maxmize}(s -s_c) \\leftrightarrow \\mathrm{minmize}(s_c - s) \\] 优化目标函数如下： \\[ \\rm{minimize} \\; J = \\max(s_c - s, 0) \\; = \\begin{cases} &amp; s_c - s, &amp; s &lt; s_c \\\\ &amp; 0, &amp; s \\ge s_c \\end{cases} \\] 上式其实有风险，更需要\\(s - s_c &gt; \\Delta\\)， 即\\(s\\)比\\(s_c\\)得分大于\\(\\Delta\\)，来保证一个安全的间距。 \\[ \\rm{minimize} \\; J = \\max(\\Delta + s_c - s, 0) \\] 给具体间距\\(\\Delta=1\\)， 所以优化目标函数：详情见SVM。 \\[ \\rm{minimize} \\; J = \\max(1 + s_c - s, 0) \\] 其中\\(s_c = U^T f(Wx_c + b), \\; s = U^T f(Wx+b)\\) 。 反向传播训练 梯度下降 ，或者SGD： \\[ \\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\cdot \\Delta_{\\theta^{(t)}} J \\] 反向传播 使用链式法则 来计算前向计算中用到的参数的梯度。 符号定义 如下图，一个简单的网络： 网络在输入层和输出层是等价输入和等价输出，只有中间层会使用激活函数进行非线性变换。 符号 意义 \\(x\\) 网络输入，这里是4维 \\(s\\) 网络输出，这里是1维，即一个数字 \\(W^{(k)}\\) 第\\(k \\to k+1\\)层的转移矩阵。\\(W \\in \\mathbb R^{n \\times m}\\)。 k层m个神经元，k+1层n个神经元 \\(W_{ij}^{(k)}\\) k+1层的\\(i\\) 神经元 到 到\\(k\\)层\\(j\\)神经元的 的权值 \\(b_i^{(k)}\\) \\(k \\to k+1\\) 转移， k+1层的\\(i\\) 神经元的接收偏置 \\(z^{(k)}_j\\) 第\\(k\\)层的第\\(j\\)个神经元的输入 计算输入 \\(z_j^{(k+1)} = \\sum_i W_{ji}^{(k)} \\cdot a^{(k)}_i + b^{(k)}_j\\) \\(a_j^{(k)}\\) 第\\(k\\)层的第\\(j\\)个神经元的输入。\\(a = f(z)\\) \\(\\delta_j^{(k)}\\) BP时，在\\(z_j^{(k)}\\)处的梯度。即\\(f^\\prime(z_j^{(k)}) \\cdot g\\) ，\\(g\\)是传递来的梯度 W梯度推导 误差函数\\(J = \\max (1 + s_c - s, 0)\\) ，当\\(J &gt; 0\\)的时候，\\(J = 1 + s_c - s\\)要去更新参数W和b。 \\[ \\frac{\\partial J} {\\partial s} = - \\frac{\\partial J} {\\partial s_c} = -1 \\] 反向传播时，必须知道参数在前向时所贡献所关联的对象，即知道路径。 这里是等价输出： \\[ s = a_1^{(3)} = z_1^{(3)} = W_1^{(2)}a_1^{(2) } + W_2^{(2)}a_2^{(2) } \\] 这里对\\(W_{ij}^{(1)}\\)的偏导进行反向传播推导： \\[ \\begin{align} \\frac{\\partial s}{\\partial W_{ij}^{(1)}} &amp; = \\frac{\\partial W^{(2)} a^{(2)}}{\\partial W_{ij}^{(1)}} \\\\ &amp;= \\frac{ \\color{blue} {\\partial W_i^{(2)} a_i^{(2)}}} {\\partial W_{ij}^{(1)}} = \\color{blue}{W_i^{(2)}} \\cdot \\frac{\\partial a_i^{(2)}}{\\partial W_{ij}^{(1)}} \\\\ &amp; = W_i^{(2)} \\cdot \\color{blue} {\\frac{\\partial a_i^{(2)}}{\\partial z_i^{(2)}} \\cdot \\frac{\\partial z_i^{(2)}}{\\partial W_{ij}^{(1)}}} \\\\ &amp; = W_i^{(2)} \\cdot \\color{blue}{f^\\prime(z_i^{(2)})} \\cdot \\frac{\\partial }{\\partial W_{ij}^{(1)}} \\left(\\color{blue}{b_i^{(2)} + \\sum_k^4 a_k^{(1)}W_{ik}^{(1)}}\\right) \\\\ &amp; = W_i^{(2)}f^\\prime(z_i^{(2)}) \\color{blue}{a_j^{(1)}} \\\\ &amp; = \\color{blue}{\\delta^{(2)}_i} \\cdot a_j^{(1)} \\end{align} \\] 结果分析 我们知道\\(z_i^{(2)} = \\sum_k^4 a_k^{(1)}W_{ik}^{(1)} + b_i^{(2)}\\)。 单纯\\(z_i^{(2)}\\)对\\(W_{ij}^{(2)}\\)的导数是\\(a_j^{(1)}\\)。反向时，在\\(z_i^{(2)}\\)处的梯度是\\(\\delta_i^{(2)}\\)。 反向时，\\(\\frac{\\partial s}{\\partial W_{ij}^{(1)}} = \\delta^{(2)}_i \\cdot a_j^{(1)}\\)，是传来的梯度和当前梯度的乘积。这正好应证了反向传播。 传来的梯度也作error signal。 反向过程也是error sharing/distribution。 W元素实例 \\(W_{14}^{(1)}\\) 只直接贡献于\\(z_1^{(2)}\\)和\\(a_1^{(2)}\\) 步骤 梯度 \\(s \\to a_1^{(3)}\\) 梯度\\(g=1\\)。开始为1。 \\(a_1^{(3)} \\to z_1^{(3)}\\) 在\\(z_1^{(3)}\\)处的梯度\\(g = 1 \\cdot 1 = \\delta_1^{(3)}\\) 。\\(local \\; g= 1\\) ，等价变换 \\(z_1^{(3)} \\to a_1^{(2)}\\) \\(g = \\delta_1^{(3)} \\cdot W_1^{(2)} = W_1^{(2)}\\) 。\\(lg = w\\), \\(z=wa+b\\) \\(a_1^{(2)} \\to z_1^{(2)}\\) \\(g = W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) = \\delta_1^{(2)}\\)。 \\(lg=f^\\prime(z_1^{(2)})\\) \\(z_1^{(2)} \\to W_{14}^{(1)}\\) \\(g =W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) \\cdot a_4^{(1)} = \\delta_1^{(2)} \\cdot a_4^{(1)}\\)。 \\(lg = a_4^{(1)}\\) ， 因为\\(z =wa+b\\) \\(z_1^{(2)} \\to b_1^{(1)}\\) \\(g = W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) \\cdot 1 = \\delta_1^{(2)} \\cdot a_4^{(1)}\\)。 \\(lg = 1\\) ， 因为\\(z =wa+b\\) 对于上式的梯度计算，有两种理解方法，通过这两种思路去思考能更深入了解。 链式法则 error sharing and distributed flow approach 梯度反向传播 \\(\\delta_i^{(k)} \\to \\delta_j^{(k-1)}\\) 传播图如下： 但是更多时候，当前层的某个神经元的信息会传播到下一层的多个节点上，如下图： 梯度推导公式如下： \\[ \\begin{align} &amp; g_w = \\delta_i^{(k)} \\cdot a_j^{(k-1)} &amp; W_{ij}^{(k-1)}的梯度\\\\\\\\ &amp; g_a = \\sum_i \\delta_i^{(k)}W_{ij}^{(k-1)} &amp; a_j^{(k-1)}的梯度 \\\\\\\\ &amp; g_z = \\delta_j^{(k-1)} = f^\\prime(z_j^{(k-1)}) \\cdot \\sum_i \\delta_i^{(k)}W_{ij}^{(k-1)} &amp; z_j^{(k-1)}的梯度 \\\\\\\\ \\end{align} \\] BP向量化 很明显，不能一个一个参数地去更新element-wise。所以需要用矩阵和向量去表达，去一次性全部更新matrix-vector level。 梯度计算， \\(W_{ij}^{(k)}\\)的梯度是\\(\\delta_i^{(k+1)} \\cdot a_j^{(k)}\\) 。向量表达如下： \\[ \\Delta _{W^{(k)}} = \\begin{bmatrix} \\delta_1^{(k+1)} \\cdot a_1^{(k)} &amp; \\delta_1^{(k+1)} \\cdot a_2^{(k)} &amp; \\cdots\\\\ \\delta_2^{(k+1)} \\cdot a_1^{(k)} &amp; \\delta_2^{(k+1)} \\cdot a_2^{(k)} &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix} = \\delta^{(k+1)} a^{(k)T} \\] 梯度传播，\\(\\delta_j^{(k)} = f^\\prime(z_j^{(k)}) \\cdot \\sum_i \\delta_i^{(k+1)}W_{ij}^{(k)}\\)。向量表达如下： \\[ \\delta^{(k)} = f^\\prime(z^{(k)}) \\circ (\\delta^{(k+1)}W^{(k)}) \\] 其中\\(\\circ\\)是叉积向量积element-wise，是各个位置相乘， 即\\(\\mathbb R^N \\times \\mathbb R^N \\to \\mathbb R^N\\)。 点积和数量积是各个位置相乘求和。 计算效率 很明显，在计算的时候要把上一层的\\(\\delta^{(k+1)}\\)存起来，去计算\\(\\delta^{(k)}\\) ，这样可以减少大量的多余的计算。 神经网络常识 梯度检查 使用导数的定义来估计导数，去和BP算出来的梯度做对比。 \\[ f ^\\prime (\\theta) \\approx \\frac{J(\\theta^{(i+)}) - J(\\theta^{(i-)})} {2 \\epsilon } \\] 由于这样计算非常，效率特别低，所以只用这种办法来检查梯度。具体实现代码见原notes。 激活函数 激活函数有很多，现在主要用ReLu，不要用sigmoid。 用ReLU学习率一定不要设置太大！同一个网络中都使用同一种类型的激活函数。 Sigmoid 数学形式和导数如下： \\[ \\begin{align} &amp; \\sigma (z) = \\frac {1} {1 + \\exp(-z)}, \\; \\sigma(z) \\in (0,1) \\\\ \\\\ &amp; \\sigma^\\prime (z) = \\sigma(z) (1 - \\sigma(z)) \\\\ \\end{align} \\] 图像 优点是具有好的解释性，将实数挤压到\\((0,1)\\)中，很大的负数变成0，很大的正数变成1 。但现在用的已经越来越少了。有下面2个缺点。 Sigmoid会造成梯度消失 靠近0和1两端时，梯度会变成0。 BP链式法则，\\(0 \\times g_{from} = 0\\) ，后面的梯度接近0， 将没有信息去更新参数。 初始化权重过大，大部分神经元会饱和，无法更新参数。因为输入值很大，靠近1了。\\(f^\\prime(z) = 0\\)， 没法传播了。 Sigmoid输出不是以0为均值 如果输出\\(x\\)全是正的，\\(z=wx+b\\)， 那么\\(\\frac{\\partial z}{\\partial w} = x\\) 梯度就全是正的 不过一般是batch训练，其实问题也还好 Sigmoid梯度消失的问题最严重。 Tanh 数学公式和导数如下： \\[ \\begin{align} &amp; \\tanh (z) = \\frac{\\exp(z) - \\exp(-z)}{\\exp(z) + \\exp(-z)} = 2 \\sigma(2z) - 1, \\; \\tanh(z) \\in(-1, 1) \\\\ \\\\ &amp; \\tanh^\\prime (z) = 1 - \\tanh^2 (z) \\end{align} \\] 图像： Tanh是Sigmoid的代替，它是0均值的，但是依然存在梯度消失的问题。 ReLU ReLURectified Linear Unit 最近越来越流行，不会对于大值\\(z\\)就导致神经元饱和的问题。在CV取得了很大的成功。 \\[ \\begin{align} &amp; \\rm{rect}(z) = \\max(z, 0) \\\\ \\\\ &amp; \\rm{rect}^\\prime (z) = \\begin{cases} &amp;1, &amp;z &gt; 0 \\\\&amp; 0, &amp; z \\le 0 \\end{cases} \\\\ \\end{align} \\] 其实ReLU是一个关于0的阈值，现在一般都用ReLU： ReLU的优点 加速收敛（6倍）。线性的，不存在梯度消失的问题。一直是1。 计算简单 ReLU的缺点 训练的时候很脆弱 BP时，如果有大梯度经过ReLU，当前在z处的梯度\\(\\delta^{(k+1)} = 1 \\times g_m\\) 就很大 对参数\\(w\\)的梯度 \\(\\Delta_{W^{(k)}} =\\delta^{(k)} a^{(k)T}\\) 也就很大 参数\\(w\\)会更新的特别小 \\(W^{(k)} = W^{(k)} - \\alpha \\cdot \\Delta_{W^{(k)}}\\) 前向时，\\(z =wx+b \\le 0\\) 也就特别小，激活函数就不会激活 不激活，梯度就为0。 再BP的时候，就无法更新参数了 总结也就是：大梯度\\(\\to\\)小参数\\(w\\) ，新小$z = wx+b $ ReLU不激活， 不激活梯度为0 \\(\\to\\) 不更新参数w了。 当然可以使用比较小的学习率来解决这个问题。 Maxout maxout 有ReLU的优点，同时避免了它的缺点。但是maxout加倍了模型的参数，导致了模型的存储变大。 \\[ \\begin{align} &amp; \\rm{mo}(x) = \\max(w_1x+b_1, w_2x+b_2) \\\\ \\\\ &amp; \\rm{mo}^\\prime (x) = \\begin{cases} &amp;w_1, &amp;w_1x+b_1 大 \\\\&amp; w_2, &amp; 其它 \\\\\\end{cases} \\\\ \\end{align} \\]","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"CS224N","slug":"CS224N","permalink":"http://plmsmile.github.io/tags/CS224N/"},{"name":"反向传播","slug":"反向传播","permalink":"http://plmsmile.github.io/tags/反向传播/"},{"name":"激活函数","slug":"激活函数","permalink":"http://plmsmile.github.io/tags/激活函数/"}]},{"title":"CS224n笔记1-Word2Vec","date":"2017-11-12T12:54:37.000Z","path":"2017/11/12/cs224n-notes1-word2vec/","text":"Word2vec 简介 把词汇变成词向量。 类别1 类别2 算法 CBOW，上下文预测中心词汇 Skip-gram，中心词汇预测上下文 训练方法 负采样 哈夫曼树 语言模型 两种句子： 正常的句子：The cat jumped over the puddle。 概率高，有意义。 没意义的句子：stock boil fish is toy 。概率低，没意义。 二元模型 一个句子，有\\(n\\)个单词。每个词出现的概率由上一个词语来决定。则整体句子的概率如下表示： \\[ P(w_1, w_2, \\cdots, w_n) = \\prod_{i=2}^n P(w_i \\mid w_{i-1}) \\] 缺点 只考虑单词相邻传递概率，而忽略句子整体的可能性。 context size=1，只学了相邻单词对的概率 会计算整个大数据集的全局信息 CBOW 给上下文The cat _ over the puddle，预测jump 。对于每个单词，学习两个向量： \\(v\\) ：输入向量 ，（上下文单词） \\(u\\)： 输出向量 ， （中心单词） 符号说明 \\(V\\) ：词汇表，后面用\\(V\\)代替词汇表单词个数 \\(w_i\\) ：词汇表中第\\(i\\)个单词 \\(d\\) ：向量的维数 \\(\\mathcal V_{d \\times |V|}\\)：输入矩阵，也可以用\\(W\\)来表达 \\(v_i\\) ：\\(\\mathcal{V}\\)的第\\(i\\)列，\\(w_i\\)的输入向量表达 \\(\\mathcal {U}_{|V| \\times d}\\) ：输出矩阵，可以用\\(W ^ \\prime\\)来表达 \\(u_i\\) ：\\(\\mathcal U\\)的第i行， \\(w_i\\)的输出向量表达 输入与输出 \\(x^{(c)}\\)， 输入\\(2m\\)个上下文单词，上下文词汇的one-hot向量 \\(y_c\\)： 真实标签 \\(\\hat y^{(c)}\\)， 输出一个中心单词，中心词汇的one-hot向量 步骤 1 上下文单词onehot向量 one-hot向量的表达：\\((x^{(c-m)}, \\cdots, x^{(c-1)}, x^{(c+1)}, x^{(c+m)} \\in \\mathbb R^V)\\) 2 上下文单词向量 \\((v_{c-m}, v_{c-m+1}, \\cdots. v_{c+m} \\in \\mathbb{R}^d)\\)， 其中，\\(v_{c-m}=\\mathcal V x^{(c-m)}\\)， 即输入矩阵乘以one-hot向量就找到所在的列 3 平均上下文词向量 \\(\\hat v = \\frac {v_{c-m} + \\cdots + v_{c+m}}{2m} \\in \\mathbb R^d\\) 4 输出单词与上下文计算得分向量 \\(z = \\mathcal U \\hat v \\in \\mathbb R ^V\\) 。点积，单词越相似，得分越高 5 得分向量转为概率 $y = (z) R^V $ 6 真实预测概率对比 预测的概率向量\\(\\hat y\\)与唯一真实中心单词one-hot向量\\(y\\)，进行交叉熵比较算出loss。 目标函数 使用交叉熵计算loss，损失函数如下： \\[ H(\\hat y, y) = - \\sum_{j=1}^{|V|} y_j \\log (\\hat y_j) \\] 由于中心单词\\(y\\)是one-hot编码，只有正确位置才为1，其余均为0，所以只需计算中心单词对应的位置概率的loss即可： \\[ H(\\hat y, y) = - y_c \\log (\\hat y_c) = - \\log (\\hat y_c) \\] 交叉熵很好是因为 \\(-1 \\cdot \\log (1) = 0\\)，预测得好 \\(-1 \\cdot \\log (0.01) = 4.605\\)， 预测得不好 最终损失函数： \\[ \\begin{align} \\rm{minimize} \\; J &amp; = - \\log P(w_c \\mid w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m}) \\\\ &amp; = - \\log P(u_c \\mid \\hat v) \\\\ &amp; = - \\log \\frac {\\exp(u_c^T \\hat v)}{\\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v)} \\\\ &amp; = -u_c^T \\hat v + \\log \\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v) \\end{align} \\] 再使用SGD方法去更新相关的两种向量\\(u_c, v_j\\) 。 Skip-gram 给中心单词jump，预测上下文The cat _ over the puddle 。 输入中心单词\\(x\\)， 输出上下文单词\\(y\\) 。与CBOW正好输入输出相反，但同样有两个矩阵\\(\\mathcal {U, V}\\) 。符号说明同CBOW。 步骤 1 中心单词onehot向量 \\(x \\in \\mathbb {R}^{|V|}\\) 2 中心单词词向量 \\(v_c = \\mathcal V x \\in \\mathbb R^d\\) 3 中心词与其他词的得分向量 \\(z = \\mathcal U v_c \\in \\mathbb R ^{|V|}\\) 4 得分向量转为概率 概率 \\(\\hat y = \\rm {softmax} (z)\\)， \\(\\hat y_{c-m}, \\ldots, \\hat y_{c+m}\\) 是目标上下文单词是中心单词的上下文的预测概率。 5 预测真实概率对比 预测概率\\(\\hat y\\) 与\\(2m\\) 个真实上下文onehot向量\\(y_{c-m}, \\ldots, y_{c+m}\\)进行交叉熵对比，算出loss 目标函数 与CBOW不同的是，Skip-gram做了一个朴素贝叶斯条件假设，所有的输出上下文单词都是独立的。 \\[ \\begin {align} \\rm{minimize} \\; J &amp; = - \\log P(w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m} \\mid w_c) \\\\ &amp; = - \\log \\prod_{j=0, j \\neq m}^{2m} P(w_{c-m+j} \\mid w_c) \\\\ &amp; = -\\log \\prod_{j=0, j \\neq m}^{2m} \\frac {\\exp (u_{c-m+j}^T \\cdot v_c)} {\\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c)} \\\\ &amp; = - \\sum_{j=0, j \\neq m}^{2m} \\left ( \\log \\exp (u_{c-m+j}^T \\cdot v_c) - \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\right) \\\\ &amp; = - \\sum_{j=0, j \\neq m}^{2m} u_{c-m+j}^T v_c + 2m \\cdot \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\end {align} \\] 一样，使用SGD去优化U和V。 损失函数实际上是\\(2m\\)个交叉熵求和，求出的向量\\(\\hat y\\)与\\(2m\\)个onehot向量\\(y_{c-m+j}\\) 计算交叉熵： \\[ \\begin {align} J &amp; = - \\sum_{j=0, j \\neq m}^{2m} \\log P(u_{c-m+j} \\mid v_c) \\\\ &amp; = \\sum_{j=0, j \\neq m}^{2m} H(\\hat y, y_{c-m+j}) \\\\ \\end{align} \\] 负采样训练 每次计算都会算整个\\(|V|\\)词表，太耗时了。 可以从噪声分布\\(P_n(w)\\)中进行负采样，来代替整个词表。当然单词采样概率与其词频相关。只需关心：目标函数、梯度、更新规则。 标签函数 对于一对中心词和上下文单词\\((w, c)\\) ，设标签如下： \\(P(l = 1 \\mid w, c)\\)， \\((w, c)\\) 来自于真实语料 \\(P(l = 0 \\mid w, c)\\) ，\\((w, c)\\)来自于负样本，即不在语料中 用sigmoid表示标签函数： \\[ \\begin {align} &amp; P(l = 1 \\mid w, c; \\theta) = \\sigma (u^T_w v_c) = \\frac {1}{ 1 + e^{-u^T_w v_c}} \\\\ &amp; P(l = 0 \\mid w, c; \\theta) = 1 - \\sigma (u^T_w v_c) = \\frac {1}{ 1 + e^{u^T_w v_c} } \\\\ \\end {align} \\] 目标函数 选取合适的\\(\\theta= \\mathcal {U, V}\\) ，去增大正样本的概率，减小负样本的概率。设\\(D\\)为正样本集合，\\(\\bar D\\)为负样本集合。 \\[ \\begin {align} \\theta &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\prod_{(w, c) \\in D} P(l=1 \\mid w, c, \\theta) \\prod_{(w, c) \\in \\bar D} P(l=0 \\mid w, c, \\theta) \\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\prod_{(w, c) \\in D} P(l=1 \\mid w, c, \\theta) \\prod_{(w, c) \\in \\bar D} (1 - P(l=1 \\mid w, c, \\theta) )\\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log P(l=1 \\mid w, c, \\theta) + \\sum_{(w, c) \\in \\bar D} \\log (1 - P(l=1 \\mid w, c, \\theta) )\\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log \\frac {1}{ 1 + \\exp (-u^T_w v_c)}+ \\sum_{(w, c) \\in \\bar D} \\log \\frac {1}{ 1 + \\exp (u^T_w v_c) } \\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log \\sigma(u^T_w v_c) + \\sum_{(w, c) \\in \\bar D} \\log \\sigma (-u^T_w v_c) \\end {align} \\] 最大化概率也就是最小化负对数似然 \\[ J = - \\sum_{(w, c) \\in D} \\log \\sigma(u^T_w v_c) - \\sum_{(w, c) \\in \\bar D} \\log \\sigma (-u^T_w v_c) \\] 负采样集合选择 为中心单词\\(w_c\\) 从\\(P_n(w)\\) 采样\\(K\\)个假的上下文单词。表示为\\(\\{ \\bar u_k \\mid k=1\\ldots K\\}\\) CBOW 给上下文向量\\(\\hat{v}=\\frac {v_{c-m} + \\cdots + v_{c+m}}{2m}\\) 和真实中心词\\(u_c\\) 原始loss \\[ J = -u_c^T \\hat v + \\log \\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v) \\] 负采样loss \\[ J = - \\log \\sigma (u_c^T \\cdot \\hat v) - \\sum_{k=1}^K \\log \\sigma (- \\bar u_k^T \\cdot \\hat v) \\] Skip-gram 给中心单词\\(v_c\\)， 和\\(2m\\)个真实上下文单词\\(u_{c-m+j}\\) 原始loss \\[ J = - \\sum_{j=0, j \\neq m}^{2m} u_{c-m+j}^T v_c + 2m \\cdot \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\] 负采样loss \\[ J = - \\sum_{j=0, j \\neq m}^{2m} \\log \\sigma (u_{c-m+j}^T \\cdot v_c) - \\sum_{k=1}^K \\log \\sigma (-\\bar u_{k}^T \\cdot v_c) \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"},{"name":"CS224N","slug":"CS224N","permalink":"http://plmsmile.github.io/tags/CS224N/"}]},{"title":"word2vec中的数学模型","date":"2017-11-02T13:53:49.000Z","path":"2017/11/02/word2vec-math/","text":"word2vec有两种模型：CBOW和Skip-gram，有两种训练方法：Hierarchical Softmax和Negative Sampling 背景介绍 符号 \\(C\\) ：语料Corpus，所有的文本内容，包含重复的词。 D：词典，D是从C中取出来的，不重复。 \\(w\\)：一个词语 \\(m\\)：窗口大小，词语\\(w\\)的前后\\(m\\)个词语 \\(\\rm{Context(w)} = C_w\\)： 词\\(w\\)的上下文词汇，取决于\\(m\\) \\(v(w)\\)： 词典\\(D\\)中单词\\(w\\)的词向量 \\(k\\)：词向量的长度 \\(i_w\\)：词语\\(w\\)在词典\\(D\\)中的下标 \\(NEG(w)\\) ： 词\\(w\\)的负样本子集 常用公式： \\[ \\begin {align} &amp; \\log (a^n b^m) = \\log a^n + \\log b^m = n \\log a + m \\log b \\\\ \\end{align} \\] \\[ \\log \\prod_{i=1}a^i b^{1-i} = \\sum_{i=1} \\log a^i + \\log b^{1-i} = \\sum_{i=1} i \\cdot \\log a + (i-1) \\cdot \\log b \\] 目标函数 n-gram模型。当然，我们使用神经概率语言模型。 \\(P(w \\mid C_w)\\) 表示上下文词汇推出中心单词\\(w\\)的概率。 对于统计语言模型来说，一般利用最大似然，把目标函数设为： \\[ \\prod_{w \\in C} p(w \\mid C_w) \\] 一般使用最大对数似然，则目标函数为： \\[ L = \\sum_{w \\in C} \\log p(w \\mid C_w) \\] 其实概率\\(P(w \\mid C_w)\\)是关于\\(w\\)和\\(C_w\\)的函数，其中\\(\\theta\\)是待定参数集，就是要求最优 \\(\\theta^*\\)，来确定函数\\(F\\)： \\[ p(w \\mid C_w) = F(w, C_w; \\; \\theta) \\] 有了函数\\(F\\)以后，就能够直接算出所需要的概率。 而F的构造，就是通过神经网络去实现的。 神经概率语言模型 一个二元对\\((C_w, w)\\)就是一个训练样本。神经网络结构如下，\\(W, U\\)是权值矩阵，\\(p, q\\)是对应的偏置。 但是一般会减少一层，如下图：（其实是去掉了隐藏层，保留了投影层，是一样的） 窗口大小是\\(m\\)，\\(\\rm{Context}(w)\\)包含\\(2m\\)个词汇，词向量长度是\\(k\\)。可以做拼接或者求和（下文是）。拼接得到长向量\\(2mk\\)， 在投影层得到\\(\\mathbf{x_w}\\)，然后给到隐藏层和输出层进行计算。 \\[ \\mathbf{z}_w = \\rm{tanh}(W\\mathbf{z}_w + \\mathbf{p}) \\;\\to \\;\\mathbf{y}_w = U \\mathbf{z}_w + \\mathbf{q} \\] 再对\\(\\mathbf{y}_w = (y_1, y_2, \\cdots, y_K)\\) 向量进行softmax即可得到所求得中心词汇的概率： \\[ p(w \\mid C_w) = \\frac{e^{y_{i_w}}}{\\sum_{i=1}^K e^{y_i}} \\] 优点 词语的相似性可以通过词向量来体现 自带平滑功能。N-Gram需要自己进行平滑。 词向量的理解 有两种词向量，一种是one-hot representation，另一种是Distributed Representation。one-hot太长了，所以DR中把词映射成为相对短的向量。不再是只有1个1（孤注一掷），而是向量分布于每一维中（风险平摊）。再利用欧式距离就可以算出词向量之间的相似度。 传统可以通过LSA（Latent Semantic Analysis）和LDA（Latent Dirichlet Allocation）来获得词向量，现在也可以用神经网络算法来获得。 可以把一个词向量空间向另一个词向量空间进行映射，就可以实现翻译。 Hierarchical Softmax 两种模型都是基于下面三层模式（无隐藏层），输入层、投影层和输出层。没有hidden的原因是据说是因为计算太多了。 CBOW和Skip-gram模型： CBOW模型 一共有\\(\\left| C \\right|\\)个单词。CBOW是基于上下文\\(context(w) = c_w\\)去预测目标单词\\(w\\)，求条件概率\\(p(w \\mid c_w)\\)，语言模型一般取目标函数为对数似然函数： \\[ L = \\sum_{w \\in C} \\log p(w \\mid c_w) \\] 窗口大小设为\\(m\\)，则\\(c_w\\)是\\(w\\)的前后m个单词。 输入层 是上下文单词的词向量。（初始随机，训练过程中逐渐更新） 投影层 就是对上下文词向量进行求和，向量加法。得到单词\\(w\\)的所有上下文词\\(c_w\\)的词向量的和\\(\\mathbf{x}_w\\)，待会儿参数更新的时候再依次更新回来。 输出层 从\\(C\\)中选择一个词语，实际上是多分类。这里是哈夫曼树层次softmax。 因为词语太多，用softmax太慢了。多分类实际上是多个二分类组成的，比如SVM二叉树分类： 这是一种二叉树结构，应用到word2vec中，被称为Hierarchical Softmax。CBOW完整结构如下： 每个叶子节点代表一个词语\\(w\\)，每个词语被01唯一编码。 哈夫曼编码 哈夫曼树很简单。每次从许多节点中，选择权值最小的两个合并，根节点为合并值；依次循环，直到只剩一棵树。 比如“我 喜欢 看 巴西 足球 世界杯”，这6个词语，出现的次数依次是15, 8, 6, 5, 3, 1。建立得到哈夫曼树，并且得到哈夫曼编码，如下： CBOW足球例子 引入一些符号： \\(p^w\\) ：从根节点到达\\(w\\)叶子节点的路径 \\(l^w\\) ： 路径\\(p^w\\)中节点的个数 \\(p^w_1, \\cdots, p^w_{l_w}\\) ：依次代表路径中的节点，根节点-中间节点-叶子节点 \\(d^w_2, \\cdots, d^w_{l^w} \\in \\{0, 1\\}\\)：词\\(w\\)的哈夫曼编码，由\\(l^w-1\\)位构成， 根节点无需编码 \\(\\theta_1^w, \\cdots, \\theta^w_{l^w -1}\\)：路径中非叶子节点对应的向量， 用于辅助计算。 单词\\(w\\)是足球，对应的所有上下文词汇是\\(c_w\\)， 上下文词向量的和是\\(\\mathbf{x}_w\\) 看一个例子： 约定编码为1是负类，为0是正类。即左边是负类，右边是正类。 每一个节点就是一个二分类器，是逻辑回归(sigmoid)。其中\\(\\theta\\)是对应的非叶子节点的向量，一个节点被分为正类和负类的概率分别如下： \\[ \\sigma(\\mathbf{x}_w^T \\theta) = \\frac {1}{ 1 + e^{-\\mathbf{x}_w^T \\theta}}, \\quad 1 - \\sigma(\\mathbf{x}_w^T \\theta) \\] 那么从根节点到达足球的概率是： \\[ p (足球 \\mid c_{足球}) = \\prod_{j=2}^5 p(d_j^w \\mid \\mathbf{x}_w, \\theta_{j-1}^w) \\] CBOW总结 目标函数 从根节点到每一个单词\\(w\\)都存在一条路径\\(p^w\\)，路径上有\\(l^w-1\\)个分支节点，每个节点就是一个二分类，每次产生一个概率 \\(p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1})\\)， 把这些概率乘起来就得到了\\(p(w \\mid c_w)\\)。 其中每个节点的概率是，与各个节点的参数和传入的上下文向量和\\(\\mathbf{x}_w\\)相关。 \\[ p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) = \\begin{cases} &amp; \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}), &amp; d_j^w = 0 \\\\ &amp; 1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}), &amp; d_j^w = 1\\\\ \\end{cases} \\] 写成指数形式是 \\[ p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) = [\\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{1-d_j^w} \\cdot [1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{d_j^w} \\] 则上下文推中间单词的概率，即目标函数： \\[ p(w \\mid c_w) = \\prod_{j=2}^{l^w} p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) \\] 对数似然函数 对目标函数取对数似然函数是： \\[ \\begin{align} L &amp; = \\sum_{w \\in C} \\log p(w \\mid c_w) = \\sum_{w \\in C} \\log \\prod_{j=2}^{l^w} [\\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{1-d_j^w} \\cdot[1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{d_j^w} \\\\ &amp; = \\sum_{w \\in C} \\sum_{j=2}^{l^w} \\left( (1-d_j^w) \\cdot \\log \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}) + d_j^w \\cdot \\log (1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})) \\right) \\\\ &amp; = \\sum_{w \\in C} \\sum_{j=2}^{l^w} \\left( (1-d_j^w) \\cdot \\log A + d_j^w \\cdot \\log (1 -A)) \\right) \\end{align} \\] 简写： \\[ \\begin{align} &amp; L(w, j) = (1-d_j^w) \\cdot \\log \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}) + d_j^w \\cdot \\log (1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})) \\\\ &amp; L = \\sum_{w,j} L(w, j) \\end{align} \\] 怎样最大化对数似然函数呢，可以最大化每一项，或者使整体最大化。尽管最大化每一项不一定使整体最大化，但是这里还是使用最大化每一项\\(L(w, j)\\)。 sigmoid函数的求导： \\[ \\sigma ^{\\prime}(x) = \\sigma(x)(1 - \\sigma(x)) \\] \\(L(w, j)\\)有两个参数：输入层的\\(\\mathbf{x}_w\\) 和 每个节点的参数向量\\(\\theta_{j-1}^w\\) 。 分别求偏导并且进行更新参数： \\[ \\begin{align} &amp; \\frac{\\partial}{\\theta_{j-1}^w} L(w, j) = [1 - d_j^w - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})] \\cdot \\mathbf{x}_w \\quad \\to \\quad \\theta_{j-1}^w = \\theta_{j-1}^w + \\alpha \\cdot \\frac{\\partial}{\\theta_{j-1}^w} L(w, j) \\\\ &amp; \\frac{\\partial}{\\mathbf{x}_w} L(w, j) = [1 - d_j^w - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})] \\cdot \\theta_{j-1}^w \\quad \\to \\quad v(\\hat w)+= v(\\hat w) + \\alpha \\cdot \\sum_{j=2}^{l^w} \\frac{\\partial}{\\mathbf{x}_w} L(w, j), \\hat w \\in c_w \\\\ \\end{align} \\] 注意：\\(\\mathbf{x}_w\\)是所有上下文词向量的和，应该把它的更新平均更新到每个上下文词汇中去。\\(\\hat w\\) 代表\\(c_w\\)中的一个词汇。 Skip-Gram模型 Skip-gram模型是根据当前词语，预测上下文。网络结构依然是输入层、投影层(其实无用)、输出层。如下： 输入一个中心单词的词向量\\(v(w)\\)，简记为\\(v_w\\)，输出是一个哈夫曼树。单词\\(u\\)是\\(w\\)的上下文单词\\(c_w\\)中的一个。这是一个词袋模型，每个\\(u\\)是互相独立的。 目标函数 所以\\(c_w\\)是\\(w\\)的上下文词汇的概率是： \\[ p(c_w \\mid w) = \\prod_{u \\in c_w} p(u \\mid w) \\] 与上面同理，\\(p(u \\mid w)\\) 与传入的中心单词向量\\(v(w)\\)和路径上的各个节点相关： \\[ \\begin{align} &amp; p(u \\mid w) = \\prod_{j=2}^{l^w} p(d_j^u \\mid v_w,\\; \\theta^u_{j-1}) \\\\ &amp; p(d_j^u \\mid v_w ,\\; \\theta^u_{j-1} ) = [\\sigma(v_w^T \\theta^u_{j-1})]^{1-d_j^u} \\cdot [1 - \\sigma(v_w^T \\theta^u_{j-1})]^{d_j^u} \\\\ \\end{align} \\] 下文\\(v_w^T \\theta^w_{j-1}\\)简记为\\(v_w \\theta_{j-1}^w\\)，要记得转置向量相乘就可以了。 对数似然函数 \\[ \\begin{align} L &amp; = \\sum_{w \\in C} \\log p(c_w \\mid w) \\\\ &amp; = \\sum_{w \\in C} \\log \\prod_{u \\in c_w} \\prod _{j=2}^{l^w} [\\sigma(v_w^T \\theta^u_{j-1})]^{1-d_j^u} \\cdot [1 - \\sigma(v_w^T \\theta^u_{j-1})]^{d_j^u} \\\\ &amp; = \\sum_{w \\in C} \\sum_{u \\in c_w} \\sum_{j=2}^{l^w} \\left( (1-d_j^u) \\cdot \\log \\sigma(v_w^T \\theta^u_{j-1}) + d_j^u \\cdot \\log (1 - \\sigma(v_w^T \\theta^u_{j-1})) \\right) \\\\ \\end{align} \\] 同样，简写每一项为\\(L(w, u, j)\\) \\[ L(w, u, j) = (1-d_j^u) \\cdot \\log \\sigma(v_w^T \\theta^u_{j-1}) + d_j^u \\cdot \\log (1 - \\sigma(v_w^T \\theta^u_{j-1})) \\] 然后就是，分别对\\(v_w\\)和\\(\\theta_{j-1}^u\\)求梯度更新即可，同上面的类似。得到下面的更新公式 \\[ \\begin{align} &amp; \\theta_{j-1}^u = \\theta_{j-1}^u + \\alpha \\cdot [1 - d_j^u - \\sigma(v_w^t \\cdot \\theta_{j-1}^u)] \\cdot v(w) \\\\ &amp; v_w = v_w + \\alpha \\cdot \\sum_{u \\in c_w} \\sum_{j=2}^{l^w} \\frac{\\partial L(w, u, j)}{\\partial v_w} \\\\ \\end{align} \\] Negative Sampling 背景知识介绍 Negative Sampling简称NEG，是Noise Contrastive Estimation(NCE)的一个简化版本，目的是用来提高训练速度和改善所得词向量的质量。 NEG不使用复杂的哈夫曼树，而是使用随机负采样，大幅度提高性能，是Hierarchical Softmax的一个替代。 NCE 细节有点复杂，本质上是利用已知的概率密度函数来估计未知的概率密度函数。简单来说，如果已知概率密度X，未知Y，如果知道X和Y的关系，Y也就求出来了。 在训练的时候，需要给正例和负例。Hierarchical Softmax是把负例放在二叉树的根节点上，而NEG，是随机挑选一些负例。 CBOW 对于一个单词\\(w\\)，输入上下文\\(\\rm{Context}(w) = C_w\\)，输出单词\\(w\\)。那么词\\(w\\)是正样本，其他词都是负样本。 负样本很多，该怎么选择呢？后面再说。 定义\\(\\rm{Context}(w)\\)的负样本子集\\(\\rm{NEG}(w)\\)。对于样本\\((C_w, w)\\)，\\(\\mathbf{x}_w\\)依然是\\(C_w\\)的词向量之和。\\(\\theta_u\\)为词\\(u\\)的一个（辅助）向量，待训练参数。 设集合\\(S_w = w \\bigcup NEG(w)\\) ，对所有的单词\\(u \\in S_w\\)，有标签函数： \\[ b^w(u) = \\begin{cases} &amp; 1, &amp; u = w \\\\ &amp; 0, &amp; u \\neq w \\\\ \\end{cases} \\] 单词\\(u\\)是\\(C_w\\) 的中心词的概率是： \\[ p(u \\mid C_w) = \\begin{cases} &amp; \\sigma(\\mathbf x_w^T \\theta^u), &amp; u=w \\; \\text{正样本} \\\\ &amp; 1 - \\sigma(\\mathbf x_w^T \\theta^u), &amp; u \\neq w \\; \\text{负样本} \\\\ \\end{cases} \\] 简写为： \\[ \\color{blue} {p(u \\mid C_w)} = [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\] 要最大化目标函数\\(g(w) = \\sum_{u \\in S_w} p(u \\mid C_w)\\)： \\[ \\begin{align} \\color{blue}{g(w) } &amp; = \\prod_{u \\in S_w} [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\\\ &amp;= \\color{blue} {\\sigma(\\mathbf x_w^T \\theta^u) \\prod_{u \\in NEG(w)} (1 - \\sigma(\\mathbf x_w^T \\theta^u)) } \\\\ \\end{align} \\] 观察\\(g(w)\\)可知，最大化就是要：增大正样本概率和减小化负样本概率。 每个词都是这样，对于整个语料库的所有词汇，将\\(g\\)累计得到优化目标，目标函数如下： \\[ \\begin {align} L &amp; = \\log \\prod_{w \\in C}g(w) = \\sum_{w \\in C} \\log g(w) \\\\ &amp; = \\sum_{w \\in C} \\log \\left( \\prod_{u \\in S_w} [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\right) \\\\ &amp;= \\sum_{w \\in C} \\sum_{u \\in S_w} \\left[ b^w_u \\cdot \\sigma(\\mathbf x_w^T \\theta^u) + (1-b^w_u) \\cdot (1 - \\sigma(\\mathbf x_w^T \\theta^u)) \\right] \\end {align} \\] 简写每一步\\(L(w, u)\\)： \\[ L(w, u) = b^w_u \\cdot \\sigma(\\mathbf x_w^T \\theta^u) + (1-b^w_u) \\cdot (1 - \\sigma(\\mathbf x_w^T \\theta^u)) \\] 计算\\(L(w, u)\\)对\\(\\theta^u\\)和\\(\\mathbf{x}_w\\)的梯度进行更新，得到梯度(对称性)： \\[ \\frac{\\partial L(w, u) }{ \\partial \\theta^u} = [b^w(u) - \\sigma(\\mathbf x_w^T \\theta^u)] \\cdot \\mathbf{x}_w, \\quad \\frac{\\partial L(w, u) }{ \\partial \\mathbf{x}_w} = [b^w(u) - \\sigma(\\mathbf x_w^T \\theta^u)] \\cdot \\theta^u \\] 更新每个单词的训练参数\\(\\theta^u\\) ： \\[ \\theta^u = \\theta^u + \\alpha \\cdot \\frac{\\partial L(w, u) }{ \\partial \\theta^u} \\] 对每个单词更新词向量\\(v(u)\\) ： \\[ v(u) = v(u) + \\alpha \\cdot \\sum_{u \\in S_w} \\frac{\\partial L(w, u) }{ \\partial \\mathbf{x}_w} \\] Skip-gram H给单词\\(w\\)，预测上下文向量\\(\\rm{Context}(w) = C_w\\)。 输入样本\\((w, C_w)\\)。 中心单词是\\(w\\)，遍历样本中的上下文单词\\(w_o \\in C_w\\)，为每个上下文单词\\(w_o\\)生成一个包含负采样的集合\\(S_o = w \\bigcup \\rm{NEG}(o)\\) 。即\\(S_o\\)里面只有\\(w\\)才是\\(o\\)的中心单词。 下面\\(w_o\\)简写为\\(o\\)，要注意实际上是当前中心单词\\(w\\)的上下文单词。 \\(S_o\\)中的\\(u\\)是实际的w就为1，否则为0。标签函数如下： \\[ b^w(u) = \\begin{cases} &amp; 1, &amp; u = w \\\\ &amp; 0, &amp; u \\neq w \\\\ \\end{cases} \\] \\(S_o​\\)中的\\(u​\\)是\\(o​\\)的中心词的概率是 \\[ p(u \\mid o) = \\begin{cases} &amp; \\sigma (v_o^T \\theta^u ), &amp; u=w \\; \\leftrightarrow \\; b^w(u) = 1 \\\\ &amp; 1 - \\sigma (v_o^T \\theta^u ), &amp;u \\neq w \\; \\leftrightarrow \\; b^w(u) = 0 \\\\ \\end{cases} \\] 简写为 \\[ \\color{blue} {p(u \\mid o)} = [ \\sigma(v_o^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(v_o^T \\theta^u)]^{1 - b^w(u)} \\] 对于\\(w\\)的一个上下文单词\\(o\\)来说，要最大化这个概率： \\[ \\prod_{u \\in S_o} p(u \\mid o ) \\] 对于\\(w\\)的所有上下文单词\\(C_w\\)来说，要最大化： \\[ g(w) = \\prod_{o \\in C_w} \\prod_{u \\in S_o} p(u \\mid o ) \\] 那么，对于整个预料，要最大化： \\[ G = \\prod_{w \\in C} g(w) =\\prod_{w \\in C} \\prod_{o \\in C_w} \\prod_{u \\in S_o} p(u \\mid o ) \\] 对G取对数，最终的目标函数就是： \\[ \\begin {align} L &amp; = \\log G = \\sum_{w \\in C} \\sum_{o \\in C_w} \\log \\prod_{u \\in S_o} p(u \\mid o ) \\\\ &amp;= \\sum_{w \\in C} \\sum_{o \\in C_w} \\log \\prod_{u \\in S_o} [ \\sigma(v_o^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(v_o^T \\theta^u)]^{1 - b^w(u)} \\\\ &amp; = \\sum_{w \\in C} \\sum_{o \\in C_w} \\sum_{u \\in S_o} \\left ( b^w_u \\cdot \\sigma(v_o^T \\theta^u) + (1 - b^w_u) \\cdot (1-\\sigma(v_o^T \\theta^u)) \\right) \\end{align} \\] 取\\(w, o, u\\)简写L(w, o, u)： \\[ L(w, o, u) = b^w_u \\cdot \\sigma(v_o^T \\theta^u) + (1 - b^w_u) \\cdot (1-\\sigma(v_o^T \\theta^u)) \\] 分别对\\(\\theta^u、v_o\\)求梯度 \\[ \\frac{\\partial L(w, o, u) }{ \\partial \\theta^u} = [b^w_u - \\sigma(v_o^T \\theta^u)] \\cdot v_o, \\quad \\frac{\\partial L(w,o, u) }{ \\partial v_o} = [b^w_u - \\sigma(v_o^T \\theta^u)] \\cdot \\theta^u \\] 更新每个单词的训练参数\\(\\theta^u\\) ： \\[ \\theta^u = \\theta^u + \\alpha \\cdot \\frac{\\partial L(w, o,u) }{ \\partial \\theta^u} \\] 对每个单词更新词向量\\(v(o)\\) ： \\[ v(o) = v(o) + \\alpha \\cdot \\sum_{u \\in S_o} \\frac{\\partial L(w, u) }{ \\partial v_o} \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"}]},{"title":"CS224N教程2-Word2Vec","date":"2017-11-02T01:33:45.000Z","path":"2017/11/02/cs224n-lecture2-word2vec/","text":"Word meaning 词意 词的意思就是idea，如下： 词汇本身表达的意义 人通过词汇传达的想法 在写作、艺术中表达的意思 signifier - signified(idea or thing) - denotation 传统离散表达 传统使用分类学去建立一个WordNet，其中包含许多上位词is-a和同义词集等。如下： 上义词 同义词 entity, physical_entity,object, organism, animal full, good; estimable, good, honorable, respectable 离散表达的问题： 丢失了细微差别，比如同义词：adept, expert, good, practiced, proficient, skillful 不能处理新词汇 分类太主观 需要人力去构建和修改 很难去计算词汇相似度 每个单词使用one-hot编码，比如hotel=\\([0, 1, 0, 0, 0]\\)，motel=\\([0, 0, 1, 0, 0]\\)。 当我搜索settle hotel的时候也应该去匹配包含settle motel的文章。 但是我们的查询hotel向量和文章里面的motel向量却是正交的，算不出相似度。 分布相似表达 通过一个单词的上下文去表达这个单词。 You shall know a word by the company it keeps. --- JR. Firth 例如，下面用周围的单词去表达banking ： government debt problems turning into banking crises as has happened in ​ saying that Europe needs unified banking regulation to replace the hodgepodge 稠密词向量 一个单词的意义应该是由它本身的词向量来决定的。这个词向量可以预测出的上下文单词。 比如lingustics的词向量是\\([0.286, 0.792, -0.177, -0.107, 0.109, -0.542, 0.349]\\) 词嵌入思想 构建一个模型，根据中心单词\\(w_t\\)，通过自身词向量，去预测出它的上下文单词。 \\[ p (context \\mid w_t) = \\cdots \\] 损失函数如下，\\(w_{-t}\\)表示\\(w_t\\)的上下文（负号通常表示除了某某之外），如果完美预测，损失函数为0。 \\[ J = 1 - p(w_{-t} \\mid w_t) \\] Word2Vec 在每个单词和其上下文之间进行预测。 有两种算法： Skip-grams(SG)： 给目标单词，预测上下文 Continuous Bag of Words(CBOW)：给上下文，预测目标单词 两个稍微高效的训练方法： 分层softmax 负采样 课上只是Naive softmax。两个模型，两种方法，一共有4种实现。这里是word2vec详细信息。 Skip-gram 对于每个单词\\(w_t\\)，会选择一个上下文窗口\\(m\\)。 然后要预测出范围内的上下文单词，使概率\\(P(w_{t+i} \\mid w_t)\\)最大。 目标函数 \\(\\theta\\)是我们要训练的参数，目标函数就是所有位置预测结果的乘积，最大化目标函数： \\[ J^\\prime (\\theta) = \\prod_{t=1}^T \\prod_{-m\\le j \\le m} p(w_{t+j} \\mid w_t ;\\; \\theta), \\quad t \\neq j \\] 一般使用negative log likelihood ：负采样教程。 要最大化目标函数，就得得到损失函数。对于对数似然函数，取其负对数就可以得到损失函数，再最小化损失函数，其中\\(T\\)是文本长度，\\(m\\)是窗口大小： \\[ J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T \\sum_{-m\\le j \\le m} \\log P(w_{t+j} \\mid w_t) \\] Loss 函数 = Cost 函数 = Objective 函数 对于softmax概率分布，一般使用交叉熵作为损失函数 单词\\(w_{t+j}\\)是one-hot编码 negative log probability Word2vec细节 词汇和词向量符号说明： \\(u\\) 上下文词向量，向量是\\(d\\)维的 \\(v\\) 词向量 中心词汇\\(t\\)，对应的向量是\\(v_t\\) 上下文词汇\\(j\\) ，对应的词向量是\\(u_j\\) 一共有\\(V\\)个词汇 计算\\(p(w_{t+j} \\mid w_t)\\)， 即： \\[ p(w_{j} \\mid w_t) = \\mathrm{softmax} (u_j^T \\cdot v_t) = \\frac{\\exp(u_j^T \\cdot v_t)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_t)} \\] 两个单词越相似，点积越大，向量点积如下： \\[ u^T \\cdot v = \\sum_{i=1}^M u_i \\times v_i \\] softmax之所以叫softmax，是因为指数会让大的数越大，小的数越小。类似于max函数。下面是计算的详细信息： 一些理解和解释： \\(w_t\\)是one-hot编码的中心词汇，维数是\\((V, 1)\\) \\(W\\)是词汇表达矩阵，维数是\\((d, V)\\)，一列就是一个单词 \\(Ww_t = v_t\\) 相乘得到词向量\\(v_t\\) ，\\((d, V) \\cdot (V, 1) \\to (d, 1)\\)， 用\\(d\\)维向量去表达了词汇t \\(W^\\prime\\)， \\(W^{\\prime}\\cdot v_t = s\\)，\\((V, d) \\cdot (d, 1) \\to (V, 1)\\) ， 得到 语义相似度向量\\(s\\) 再对\\(s\\)进行softmax即可求得上下文词汇 每个单词有两个向量，作为center单词向量和context单词向量 偏导计算 设\\(o\\)是上下文单词，\\(c\\)是中心单词，条件概率如下： \\[ P(o \\mid c) = \\frac{\\exp(u_o^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\] 这里只计算\\(\\log P\\)对\\(v_c\\)向量的偏导。 用\\(\\mathbf{\\theta}\\)向量表示所有的参数，有\\(V\\)个单词，\\(d\\)维向量。每个单词有2个向量。参数个数一共是\\(2dV\\)个。 向量偏导计算公式，\\(\\mathbf{x, a}\\) 均是向量 \\[ \\frac {\\partial \\mathbf{x}^T \\mathbf{a}} { \\partial \\mathbf{x}} = \\frac {\\partial \\mathbf{a}^T \\mathbf{x}} { \\partial \\mathbf{x}} = \\mathbf{a} \\] 函数偏导计算，链式法则，\\(y=f(u), u=g(x)\\) \\[ \\frac{\\mathrm{d}y}{\\mathrm{d} x} = \\frac{\\mathrm{d}y}{\\mathrm{d} u} \\frac{\\mathrm{d}u}{\\mathrm{d} x} \\] 最小化损失函数： \\[ J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T \\sum_{-m\\le j \\le m} \\log P(w_{t+j} \\mid w_t), \\quad j \\neq m \\] 这里只计算\\(v_c\\)的偏导，先进行分解原式为2个部分： \\[ \\frac { \\partial} {\\partial v_c} \\log P(o \\mid c) = \\frac { \\partial} {\\partial v_c} \\log \\frac{\\exp(u_o^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} = \\underbrace { \\frac { \\partial} {\\partial v_c} \\log \\exp (u_o^T \\cdot v_c) }_{1} - \\underbrace { \\frac { \\partial} {\\partial v_c} \\log \\sum_{i=1}^V \\exp(u_i^T \\cdot v_c) }_{2} \\] 部分1推导 \\[ \\begin{align} \\frac { \\partial} {\\partial v_c} \\color{red}{\\log \\exp (u_o^T \\cdot v_c) } &amp; = \\frac { \\partial} {\\partial v_c} \\color{red}{u_o^T \\cdot v_c} = \\mathbf{u_o} \\end{align} \\] 部分2推导 \\[ \\begin{align} \\frac { \\partial} {\\partial v_c} \\log \\sum_{i=1}^V \\exp(u_i^T \\cdot v_c) &amp; = \\frac{1}{\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\cdot \\color{red}{ \\frac { \\partial} {\\partial v_c} \\sum_{x=1}^V \\exp(u_x^T \\cdot v_c)} \\\\ &amp; = \\frac{1}{A} \\cdot \\sum_{x=1}^V \\color{red} {\\frac { \\partial} {\\partial v_c} \\exp(u_x^T \\cdot v_c)} \\\\ &amp; = \\frac{1}{A} \\cdot \\sum_{x=1}^V \\exp (u_x^T \\cdot v_c) \\color{red} {\\frac { \\partial} {\\partial v_c} u_x^T \\cdot v_c} \\\\ &amp; = \\frac{1}{\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\cdot \\sum_{x=1}^V \\exp (u_x^T \\cdot v_c) \\color{red} {u_x} \\\\ &amp; = \\sum_{x=1}^V \\color{red} { \\frac{\\exp (u_x^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)}} \\cdot u_x \\\\ &amp; = \\sum_{x=1}^V \\color{red} {P(x \\mid c) }\\cdot u_x \\end{align} \\] 所以，综合起来可以求得，单词o是单词c的上下文概率\\(\\log P(o \\mid c)\\) 对center向量\\(v_c\\)的偏导： \\[ \\frac { \\partial} {\\partial v_c} \\log P(o \\mid c) = u_o -\\sum_{x=1}^V P(x \\mid c) \\cdot u_x = \\color{blue} {\\text{观察到的} - \\text{期望的}} \\] 实际上偏导是，单词\\(o\\)的上下文词向量，减去，所有单词\\(x\\)的上下文向量乘以x作为\\(c\\)的上下文向量的概率。 总体梯度计算 在一个window里面，对中间词汇\\(v_c\\)求了梯度， 然后再对各个上下文词汇\\(u_o\\)求梯度。 然后更新这个window里面用到的参数。 比如句子We like learning NLP。设\\(m=1\\)： 中间词汇求梯度 \\(v_{like}\\) 上下文词汇求梯度 \\(u_{we}\\) 和 \\(u_{learning}\\) 更新参数 梯度下降 有了梯度之后，参数减去梯度，就可以朝着最小的方向走了。机器学习梯度下降 \\[ \\theta^{new} = \\theta^{old} - \\alpha \\frac{\\partial}{\\partial \\theta^{old}} J(\\theta), \\quad \\quad \\theta^{new} = \\theta^{old} - \\alpha \\Delta_{\\theta} J(\\theta) \\] 随机梯度下降 预料会有很多个window，因此每次不能更新所有的。只更新每个window的，对于window t： \\[ \\theta^{new} = \\theta^{old} - \\alpha \\Delta_{\\theta} J_t(\\theta) \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"},{"name":"CS224N","slug":"CS224N","permalink":"http://plmsmile.github.io/tags/CS224N/"}]},{"title":"subword-units","date":"2017-10-19T14:16:07.000Z","path":"2017/10/19/subword-units/","text":"Neural Machine Translation of Rare Words with Subword Units 背景 摘要 NMT处理的词汇表是定长的，但是实际翻译却是OOV(out of vocabulary)的。以前是把 新词汇加到词典里去。本文是提出一种subword单元的策略，会把稀有和未知词汇以subword units序列来进行编码，更简单更有效。 会介绍不同分词技术的适用性，包括简单的字符n元模型和基于字节对编码压缩算法的分词技术。也会以经验说明subword模型比传统back-off词典的方法好。 简介 NMT模型的词汇一般是30000-5000，但是翻译却是open-vocabulary的问题。很多语言富有信息创造力，比如凝聚组合等等，翻译系统就需要一种低于word-level的机制。 Word-level NMT的缺点 对于word-level的NMT模型，翻译out-of-vocabulary的单词会回退到dictionary里面去查找。有下面几个缺点 种技术在实际上使这种假设并不成立。比如源单词和目标单词并不是一对一的，你怎么找呢 不能够翻译或者产生未见单词 把unknown单词直接copy到目标句子中，对于人名有时候可以。但是有时候却需要改变形态或者直译。 Subword-level NMT 我们的目标是建立open-vocabulary的翻译模型，不用针对稀有词汇去查字典。事实证明，subword模型效果比传统大词汇表方法更好、更精确。Subword神经网络模型可以从subword表达中学习到组合和直译等能力，也可以有效的产生不在训练数据集中的词汇。本文主要有下面两个贡献 open-vocabulary的问题可以通过对稀有词汇使用subword units单元来编码解决 采用Byte pair encoding (BPE) 算法来进行分割。BPE通过一个固定大小的词汇表来表示开放词汇，这个词汇表里面的是变长的字符串序列。这是一种对于神经网络模型非常合适的词分割策略。 神经机器翻译 NMT是使用的Bahdanau的Attention模型。Encoder是双向RNN，输入\\(X=(x_1, x_2, \\cdots, x_m)\\)，会把两个方向的隐状态串联起来得到annotation向量\\(\\mathbf x\\)。实际上是一个矩阵，对于单个\\(x_j\\)来说，对应的注释向量是\\(\\mathbf{x}_j\\)。 Decoder是一个单向的RNN，预测\\(Y=(y_1, y_2, \\cdots, y_n)\\)。 预测\\(y_i\\) 时，需要： 当前的隐状态\\(s_i\\) 上一时刻的输出\\(y_{i-1}\\)作为当前的输入 语义向量\\(\\mathbf c_i\\) 。语义向量是由所有的注释向量\\(x_j\\) 加权求和得到的。权就是对齐概率\\(\\alpha_{ij}\\)。 即\\(\\mathbf c_i = \\sum_{j=1} ^ m \\alpha_{ij} \\mathbf x_j\\) 详情请看谷歌论文里面的介绍或者Bahdanau的论文。 Subword 翻译 下面词汇的翻译是透明的(transparent，明显的) 命名实体。如果两个语言的字母表相同，可以直接copy到目标句子中去，也可以抄写音译直译等。 同源词和外来词。有着共同的起源，但是不同的语言表达形式不同，所以character-level翻译规则就可以了。 形态复杂的词语。包含多个语素的单词，可以通过单独翻译语素来翻译。 总之，通过subword单元表示稀有词汇对于NMT来说可以学到transparent翻译，并且可以翻译和产生未见词汇。 相关工作 对于SMT(Statistical Machine Translation)来说，翻译未见单词一直是研究的主题。 很多未见单词都是人名，如果两种语言的字母表一样，那么可以直接复制过去。如果不一样，那么就得音译过去。基于字符（character-based）的翻译是比较成功的。 形态上很复杂的单词往往需要分割，这里有很多的分割算法。基于短语的SMT的分割算法是比较保守的。而我们需要积极的细分，让网络可以处理open-vocabulary，而不是去求助于背字典。 怎么选择subword units要看具体的任务。 提出了很多这样的技术：生成基于字符或者基于语素的定长的连续的词向量。于此同时，word-based的方法并没有重大发现。现在的注意力机制还是基于word-level的。我们希望，注意力机制能从我们变长表达中收益：网络可以把注意力放在不同的subword units中。 这可以突破定长表达的信息传达瓶颈。 NMT减少词汇表可以大大节省时间和增加空间效率。我们也想要对一个句子更紧凑的表达。因为文本长度增加了，会减少效率，也会增加模型传递信息的距离。(hidden size？) 权衡词汇表大小和文本长度，可以用未分割单词列表，subword 单元表达的稀有词汇。作为一个代替，Byte pair encoding就是这样的一种分割算法，可以学到一个词汇表，同时对文本有很好的压缩率。 Byte Pair Encoding Byte pair encoding是一种简单的数据压缩技术，它把句子中经常出现的字节pairs用一个没有出现的字节去替代。我们使用这种算法去分割单词，但我们合并字符或者字符序列。 算法步骤 算法步骤如下： 初始化符号词表。用所有的字符加入到符号词表中。对所有单词的末尾加入特殊标记，如-。翻译后恢复原始的标记。 迭代对所有符号进行计数，找出次数最多的(A, B)，用AB代替。 每次合并，会产生一个新的符号，代表着n-gram字符 常见的n-grams字符(或者whole words)，最终会被合并到一个符号 最终符号词表大小=初始大小+合并操作次数。操作次数是算法唯一的超参数。 不用考虑不在训练集里面的pair，为每个word根据出现频率设置权重。 和传统的压缩算法(哈夫曼编码)相比，我们的以subword 单元堆积的符号序列依然是可以解释的，网络也可以翻译和产生新的词汇（训练集没有见过的）。 下面是代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def process_raw_words(words, endtag='-'): '''把单词分割成最小的符号，并且加上结尾符号''' vocabs = &#123;&#125; for word, count in words.items(): # 加上空格 word = re.sub(r'([a-zA-Z])', r' \\1', word) word += ' ' + endtag vocabs[word] = count return vocabsdef get_symbol_pairs(vocabs): ''' 获得词汇中所有的字符pair，连续长度为2，并统计出现次数 Args: vocabs: 单词dict，(word, count)单词的出现次数。单词已经分割为最小的字符 Returns: pairs: ((符号1, 符号2), count) ''' #pairs = collections.defaultdict(int) pairs = dict() for word, freq in vocabs.items(): # 单词里的符号 symbols = word.split() for i in range(len(symbols) - 1): p = (symbols[i], symbols[i + 1]) pairs[p] = pairs.get(p, 0) + freq return pairsdef merge_symbols(symbol_pair, vocabs): '''把vocabs中的所有单词中的'a b'字符串用'ab'替换 Args: symbol_pair: (a, b) 两个符号 vocabs: 用subword(symbol)表示的单词，(word, count)。其中word使用subword空格分割 Returns: vocabs_new: 替换'a b'为'ab'的新词汇表 ''' vocabs_new = &#123;&#125; raw = ' '.join(symbol_pair) merged = ''.join(symbol_pair) # 非字母和数字字符做转义 bigram = re.escape(raw) p = re.compile(r'(?&lt;!\\S)' + bigram + r'(?!\\S)') for word, count in vocabs.items(): word_new = p.sub(merged, word) vocabs_new[word_new] = count return vocabs_newraw_words = &#123;\"low\":5, \"lower\":2, \"newest\":6, \"widest\":3&#125;vocabs = process_raw_words(raw_words)num_merges = 10print (vocabs)for i in range(num_merges): pairs = get_symbol_pairs(vocabs) # 选择出现频率最高的pair symbol_pair = max(pairs, key=pairs.get) vocabs = merge_symbols(symbol_pair, vocabs)print (vocabs) 单独BPE 为目标语言和原语言分别使用BPE去计算词典。从文本和词汇表大小来说更加紧凑，能保证每个subword单元在各自的训练数据上都有。同样的名字在不同的语言中可能切割的不一样，神经网络很难去学习subword units之间的映射。 Joint BPE 为目标语言和原语言一起使用BPE，即联合两种语言的词典去做BPE。提高了源语言和目标语言的分割一致性。训练中一般concat两种语言。 评估 有两个重要问题 subword units表达稀有词汇，是否真的对翻译有效果？ 根据词汇表大小，文本长度，翻译质量，怎样分割才是最好的？ 我们的使用WMT2015的数据，使用BLEU来评判结果。英语-德语： 420万句子对 1亿个token 英语-俄罗斯语： 260万句子对 5000万个token minibatch-size是80，每个epoch都会reshuffle训练数据。训练了7天，每12个小时存一次模型，取最后4个模型再单独训练。分别选择clip 梯度是5.0和1.0，1.0效果好一些。最终是融合了8个模型。 Beam search的大小是12，使用双语词典进行快速对齐，类似于对稀有词汇查找词典，也会用词典去加速训练。 Subword统计 我们的目标是通过一个紧凑的固定大小的subword词典去代表一个open-vocabulary，并且能够有效的训练和解码。 一个简单的基准就是把单词分割成字符n-grams 。n的选择很重要，可以在序列长度(tokens)和词汇表大小(size)之间做一个权衡。序列的长度会增加许多，一个比较好得减少长度的方法就是使用k个最常见的未被分割的词列表。只有unigram(n=1，一元模型)表达才能真正实现open-vocabulary，但是实际上效果却并不好。Bigram效果好，但是不能产生测试集中的tokens。 BPE符合open-vocabulary的目标，并且合并操作可以应用于测试集，去发现未知符号的分割。与字符集模型的主要区别在于，BPE更紧凑的表示较短序列，注意力模型可以应对变长的单元。 分割方法 tokens types unk merge次数 BPE 112 m 63000 0 59500 BPE(joint) 111 m 82000 32 89500 实际上，NMT词汇表中，并不会包含不常见的subword单元，因为里面有很多噪声。 name seg shotlist s-v t-v S-BLEU BLEU CHAR-F3 CHAR-F3 F1 F1 F1 BPE-60k BPE 无 60000 60000 21.5 24.5 52.0 53.9 58.4 40.9 29.3 BPE-J60k BPE(joint) 无 90000 90000 22.8 24.7 51.7 54.1 58.5 41.8 33.6 翻译评估 所有的subword系统都不会去查字典。使用UNK表示模型词典以外的单词，OOV表示训练集里面没有的单词。","tags":[{"name":"NMT","slug":"NMT","permalink":"http://plmsmile.github.io/tags/NMT/"},{"name":"subword","slug":"subword","permalink":"http://plmsmile.github.io/tags/subword/"}]},{"title":"Wordpiece模型","date":"2017-10-19T06:41:00.000Z","path":"2017/10/19/26_wordpieacemodel/","text":"Japanese and Korean Voice Search 看了半天才发现不稳啊。 背景知识 摘要 这篇文章主要讲了构建基于日语和法语的语音搜索系统遇到的困难，并且提出了一些解决的方法。主要是下面几个方面： 处理无限词汇表的技术 在语言模型和词典的书面语中，完全建模并且避免系统复杂度 如何去构建词典、语言和声学模型 展示了由于模糊不清，多个script语言的打分结果的困难性。这些语言语音搜索的发展，大大简化了构建一门新的语言的语音搜索系统的最初的处理过程，这些很多都成为了语言搜索国际化的默认过程。 简介 语音搜索通过手机就可以访问到互联网，这对于一些不好输入字符的语言来说，非常有用。尽管从基础技术来讲，语音识别的技术是在不同的语言之间是非常相似的，但是许多亚洲语言面临的问题，如果只是用传统的英语的方法去对待，这根本很难解决嘛。许多亚洲语言都有非常大的字符库。这让发音词典就很复杂。在解码的时候，由于很多同音异义词汇，解码也会很复杂。基本字符集里面的很多字符都会以多种形式存在，还要数字也会有多种形式，在某些情况下，这都需要适当的标准化。 很多亚洲语言句子中没有空格去分割单词。需要使用segmenters去产生一些词单元。 这些词单元会在词典和语言模型中使用，词单元之间可能需要添加或者删除空白字符。我们开发了一个纯数据驱动的sementers，可以使用任何语言，不需要修改。 还有就是如何去处理英文中的许多词汇，比如URL、数字、日期、姓名、邮件、缩写词汇、标点符号和其它特殊词汇等等。 语音数据收集 公告开放的数据集很难用作商用，有很多限制，所以自己收集数据集。通过手机，从不同的地区、年龄、方言等等，收集数据。一般是尽可能使用这些原始的数据并且建模，而不是转化为书面的数据或者有利于英语的数据。 分词和词库 提出一种WordPieceModel去解决OOV(out-of-vocabulary)的问题。WordPieaceModel通过一种贪心算法，自动地、增量地从大量文本中学得单词单元（word units），一般数量是200k。算法可以，不关注语义，而去最大化训练数据语言模型的可能性，这也是解码过程中的度量标准。该算法可以有效地自动学习词库。 WordPieceModel算法步骤 1 初始化词库 给词库添加基本的所有的unicode字符和ascii字符。日语是22000，韩语是11000。 2 建立模型 基于训练数据，建立模型，使用初始化好的词库。 3 生成新单元 从词库中选择两个词单元组成新的词单元，加入到词库中。组成的新词要使模型的似然函数likelyhood最大。 4 继续加或者停止 如果达到词库数量的上限，或者似然函数增加很小，那么就停止，否则就继续2步，继续合并添加。 算法优化 你也发现了，计算所有可能的Pair这样会非常非常耗费时间。如果当前词库数量是\\(K\\)，那么每次迭代计算的复杂度是\\(O(K^2)\\) 。有下面3个步骤可以进行优化 选择组合新的单元时，只测试训练数据中有的单元。 只测试有很大机会成为最好的Pair，例如high priors 把一些不会影响到彼此的group pairs组合到一起，作为一个单一的迭代过程 only modify the language model counts for the affected entries （不懂什么意思） 使用这些加速算法，我们可以在一个机器上，几个小时以内，从频率加权查询列表中，构建一个200k的词库。 得到wordpiece词库之后，可以用来语言建模，做词典和解码。分割算法，构建了以基础字符开始的Pairs的逆二叉树。本身已经不需要动态规划或者其他的搜索方法。因此在计算上非常有效。分开基本的字符，基于树从上到下，会在线性时间给出一个确定的分割信息，线性时间取决于句子的长度。大约只有4%的单词具有多个发音。如果添加太多的发音会影响性能，可能是因为在训练和解码时对齐过程期间的可能数太多了 继续说明 一般是句子没有空格的，但是有的时候却有空格，比如韩文，搜索关键字。线上系统没有办法去把这些有空格的word pieces组合在一起。这对于常见的词汇和短查询是没有影响的，因为它们已经组合成一个完整的word unit。但是对于一些例如空格出现在不该出现的地方等不常见的查询，就很烦恼了。 在解码的时候，加空格效率更高，采用下面的技术： 1 原始语言模型数据被用来&quot;as written&quot;，表示一些有空格一些没有空格。 2 WPM模型分割LM数据时，每个单元在前面或者后面遇到一个空格，那么就添加一个空格标记。单元有4种情况：两边都有空格，左边有，右边有，两边都没有。使用下划线标记 3 基于这个新词库构建LM和词典 4 解码时，根据模型会选择一个最佳路径，之前在哪些地方放了空格或者没有。为了输出显示，需要把空格全部移除。有3种情况，移除所有空格；移除两个空格用一个空格表示；移除一个空格。","tags":[{"name":"WPM","slug":"WPM","permalink":"http://plmsmile.github.io/tags/WPM/"},{"name":"语音搜索","slug":"语音搜索","permalink":"http://plmsmile.github.io/tags/语音搜索/"},{"name":"语音识别","slug":"语音识别","permalink":"http://plmsmile.github.io/tags/语音识别/"}]},{"title":"循环神经网络","date":"2017-10-18T12:35:29.000Z","path":"2017/10/18/rnn/","text":"LSTM经典描述 经典RNN模型 模型 人类在思考的时候，会从上下文、从过去推断出现在的结果。传统的神经网络无法记住过去的历史信息。 循环神经网络是指随着时间推移，重复发生的结构。它可以记住之前发生的事情，并且推断出后面发生的事情。用于处理时间序列很好。所有的神经元共享权值。如下图所示。 记住短期信息 比如预测“天空中有__”，如果过去的信息“鸟”离当前位置比较近，则RNN可以利用这个信息预测出下一个词为“鸟” 不能长期依赖 如果需要的历史信息距离当前位置很远，则RNN无法学习到过去的信息。这就是不能长期依赖的问题。 LSTM总览与核心结构 总览 所有的RNN有着重复的结构，如下图，比如内部是一个简单的tanh 层。 LSTM也是一样的，只不过内部复杂一些。 单元状态 单元状态像一个传送带，通过整个链向下运行，只有一些小的线性作用。信息就沿着箭头方向流动。 LSTM的门结构 LSTM的门结构 可以添加或者删除单元状态的信息，去有选择地让信息通过。它由sigmoid网络层 和 点乘操作组成。输出属于\\([0, 1]\\)之间，代表着信息通过的比例。 LSTM细节解剖 一些符号说明，都是\\(t\\)时刻的信息 ： \\(C_{t-1}\\) : 的单元状态 \\(h_{t}\\) : 隐状态信息 （也作单个神经元的输出信息） \\(x_t\\) : 输入信息 \\(o_t\\) ：输出信息 （输出特别的信息） 1 遗忘旧信息 对于\\(C_{t-1}\\)中的每一个数字， \\(h_{t-1}\\)和\\(x_t\\)会输出0-1之间的数来决定遗忘\\(C_{t-1}\\)中的多少信息。 2 生成候选状态和它的更新比例 生成新的状态：tanh层创建新的候选状态\\(\\hat{C}_t\\) 输入门：决定新的状态哪些信息会被更新\\(i_t\\)，即候选状态\\(\\hat{C}_t\\)的保留比例。 3 新旧状态合并更新 生成新状态\\(C_t\\)：旧状态\\(C_{t-1}\\) + 候选状态\\(\\hat{C}_t\\)。 旧状态\\(C_{t-1}\\)遗忘不需要的， 候选状态\\(\\hat{C}_{t-1}\\)保留需要更新的，都是以乘积比例形式去遗忘或者更新。 4 输出特别的值 sigmoid：决定单元状态\\(C_t\\)的哪些信息要输出。 tanh: 把单元状态\\(C_t\\)的值变到\\([-1, 1]\\)之间。 LSTM总结 核心结构如下图所示 要忘掉部分旧信息，旧信息\\(C_{t-1}\\)的遗忘比例\\(f_t\\) \\[ f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f) \\] 新的信息来了，生成一个新的候选\\(\\hat{C}_t\\) \\[ \\hat{C}_t = \\tanh (W_C \\cdot [h_{t-1}, x_t] + b_C) \\] 新信息留多少呢，新候选\\(\\hat C_t\\)的保留比例\\(i_t\\) \\[ i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i) \\] 合并旧信息和新信息，生成新的状态信息\\(C_t\\) \\[ C_t = f_t * C_{t-1} + i_t * \\hat C_t \\] 输出多少呢，单元状态\\(C_t\\)的输出比例\\(o_t\\) \\[ o_t = \\sigma (W_o \\cdot [h_{t-1}, x_t] + b_o) \\] 把\\(C_t\\)化到\\([-1, 1]\\)再根据比例输出 \\[ h_t = o_t * \\tanh(C_t) \\] 图文简介描述LSTM 总体架构 单元架构 流水线架构 数据流动 圆圈叉叉代表着遗忘\\(C_{t-1}\\)的信息。乘以向量来实现，向量各个值在\\([0, 1]\\)之间。 靠近0就代表着遗忘很多，靠近1就代表着保留很多。 框框加号代表着数据的合并。旧信息\\(C_{t-1}\\)和新候选信息\\(\\hat C_t\\)的合并。 合并之后就得到新信息\\(C_t\\)。 遗忘门 上一个LSTM的输出\\(h_{t-1}\\) 和 当前的输入\\(x_t\\)，一起作为遗忘门的输入。 0是偏置\\(b_0\\)， 一起做个合并，再经过sigmoid生成遗忘权值\\(f_t\\)信息， 去遗忘\\(C_{t-1}\\)。 新信息门 新信息门决定着新信息对旧信息的影响力。和遗忘门一样\\(h_{t-1}\\)和\\(x_t\\)作为输入。 sigmoid：生成新信息的保留比例。tanh：生成新的信息。 新旧信息合并 旧信息\\(C_{t-1}\\)和新信息\\(\\hat{C}_t\\)合并，当然分别先过遗忘阀门和更新阀门。 输出特别的值 把新生成的状态信息\\(C_t\\)使用tanh变成\\((-1, 1)\\)之间，然后经过输出阀门进行输出。 LSTM变体 观察口连接 传统LSTM阀门值比例的计算，即更新、遗忘、输出的比例只和\\(h_{t-1}, x_t\\)有关。 观察口连接，把观察到的单元状态也连接sigmoid上，来计算。即遗忘、更新比例和\\(C_{t-1}, h_{t-1}, x_t\\)有关，输出的比例和\\(C_t, h_{t-1}, x_t\\)有关。 组队遗忘 如下图所示，计算好\\(C_{t-1}\\)的遗忘概率\\(i_t\\)后，就不再单独计算新候选\\(\\hat C_t\\)的保留概率\\(i_t\\)。而是直接由1减去遗忘概率得到更新概率。即\\(i_t = 1 - f_t\\)，再去更新。 GRU LSTM有隐状态\\(h_t\\)和输出状态\\(o_t\\)，而GRU只有\\(h_t\\)，即GRU的隐状态和输出状态是一样的，都用\\(h_t\\)表示。 更新门\\(z_t\\)负责候选隐层层\\(\\hat h_t\\)保留的比例， \\(1-z_t\\)负责遗忘旧状态信息\\(h_{t-1}\\)的比例 \\[ z_t = \\sigma (W_z \\cdot [h_{t-1}, x_t]) \\] 候选隐藏层\\(\\hat h_t\\)的计算由\\(h_{t-1}\\)和\\(x_t\\)一起计算得到。所以计算\\(\\hat h_t\\)之前，要先计算\\(h_{t-1}\\)的重置比例。 重置门\\(r_t\\)负责\\(h_{t-1}\\)对于新的候选\\(\\hat h_t\\)的重置比例 \\[ r_t = \\sigma (W_r \\cdot [h_{t-1}, x_t]) \\] 新候选状态\\(\\hat h_t\\)的计算 \\[ \\hat h_t = \\tanh (W \\cdot [r_t * h_{t-1}, x_t]) \\] 最终新状态\\(h_t\\)由\\(h_{t-1}\\)和\\(\\hat h_t\\)计算得到，分别的保留比例是\\(1-z_t\\)和\\(z_t\\) \\[ h_t = (1 - z_t) * h_{t-1} + z_t * \\hat h_t \\]","tags":[{"name":"RNN","slug":"RNN","permalink":"http://plmsmile.github.io/tags/RNN/"},{"name":"LSTM","slug":"LSTM","permalink":"http://plmsmile.github.io/tags/LSTM/"},{"name":"GRU","slug":"GRU","permalink":"http://plmsmile.github.io/tags/GRU/"}]},{"title":"谷歌翻译论文笔记","date":"2017-10-17T05:25:38.000Z","path":"2017/10/17/谷歌翻译论文笔记/","text":"谷歌神经机器翻译系统 简介 神经机器翻译是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点 训练和翻译都太慢了，花费代价很大 缺乏鲁棒性，特别是输入句子包含生僻词汇 精确度和速度也不行 传统NMT缺点 神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。 NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。 NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。 训练和推理速度太慢 训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。 不能有效处理稀有词汇 有两个方法去复制稀有单词： 模仿传统对齐模型去训练1个copy model 使用注意力机制去复制 但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。 不能完整翻译整个句子 不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。 GNMT的模型优点 采用的模型：深层LSTM 、Encoder8层、Decoder8层 。我的LSTM笔记。 各层之间使用残差连接促进梯度流，顶层Enocder到底层Decoder使用注意力连接，提高并行性。 进行翻译推断的时候，使用低精度算法，去加速翻译。 处理稀有词汇：使用sub-word单元，也称作wordpieces方法。把单词划分到有限的sub-word (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。 Beam Search 使用长度规范化和覆盖惩罚。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。 使用强化学习去优化模型， 优化翻译的BLEU 分数。 先进技术 有很多先进的技术来提高NMT，下面这些都有论文的。 利用attention去处理稀有词汇 建立翻译覆盖的机制 多任务和半监督训练，去合并使用更多数据 字符分割的encoder和decoder 使用subword单元处理稀疏的输出 系统架构 系统总览 架构 有3个模块：Encoder，Decoder，Attention。 Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。 Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。 Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。 符号说明 加粗小写代表向量，如\\(\\boldsymbol {v, o_i}\\) 加粗大写，矩阵，如\\(\\boldsymbol {U, W}\\) 和\\(\\mathbf{U, W}\\) 手写体，集合，如\\(\\mathcal{V, F}\\) 大写字母，句子，如\\(X, Y\\) 小写字母，单个符号，如\\(x_1, x_2\\) Encoder 输入句子和目标句子组成一个Pair \\((X, Y)\\)，其中输入句子\\(X = x_1, x_2, \\cdots, x_M\\) ，\\(M\\) 个单词，翻译的输出目标句子\\(Y = y_1, y_2, \\cdots, y_N\\) ，有\\(N\\)个单词。 Encoder其实就是一个转换函数，得到\\(M\\)个长度固定的向量，也就是其中Encoder对各个\\(x_i\\)的编码向量 \\(\\mathbf{x_i}\\) ： \\[ \\mathbf {x_1, x_2, \\cdots, x_M} = \\mathit{EncoderRNN} (x_1, x_2, \\cdots, x_n) \\] 使用链式条件概率可得到翻译概率\\(\\color{blue} {P (Y \\mid X)}\\) ，其中\\(y_0\\)是起始符号\\(SOS\\) 。 \\[ \\begin{align} P(Y \\mid X) &amp; = P(Y \\mid \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ &amp; =\\prod_{i=1}^N P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ \\end{align} \\] Decoder 在翻译\\(y_i\\)的时候， 利用Encoder得到的编码向量\\(\\mathbf{x_i}\\) 和 \\(y_0 \\sim y_{i-1}\\) 来进行计算概率翻译 \\[ P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\] Decoder是由RNN+Softmax构成的。会得到一个隐状态\\(\\mathbf{y_i}\\) 向量，有2个作用： 作为下一个RNN的输入 \\(\\mathbf{y_i}\\)经过softmax得到概率分布， 选出\\(y_i\\) 输出符号 Attention 在之前的文章里有介绍论文 和 通俗理解，其实就是影响力模型。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作对齐概率吧。使用decoder-RNN的输出\\(\\mathbf{y_{t-1}}\\) 向量作为时刻\\(t\\)的输入。 时刻\\(t\\)，给定\\(\\mathbf{y_{t-1}}\\) 有3个符号定义： \\(s_i\\) ： \\(y_t\\)与\\(x_i\\)的得分，在luong论文里面有3种计算方式，分别是dot, general和concat。 \\(p_i\\) ：\\(y_t\\)与\\(x_i\\)的对齐概率，\\((p_1, p_2, \\cdots, p_M)\\) 联合起来就是\\(y_t\\)与\\(X\\)的对齐向量。其实就是对得分softmax。 \\(\\mathbf{a_t}\\) ：带注意力的语义向量。对于所有的\\(x_i\\)，使用\\(y_t\\)与它的对齐概率\\(p_i\\)乘以本身的编码向量\\(\\mathbf{x_i}\\)，得到\\(x_i\\)传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。 整体详细计算的流程，如下面的公式： \\[ \\begin {align} &amp; s_i = \\mathit{AttentionFunction} (\\mathbf{y_{t-1}}, \\mathbf{x_i}), \\quad i \\in [1, M] \\\\ &amp; p_i = \\frac {\\exp (s_i)}{\\sum_{j=1}^M \\exp(s_j)} \\quad i \\in [1, M] \\\\ &amp; \\mathbf{a_t} = \\sum_{i=1}^M p_i \\cdot \\mathbf{x_i} \\quad \\color{blue}{对所有带注意力的x_i的语义求和得总体的语义} \\end{align} \\] 计算打分的函数即\\(\\mathit{AttentionFunction}\\)是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。 系统架构图说明 架构图如下 Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。 训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。 为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。 经验说明 实验结果得到，要想NMT有好效果，Encoder和Decoder的网络层数一定要够深，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。 残差连接 残差网络讲解 。 虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。 根据在中间层和目标之间建立差别的思想，引入残差连接，如下图右边所示。其实就是把之前层的输入和当前的输出合并起来，作为下一层的输入。 一些参数和符号说明，一下均是时刻\\(t\\) \\(\\mathbf{x^i_t}\\) : 第\\(i+1\\)层 \\(\\mathit{LSTM}_{i+1}\\)的输入。 即上标代表LSTM的层数，下标代表时间。 \\(\\mathbf{W} ^i\\) : 第\\(i\\)层LSTM的参数 \\(\\mathbf {h} _t^i\\) : 第\\(i\\)层输出隐状态 \\(\\mathbf {c} ^i_t\\) : 第\\(i\\)层输出单元状态 那么\\(LSTM_i\\)和\\(LSTM_{i+1}\\)是这样交互的。即层层纵向传递输入，时间横向传递隐状态和单元状态。 \\[ \\begin{align} &amp; \\mathbf{c}_t^i, \\mathbf{h}_t^i = LSTM_i(\\mathbf{c}_{t-1}^i, \\mathbf{h}_{t-1}^i, \\mathbf x_{t}^{i-1} ; \\; \\mathbf W^i ) \\\\ &amp; \\mathbf x_t^i = \\mathbf h_t^i \\quad\\quad\\quad\\quad \\color{blue}{普通连接：i+1层输入=i层隐层输出} \\\\ &amp; \\mathbf{x}_t^i = \\mathbf h_t^i + \\mathbf{x}_t^{i-1} \\quad \\color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \\\\ &amp; \\mathbf{c}_t^{i+1}, \\mathbf{h}_t^{i+1} = LSTM_{i+1} (\\mathbf c_{t-1}^{i+1}, \\mathbf {h} _{t-1}^{i+1}, x_{t}^i ; \\; \\mathbf W ^{i+1}) \\\\ \\end{align} \\] 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。 双向Encoder 一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。 这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。 \\(LSTM_f\\)从左到右处理句子，\\(LSTM_b\\)从右到左处理句子。把两个方向的信息\\(\\mathbf{x^f_i}\\)和\\(\\mathbf{x}^b_i\\)concat起来，传递给下一层。 模型并行性 模型很复杂，所以使用模型并行和数据并行，来加速。 数据并行 数据并行很简单，使用大规模分布式深度网络(Downpour SGD) 同时训练\\(n\\)个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，\\(n=10, m=128\\)。 模型并行 除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第\\(i+1\\)层可以提前运行，不必等到第\\(i\\)层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。 并行带来的约束 由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。 分割技巧 一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。 复制策略 有下面几种复制策略 把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名 使用注意力模型，添加特别的注意力 使用一个外部的对齐模型，去处理稀有词汇 使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇 sub-word单元 比如字符，混合单词和字符，更加智能的sub-words。 Wordpiece 模型","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"神经机器翻译","slug":"神经机器翻译","permalink":"http://plmsmile.github.io/tags/神经机器翻译/"}]},{"title":"谷歌翻译论文笔记","date":"2017-10-17T05:25:38.000Z","path":"2017/10/17/25_谷歌翻译论文笔记/","text":"谷歌神经机器翻译系统 简介 神经机器翻译是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点 训练和翻译都太慢了，花费代价很大 缺乏鲁棒性，特别是输入句子包含生僻词汇 精确度和速度也不行 传统NMT缺点 神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。 NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。 NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。 训练和推理速度太慢 训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。 不能有效处理稀有词汇 有两个方法去复制稀有单词： 模仿传统对齐模型去训练1个copy model 使用注意力机制去复制 但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。 不能完整翻译整个句子 不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。 GNMT的模型优点 采用的模型：深层LSTM 、Encoder8层、Decoder8层 。我的LSTM笔记。 各层之间使用残差连接促进梯度流，顶层Enocder到底层Decoder使用注意力连接，提高并行性。 进行翻译推断的时候，使用低精度算法，去加速翻译。 处理稀有词汇：使用sub-word单元，也称作wordpieces方法。把单词划分到有限的sub-word (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。 Beam Search 使用长度规范化和覆盖惩罚。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。 使用强化学习去优化模型， 优化翻译的BLEU 分数。 先进技术 有很多先进的技术来提高NMT，下面这些都有论文的。 利用attention去处理稀有词汇 建立翻译覆盖的机制 多任务和半监督训练，去合并使用更多数据 字符分割的encoder和decoder 使用subword单元处理稀疏的输出 系统架构 系统总览 架构 有3个模块：Encoder，Decoder，Attention。 Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。 Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。 Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。 符号说明 加粗小写代表向量，如\\(\\boldsymbol {v, o_i}\\) 加粗大写，矩阵，如\\(\\boldsymbol {U, W}\\) 和\\(\\mathbf{U, W}\\) 手写体，集合，如\\(\\mathcal{V, F}\\) 大写字母，句子，如\\(X, Y\\) 小写字母，单个符号，如\\(x_1, x_2\\) Encoder 输入句子和目标句子组成一个Pair \\((X, Y)\\)，其中输入句子\\(X = x_1, x_2, \\cdots, x_M\\) ，\\(M\\) 个单词，翻译的输出目标句子\\(Y = y_1, y_2, \\cdots, y_N\\) ，有\\(N\\)个单词。 Encoder其实就是一个转换函数，得到\\(M\\)个长度固定的向量，也就是其中Encoder对各个\\(x_i\\)的编码向量 \\(\\mathbf{x_i}\\) ： \\[ \\mathbf {x_1, x_2, \\cdots, x_M} = \\mathit{EncoderRNN} (x_1, x_2, \\cdots, x_n) \\] 使用链式条件概率可得到翻译概率\\(\\color{blue} {P (Y \\mid X)}\\) ，其中\\(y_0\\)是起始符号\\(SOS\\) 。 \\[ \\begin{align} P(Y \\mid X) &amp; = P(Y \\mid \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ &amp; =\\prod_{i=1}^N P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ \\end{align} \\] Decoder 在翻译\\(y_i\\)的时候， 利用Encoder得到的编码向量\\(\\mathbf{x_i}\\) 和 \\(y_0 \\sim y_{i-1}\\) 来进行计算概率翻译 \\[ P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\] Decoder是由RNN+Softmax构成的。会得到一个隐状态\\(\\mathbf{y_i}\\) 向量，有2个作用： 作为下一个RNN的输入 \\(\\mathbf{y_i}\\)经过softmax得到概率分布， 选出\\(y_i\\) 输出符号 Attention 在之前的文章里有介绍论文 和 通俗理解，其实就是影响力模型。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作对齐概率吧。使用decoder-RNN的输出\\(\\mathbf{y_{t-1}}\\) 向量作为时刻\\(t\\)的输入。 时刻\\(t\\)，给定\\(\\mathbf{y_{t-1}}\\) 有3个符号定义： \\(s_i\\) ： \\(y_t\\)与\\(x_i\\)的得分，在luong论文里面有3种计算方式，分别是dot, general和concat。 \\(p_i\\) ：\\(y_t\\)与\\(x_i\\)的对齐概率，\\((p_1, p_2, \\cdots, p_M)\\) 联合起来就是\\(y_t\\)与\\(X\\)的对齐向量。其实就是对得分softmax。 \\(\\mathbf{a_t}\\) ：带注意力的语义向量。对于所有的\\(x_i\\)，使用\\(y_t\\)与它的对齐概率\\(p_i\\)乘以本身的编码向量\\(\\mathbf{x_i}\\)，得到\\(x_i\\)传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。 整体详细计算的流程，如下面的公式： \\[ \\begin {align} &amp; s_i = \\mathit{AttentionFunction} (\\mathbf{y_{t-1}}, \\mathbf{x_i}), \\quad i \\in [1, M] \\\\ &amp; p_i = \\frac {\\exp (s_i)}{\\sum_{j=1}^M \\exp(s_j)} \\quad i \\in [1, M] \\\\ &amp; \\mathbf{a_t} = \\sum_{i=1}^M p_i \\cdot \\mathbf{x_i} \\quad \\color{blue}{对所有带注意力的x_i的语义求和得总体的语义} \\end{align} \\] 计算打分的函数即\\(\\mathit{AttentionFunction}\\)是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。 系统架构图说明 架构图如下 Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。 训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。 为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。 经验说明 实验结果得到，要想NMT有好效果，Encoder和Decoder的网络层数一定要够深，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。 残差连接 残差网络讲解 。 虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。 根据在中间层和目标之间建立差别的思想，引入残差连接，如下图右边所示。其实就是把之前层的输入和当前的输出合并起来，作为下一层的输入。 一些参数和符号说明，一下均是时刻\\(t\\) \\(\\mathbf{x^i_t}\\) : 第\\(i+1\\)层 \\(\\mathit{LSTM}_{i+1}\\)的输入。 即上标代表LSTM的层数，下标代表时间。 \\(\\mathbf{W} ^i\\) : 第\\(i\\)层LSTM的参数 \\(\\mathbf {h} _t^i\\) : 第\\(i\\)层输出隐状态 \\(\\mathbf {c} ^i_t\\) : 第\\(i\\)层输出单元状态 那么\\(LSTM_i\\)和\\(LSTM_{i+1}\\)是这样交互的。即层层纵向传递输入，时间横向传递隐状态和单元状态。 \\[ \\begin{align} &amp; \\mathbf{c}_t^i, \\mathbf{h}_t^i = LSTM_i(\\mathbf{c}_{t-1}^i, \\mathbf{h}_{t-1}^i, \\mathbf x_{t}^{i-1} ; \\; \\mathbf W^i ) \\\\ &amp; \\mathbf x_t^i = \\mathbf h_t^i \\quad\\quad\\quad\\quad \\color{blue}{普通连接：i+1层输入=i层隐层输出} \\\\ &amp; \\mathbf{x}_t^i = \\mathbf h_t^i + \\mathbf{x}_t^{i-1} \\quad \\color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \\\\ &amp; \\mathbf{c}_t^{i+1}, \\mathbf{h}_t^{i+1} = LSTM_{i+1} (\\mathbf c_{t-1}^{i+1}, \\mathbf {h} _{t-1}^{i+1}, x_{t}^i ; \\; \\mathbf W ^{i+1}) \\\\ \\end{align} \\] 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。 双向Encoder 一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。 这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。 \\(LSTM_f\\)从左到右处理句子，\\(LSTM_b\\)从右到左处理句子。把两个方向的信息\\(\\mathbf{x^f_i}\\)和\\(\\mathbf{x}^b_i\\)concat起来，传递给下一层。 模型并行性 模型很复杂，所以使用模型并行和数据并行，来加速。 数据并行 数据并行很简单，使用大规模分布式深度网络(Downpour SGD) 同时训练\\(n\\)个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，\\(n=10, m=128\\)。 模型并行 除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第\\(i+1\\)层可以提前运行，不必等到第\\(i\\)层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。 并行带来的约束 由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。 分割技巧 一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。 复制策略 有下面几种复制策略 把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名 使用注意力模型，添加特别的注意力 使用一个外部的对齐模型，去处理稀有词汇 使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇 sub-word单元 比如字符，混合单词和字符，更加智能的sub-words。 Wordpiece 模型","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"神经机器翻译","slug":"神经机器翻译","permalink":"http://plmsmile.github.io/tags/神经机器翻译/"}]},{"title":"那些年折磨过的问题","date":"2017-10-16T14:47:46.000Z","path":"2017/10/16/tips/","text":"搭建博客 搭建博客 12345678910111213141516171819202122232425mkdir PLMBlogscd PLMBlogs# install hexonpm install hexo-cli -g# inithexo init npm installhexo server# install pluginsnpm install hexo-deployer-git --savenpm install hexo-renderer-scss --save# 在默认_config.yml中添加需要的插件plugins: hexo-generator-feed #RSS订阅插件 hexo-generator-sitemap #sitemap插件git clone https://github.com/ahonn/hexo-theme-even themes/even# 替换配置文件 or 一步一步地去配置# 生成，再替换文件hexo new page tagshexo new page categories indigo主题 123456789101112131415161718192021222324hexo init# 配置.yml文件，复制旧的过来即可npm install hexo-deployer-git --savegit clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigogit checkout -b card origin/cardnpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --savehexo new page tagshexo new page categorieshexo new page about# 再去配置各个目录下的index文件，也可以直接copy# 配置主题中的yml，直接copy# 修改图片等# 后续 修改宽度# source/css/_partial/variable.css 中第28行，修改为80%的宽度contentWidth: 80%# 主题配置文件中 cdn改为falsecdn: false 搭建indigo博客 安装博客 1234567891011121314hexo initnpm install hexo-deployer-git --save''' 安装 '''git clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigonpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --save''' 配置标签和类别页面，去配置index.md中的数据 '''hexo new page tagshexo new page categories# 主要是配置 layout: tags layout: categories comment: false# 新建关于我的页面，并填上相应的信息 layout: abouthexo new page about 配置hexo/_config.yml中的主题是indigo 12# 配置indigo主题theme: indigo 配置数学公式 1234567891011# 在主题_config.yml 中配置 mathjax: true# 要先卸载已有的渲染器npm uninstall hexo-renderer-marked --save# 潜在的npm uninstall hexo-renderer-kramed --savenpm uninstall hexo-math --save# 只需要安装pandoc就可以了# 先在本地下载pandoc，安装好，再执行如下命令npm install hexo-renderer-pandoc --save 做到这里，要先启动，放两篇带数学公式的文档，去测试一下。 博客配置 编辑hexo/_config.yml ，添加如下项目 1234567891011121314151617181920212223242526272829303132333435363738# Sitetitle: PLM's Blogsubtitle: 好好学习，天天向上description: 菜鸟程序员author: 蒲黎明language: zh-CN# url url: https://plmsmile.github.io/# Deploymentdeploy: type: git repo: git@github.com:plmsmile/plmsmile.github.io.git branch: master# indigo的配置项feed: type: atom path: atom.xml limit: 0jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true indigo主题配置 参考官方配置说明 编辑themes/indigo/_config.yml 配置左侧菜单 12345678910111213141516171819menu: home: text: 主页 url: / archives: text: 归档 url: /archives tags: text: 标签 url: /tags th-list: text: 类别 url: /categories user: url: /about text: 关于我 github: url: https://github.com/plmsmile target: _blank 配置自我介绍 1about: 自然语言处理，机器学习，深度学习，Spark，Leetcode，Java，C++，数据结构。都不会呢，赶紧快学吧！ 设置图片，需要配置站点图片、头像图片，也可以换头像背景图片。 1234567891011# 你的头像urlavatar: /img/avatar.jpg# avatar linkavatar_link: https://plmsmile.github.io/about# 头像背景图brand: /img/brand.jpg# faviconfavicon: /img/favicon.png# emailemail: plmsmile@126.com 配置页面宽度 修改source/css/_partial/variable.css 中的28行，设置为80%。设置主题配置文件中，cdn: false。 、 配置支付宝和微信图片，默认就有打赏功能。可以关掉。 12345''' 是否开启打赏，关闭 reward: false'''reward: title: 谢谢大爷~ wechat: /img/wechat.png '微信，关闭设为 false alipay: /img/alipay.png '支付宝，关闭设为 false 每张文章最后的备注 1postMessage: &lt;br&gt;原始链接：&lt;a href=\"&lt;%- url_for(page.path).replace(/index\\.html$/, '') %&gt;\" target=\"_blank\" rel=\"external\"&gt;&lt;%- page.permalink.replace(/index\\.html$/, '') %&gt;&lt;/a&gt; 关闭动态title 1234# 动态定义title# title_change:# normal: (つェ⊂)咦!又好了!# leave: 死鬼去哪里了！ 配置几个页面的标题 1234# 页面标题tags_title: 标签archives_title: 归档categories_title: 类别 总结 12345678910111213141516hexo init# 1. 运行下面的脚本npm install hexo-deployer-git --savegit clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigonpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --savehexo new page tagshexo new page categorieshexo new page about# 2. 替换source/中3个文件夹，about, categories, tags# 3. 替换hexo和主题中的配置文件 _config.yml# 4. 替换\\indigo\\source\\img 文件夹# 5. 修改source/css/_partial/variable.css的28行 宽度为80%# 6. 把博客文件复制过来，运行查看。 PyPlot使用中文 参考文档 12345678910111213# 下载字体放到下面的目录# 下载simhei.tff/home/plm/app/anaconda2/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/ttf# 编辑文件/home/plm/app/anaconda2/lib/python2.7/site-packages/matplotlib/mpl-data/matplotlibrc# 打开下面的注释# font.family : sans-serif# 打开注释，加上SimHei# font.sans-serif : SimHei,Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif# 删除缓存rm -rf ~/.cache/matplotlib 简单测试例子 123456789101112131415import matplotlibmatplotlib.matplotlib_fname()import matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['simhei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号matplotlib.rcParams['axes.unicode_minus'] = False #-号为方块问题plt.plot((1,2,3),(4,3,-1))s = \"横坐标\"plt.xlabel(unicode(s))plt.ylabel(u'纵坐标')plt.show()print (s) 有时候依然不好使，那么就 123import sys reload(sys) sys.setdefaultencoding('utf8') 应该就可以了。","tags":[{"name":"心得","slug":"心得","permalink":"http://plmsmile.github.io/tags/心得/"},{"name":"hexo","slug":"hexo","permalink":"http://plmsmile.github.io/tags/hexo/"},{"name":"latex","slug":"latex","permalink":"http://plmsmile.github.io/tags/latex/"},{"name":"中文","slug":"中文","permalink":"http://plmsmile.github.io/tags/中文/"},{"name":"pyplot","slug":"pyplot","permalink":"http://plmsmile.github.io/tags/pyplot/"}]},{"title":"Attention-based NMT 阅读笔记","date":"2017-10-12T08:12:39.000Z","path":"2017/10/12/Attention-based-NMT/","text":"Effective Approaches to Attention-based Neural Machine Translation 简介 Attention介绍 在翻译的时候，选择性的选择一些重要信息。详情看这篇文章 。 本着简单和有效的原则，本论文提出了两种注意力机制。 Global 每次翻译时，都选择关注所有的单词。和Bahdanau的方式 有点相似，但是更简单些。简单原理介绍。 Local 每次翻译时，只选择关注一部分的单词。介于soft和hard注意力之间。(soft和hard见别的论文)。 优点有下面几个 比Global和Soft更好计算 局部注意力 随处可见、可微，更好实现和训练。 应用范围 在训练神经网络的时候，注意力机制应用十分广泛。让模型在不同的形式之间，学习对齐等等。有下面一些领域： 机器翻译 语音识别 图片描述 between image objects and agent actions in the dynamic control problem (不懂，以后再说吧) 神经机器翻译 思想 输入句子\\(x = (x_1, x_2, \\cdots, x_n)\\) ，输出目标句子\\(y = (y_1, y_2, \\cdots, y_m)\\) 。 神经机器翻译(Neural machine translation, NMT)，利用神经网络，直接对\\(\\color{blue} {p(y \\mid x)}\\) 进行建模。一般由Encoder和Decoder构成。Encoder-Decoder介绍文章链接 。 Encoder把输入句子\\(x\\) 编码成一个语义向量\\(s\\) (c表示也可以)，然后由Decoder 一个一个产生目标单词 \\(y_i\\) \\[ \\log p(y \\mid x) = \\sum_{j=1}^m \\log \\color{red} {p(y_j \\mid y _{&lt;j}, s) } = \\sum_{j=1}^m \\log p(y_j \\mid y_1, \\cdots, y_{j-1}, s) \\] 但是怎么选择Encoder和Decoder（RNN, CNN, GRU, LSTM），怎么去生成语义\\(s\\)却有很多选择。 概率计算 结合Dncoder上一时刻的隐状态\\(\\color{blue}{h_{j-1}}\\)和encoder给的语义向量\\(\\color{blue}{s}\\)，通过函数\\(\\color{blue}{f}\\) ，就可以计算出当前的隐状态\\(\\color{blue}{h_j}\\) ： \\[ h_j = f(h_{j-1}, s) \\] 通过函数\\(\\color{blue}{g}\\)对当前隐状态\\(h_j\\)进行转换，再softmax，就可以得到翻译的目标单词\\(y_i\\)了（选概率最大的那个）。 \\(g\\)一般是线性变换，维数变化是\\([1, h] \\to [1, vocab\\_size]\\)。 \\[ p(y_j \\mid y _{&lt;j}, s) = \\mathrm{softmax} \\; g(h_j) \\] 语义向量\\(s​\\) 会贯穿整个翻译的过程，每一步翻译都会使用到语义向量的内容，这就是注意力机制。 本论文的模型 本论文采用stack LSTM的构建NMT系统。如下所示： 训练目标是 \\[ J_t = \\sum_{(x, y)} - \\log p(y \\mid x) \\] 注意力模型 注意力模型广义上分为global和local。Global的attention来自于整个序列，而local的只来自于序列的一部分。 解码总体流程 Decoder时，在时刻\\(t\\)，要翻译出单词\\(y_t\\) ，如下步骤： 最顶层LSTM的隐状态 \\(h_t\\) 计算带有原句子信息语义向量\\(c_t\\)。Global和Local的区别在于\\(c_t\\)的计算方式不同 串联\\(h_t, c_t\\)，计算得到带有注意力的隐状态 \\(\\hat {h}_t = \\tanh (W_c [c_t; h_t])\\) 通过注意力隐状态得到预测概率 \\(p(y_t \\mid y_{&lt;t}, x) = \\rm {softmax} (W_s \\hat h _t)\\) Global Attention 总体思路 在计算\\(c_t\\) 的时候，会考虑整个encoder的隐状态。Decoder当前隐状态\\(h_t\\)， Encoder时刻s的隐状态\\(\\bar h _s\\)。 对齐向量\\(\\color{blue}{\\alpha_t}\\)代表时刻\\(t\\) 输入序列中的单词对当前单词\\(y_t\\) 的对齐概率，长度是\\(T_x\\)， 随着输入句子的长度而改变 。\\(x_s\\)与\\(y_t\\) 的对齐概率如下： \\[ \\alpha_t(s) = \\mathrm {align} (h_t, \\bar h_s) = \\frac {score(h_t, \\bar h_s)}{ \\sum_{i=1}^{T_x} score(h_t, \\bar h_i)}, \\quad 实际上\\mathrm{softmax} \\] 结合上面的解码总体流程，有下面的流程 \\[ all (\\bar h_s) , h_t \\to \\alpha_t \\to c_t . \\quad c_t , h_t \\to \\hat h_t .\\quad \\hat h_t \\to y_t \\quad \\] 简单来说是\\(h_t \\to \\alpha_t \\to c_t \\to \\hat h_t \\to y_t\\) 。 score计算 \\(score(h_t, \\bar h_s)\\) 是一种基于内容content-based的函数，有3种实现方式 \\[ \\color{blue}{score(h_t, \\bar h_s)} = \\begin{cases} h_t^T \\bar h_s &amp; dot \\\\ h_t^T W_a \\bar h_s &amp; general \\\\ v_a^T \\tanh (W_a [h_t; \\bar h_s]) &amp; concat \\\\ \\end{cases} \\] 缺点 生成每个目标单词的时候，都必须注意所有的原单词， 这样计算量很大，翻译长序列可能很难，比如段落或者文章。 Local Attention 在生成目标单词的时候，Local会选择性地关注一小部分原单词去计算\\(\\alpha_t, c_t\\)，这样就解决了Global的问题。如下图 Soft和Hard注意 Soft 注意 ：类似global注意，权值会放在图片的所有patches中。计算复杂。 Hard 注意： 不同时刻，会选择不同的patch。虽然计算少，但是non-differentiable，并且需要复杂的技术去训练模型，比如方差减少和强化学习。 Local注意 类似于滑动窗口，计算一个对齐位置\\(\\color{blue}{p_t}\\)，根据经验设置窗口大小\\(D\\)，那么需要注意的源单词序列是 ： \\[ [p_t -D, p_t + D] \\] \\(\\alpha_t\\) 的长度就是\\(2D\\)，只需要选择这\\(2D\\)个单词进行注意力计算，而不是Global的整个序列。 对齐位置选择 对齐位置的选择就很重要，主要有两种办法。 local-m (monotonic) 设置位置， 即以当前单词位置作为对齐位置 \\[ p_t = t \\] local-p (predictive) 预测位置 \\(S\\) 是输入句子的长度，预测对齐位置如下 \\[ p_t = S \\cdot \\mathrm{sigmoid} \\left(v_p^T \\tanh (W_p h_t) \\right), \\quad p_t \\in [0, S] \\] 对齐向量计算 \\(\\alpha_t\\)的长度就是\\(2D\\)，对于每一个\\(s \\in [p_t -D, p_t + D]\\)， 为了更好地对齐，添加一个正态分布\\(N(\\mu, \\sigma ^2)\\)，其中 \\(\\mu = p_t, \\sigma = \\frac{D}{2}\\)。 计算对齐概率： \\[ \\alpha_t(s) = \\mathrm{align} (h_t, \\bar h_s) \\exp \\left( - \\frac{(s - \\mu)^2}{2\\sigma^2}\\right) = \\mathrm{align} (h_t, \\bar h_s) \\exp \\left( - \\frac{2(s - p_t)^2}{D^2}\\right) \\] Input-feeding 前面的Global和Local两种方式中，在每一步的时候，计算每一个attention (实际上是指 \\(\\hat h_t\\))，都是独立的，这样只是次最优的。 在每一步的计算中，这些attention应该有所关联，当前知道之前的attention才对。实际是应该有个coverage set去追踪之前的信息。 我们会把当前的注意\\(\\hat h_t\\) 作为下一次的输入，并且做一个串联，来计算新的attention，如下图所示 这样有两重意义： 模型会知道之前的对齐选择 会建立一个水平和垂直都很深的网络 PyTorch实现Global 计算对齐向量 初始化参数 score_type: 计算score的方法 hidden_size: Encoder和Decoder的hidden_size 12345678910111213141516def __init__(self, score_type, hidden_size): ''' Args: score_type: 计算score的方法，'dot', 'general', 'concat' hidden_size: Encoder和Decoder的hidden_size ''' super(Attn, self).__init__() self.score_type = score_type self.hidden_size = hidden_size self.max_length = max_length if score_type == 'general': self.attn = nn.Linear(hidden_size, hidden_size) elif score_type == 'concat': self.attn = nn.Linear(hidden_size * 2, hidden_size) self.other = nn.Parameter(torch.FloatTensor(1, hidden_size)) 计算score 123456789101112131415161718def score(self, hidden, encoder_output): ''' 计算Decoder中LSTM的ht与Encoder中的hs的得分，便于后面算对齐概率 Args: hidden: Decoder中最顶层LSTM的隐状态，h_de_t，[1, h_size] encoder_output: Encoder某时刻的隐状态，h_en_s，[1, h_size] Returns: energy: d_ht与e_hs的得分，即Yt与Xs的得分 ''' if self.score_type == 'dot': energy = hidden.dot(encoder_output) elif self.score_type == 'general': energy = self.attn(encoder_output) energy = hidden.dot(energy) elif self.score_type == 'concat': h_o = torch.cat((hidden, encoder_output), 1) energy = self.attn(h_o) energy = self.other.dot(energy) return energy 前向计算 实际就是对所有的打分结果进行softmax 得到对齐向量，也成为注意力权值，attn_weights 12345678910111213141516def forward(self, hidden, encoder_outputs): ''' 时刻t，计算对齐向量 Args: hidden: Top LSTM的隐状态 encoder_outputs: Encoder的所有隐状态, len=Tx输入句子长度 Returns: align_vec: 当前ht与所有encoder_outputs的对齐向量，alpha_t，len=Tx，返回[1, 1, seq_len]格式 ''' seq_len = len(encoder_outputs) attn_energies = get_variable(torch.zeros(seq_len)) for i in range(seq_len): attn_energies[i] = self.score(hidden, encoder_outputs[i]) # normalize [0, 1], resize to [1, 1, seq_len] align_vec = F.softmax(attn_energies) align_vec = align_vec.unsqueeze(0).unsqueeze(0) return align_vec Decoder网络 参数 hidden_size: Embedding和GRU的hidden_size output_size: 输出的size，是目标语言的词典大小 score_type: &#39;dot&#39;, &#39;general&#39;, &#39;concat&#39; n_layers: GRU的层数 dropout_p: dropout 1234''' initArgs: '''","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"Attention","slug":"Attention","permalink":"http://plmsmile.github.io/tags/Attention/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"}]},{"title":"自然语言处理中的Attention Model","date":"2017-10-10T03:40:01.000Z","path":"2017/10/10/attention-model/","text":"Encoder-Decoder 基本介绍 举个翻译的例子，原始句子\\(X = (x_1, x_2, \\cdots, x_m)\\) ，翻译成目标句子\\(Y = (y_1, y_2, \\cdots, y_n)\\) 。 现在采用Encoder-Decoder架构模型，如下图 Encoder会利用整个原始句子生成一个语义向量，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考Encoder-Decoder框架。 Encoder对\\(X\\) 进行非线性变换得到中间语义向量c ： \\[ c = G(x_1, x_2, \\cdots, x_n) \\] Decoder根据语义\\(c\\) 和生成的历史单词\\((y_1, y_2, \\cdots, y_{i-1})\\) 来生成第\\(i\\) 个单词 \\(y_i\\)： \\[ y_i = f(c, y_1, y_2, \\cdots, y_{i-1}) \\] Encoder-Decoder是个创新大杀器，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN，RNN，BiRNN，GRU，LSTM， Deep LSTM。上面的内容任意组合，只要得到的效果好，就是一个创新，就可以毕业了。（当然别人没有提出过） 缺点 在生成目标句子\\(Y\\)的单词时，所有的单词\\(y_i\\)使用的语义编码\\(c\\) 都是一样的。而语义编码\\(c\\)是由句子\\(X\\) 的每个单词经过Encoder编码产生，也就是说每个\\(x_i\\)对所有\\(y_j\\)的影响力都是相同的，没有任何区别的。所以上面的是注意力不集中的分心模型。 句子较短时问题不大，但是较长时，所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，会丢失更多的细节信息。 例子 比如输入\\(X\\)是Tom chase Jerry，模型翻译出 汤姆 追逐 杰瑞。在翻译“杰瑞”的时候，“Jerry”对“杰瑞”的贡献更重要。但是显然普通的Encoder-Decoder模型中，三个单词对于翻译“Jerry-杰瑞”的贡献是一样的。 解决方案应该是，每个单词对于翻译“杰瑞”的贡献应该不一样，如翻译“杰瑞”时： \\[ (Tom, 0.3), \\; (Chase, 0.2), \\; (Jerry, 0.5) \\] Attention Model 基本架构 Attention Model的架构如下： 如图所示，生成每个单词\\(y_i\\)时，都有各自的语义向量\\(C_i\\)，不再是统一的\\(C\\) 。 \\[ y_i = f(C_i, y_1, \\cdots, y_{i-1}) \\] 例如，前3个单词的生成： \\[ \\begin{align} &amp; y_1 = f(C_1) \\\\ &amp; y_2 = f(C_2, y_1) \\\\ &amp; y_3 = f(C_3, y_1, y_2) \\\\ \\end{align} \\] 语义向量的计算 注意力分配概率 \\(a_{ij}\\) 表示 \\(y_i\\)收到\\(x_j\\) 的注意力概率。 例如\\(X=(Tom, Chase, Jerry)\\)，\\(Y = (汤姆, 追逐, 杰瑞)\\) 。\\(a_{12}=0.2\\)表示汤姆 收到来自Chase的注意力概率是0.2。 有下面的注意力分配矩阵： \\[ A = [a_{ij}] = \\begin {bmatrix} 0.6 &amp; 0.2 &amp; 0.2 \\\\ 0.2 &amp; 0.7 &amp; 0.1 \\\\ 0.3 &amp; 0.1 &amp; 0.5 \\\\ \\end {bmatrix} \\] 第\\(i\\)行表示\\(y_i\\) 收到的所有来自输入单词的注意力分配概率。\\(y_i\\) 的语义向量\\(C_i\\) 由这些注意力分配概率和Encoder对单词\\(x_j\\)的转换函数相乘，计算而成，例如： \\[ \\begin {align} &amp; C_1 = C_{汤姆} = g(0.6 \\cdot h(Tom),\\; 0.2 \\cdot h(Chase),\\; 0.2 \\cdot h(Jerry)) \\\\ &amp; C_2 = C_{追逐} = g(0.2 \\cdot h(Tom) ,\\;0.7 \\cdot h(Chase) ,\\;0.1 \\cdot h(Jerry)) \\\\ &amp; C_3 = C_{汤姆} = g(0.3 \\cdot h(Tom),\\; 0.2 \\cdot h(Chase) ,\\;0.5 \\cdot h(Jerry)) \\\\ \\end {align} \\] \\(\\color{blue}{h(x_j)}\\) 就表示Encoder对输入英文单词的某种变换函数。比如Encoder使用RNN的话，\\(h(x_j)\\)往往都是某个时刻输入\\(x_j\\) 后隐层节点的状态值。 g函数 表示注意力分配后的整个句子的语义转换信息，一般都是加权求和，则有语义向量计算公式： \\[ C_i = \\sum_{j=1}^{T_x} a_{ij} \\cdot h_j, \\quad h_j = h(x_j) \\] 其中\\(\\color{blue}{T_x}\\) 代表输入句子的长度。形象来看计算过程如下图： 注意力分配概率计算 语义向量需要注意力分配概率和Encoder输入单词变换函数来共同计算得到。 但是比如汤姆收到的分配概率\\(a_1 = (0.6, 0.2, 0.2)\\)是怎么计算得到的呢？ 这里采用RNN作为Encoder和Decoder来说明。 注意力分配概率如下图计算 对于\\(a_{ij}\\) 其实是通过一个对齐函数F来进行计算的，两个参数：输入节点\\(j\\)，和输出节点\\(i\\)，当然一般是取隐层状态。 \\[ a_i = F(i, j), \\quad j \\in [1, T_x], \\quad h(j)\\,Encoder, \\; H(i)\\,Decoder \\] \\(\\color{blue}{F(i, j)}\\)代表\\(y_i\\)和\\(x_j\\)的对齐可能性。一般F输出后，再经过softmax就得到了注意力分配概率。 AM模型的意义 一般地，会把AM模型看成单词对齐模型，输入句子单词和这个目标生句子成单词的对齐概率。 其实，理解为影响力模型也是合理的。就是在生成目标单词的时候，输入句子中的每个单词，对于生成当前目标单词有多大的影响程度。 AM模型有很多的应用，思想大都如此。 文本摘要例子 比如文本摘要的例子，输入一个长句，提取出重要的信息。 输入&quot;russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism&quot;。 输出&quot;russia calls for joint front against terrorism&quot;。 下图代表着输入单词对输出单词的影响力，颜色越深，影响力越大，注意力分配概率也越大。 PyTorch翻译AM实现 思想 参考这篇论文 。 生成目标单词\\(y_i\\) 的计算概率是 \\[ p(y_i \\mid (y_1,\\cdots, y_{i-1}), x) = g(y_{i-1}, s_i, c_i) \\] 符号意义说明 \\(y_i\\) 当前应该生成的目标单词，\\(y_{i-1}\\) 上一个节点的输出单词 \\(s_i\\) 当前节点的隐藏状态 \\(c_i\\) 生成当前单词应该有的语义向量 \\(g\\) 全连接层的函数 隐层状态\\(s_i\\) 求当前Decoder隐层状态\\(s_i\\)：由上一层的隐状态\\(s_{i-1}\\)，输出单词\\(y_{i-1}\\) ，语义向量\\(c_i\\) \\[ s_i = f(s_{i-1}, y_{i-1}, c_i) \\] 语义向量\\(c_i\\) 语义向量：分配权值\\(a_{ij}\\)，Encoder的输出 \\[ c_i = \\sum_{j=1}^{T_x} a_{ij} \\cdot h_j, \\quad h_j = h(x_j) \\] 分配概率\\(a_{ij}\\) 注意力分配概率\\(a_{ij} ，\\) \\(y_i\\) 收到\\(x_j\\) 的注意力：分配能量\\(e_{ij}\\) \\[ a_{ij} = \\frac{\\exp(e_{ij})} {\\sum_{k=1}^{T_x} \\exp (e_{ik})} \\] 分配能量\\(e_{ij}\\) \\(x_j\\) 注意\\(y_i\\) 的能量，由encoder的隐状态\\(h_j\\) 和 decoder的上一层的隐状态\\(s_{i-1}\\) 计算而成。a函数就是一个线性层。也就是上面的F函数。 \\[ e_{ij} = a(s_{i-1}, h_j) \\] 实现 Decoder由4层组成 embedding : word2vec attention layer: 为每个encoder的output计算Attention RNN layer: output layer: Decoder输入 \\(s_{i-1}\\) , \\(y_{i-1}\\) 和encoder的所有outputs \\(h_*\\) Embedding Layer 输入\\(y_{i-1}\\)，对其进行编码 12# y(i-1)embedded = embedding(last_rnn_output) Attention Layer 输入\\(s_{i-1}, h_j\\)，输出分配能量\\(e_{ij}\\)， 计算出\\(a_{ij}\\) 12attn_weights[j] = attn_layer(last_hidden, encoder_outputs[j])attn_weights = normalize(attn_weights) 计算语义向量 求语义向量\\(c_i\\)， 一般是加权求和 1context = sum(attn_weights * encoder_outputs) RNN Layer 输入\\(s_{i-1}, y_{i-1}, c_i\\) ，内部隐层状态，输出\\(s_i\\) 12rnn_input = concat(embeded, context)rnn_output, rnn_hidden = rnn(rnn_input, last_hidden) 输出层 输入\\(y_{i-1}, s_i, c_i\\) ，输出\\(y_i\\) 1output = out(embedded, rnn_output, context)","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"Attention Model","slug":"Attention-Model","permalink":"http://plmsmile.github.io/tags/Attention-Model/"},{"name":"Encoder-Decoder","slug":"Encoder-Decoder","permalink":"http://plmsmile.github.io/tags/Encoder-Decoder/"}]},{"title":"PyTorch快速上手","date":"2017-10-05T05:30:54.000Z","path":"2017/10/05/pytorch-start/","text":"PyTorch介绍 PyTorch Torch 是一个使用Lua 语言的神经网络库，而PyTorch是Torch在Python的衍生。 PyTorch是一个基于python的科学计算包。本质上是Numpy的代替者，支持GPU、带有高级功能，可以用来搭建和训练深度神经网络；是一个深度学习框架，速度更快，弹性更好。 PyTorch和Tensorflow Tensorflow 类似一个嵌入Python的编程语言。写的Tensorflow代码会被Python编译成一张计算图，然后由TensorFlow执行引擎运行。Tensorflow有一些额外的概念需要学习，上手时间慢。 对比参考这篇文章PyTorch还是Tensorflow 。下面是结论。后续再补充详细内容。 PyTorch更有利于研究人员、爱好者、小规模项目等快速搞出原型，易于理解。而TensorFlow更适合大规模部署，特别是需要跨平台和嵌入式部署时。 PyTorch和Tensorflow对比如下 PyTORCH Tensorflow 动静态 建立的神经网络是动态的 建立静态计算图 代码难度 易于理解，好看一些。有弹性 底层代码难以看懂 工业化 好上手 高度工业化 数据操作 Tensor 创建Tensor Tensor实际上是一个数据矩阵，PyTorch处理的单位就是一个一个的Tensor。下面是一些创建方法 12345678910111213import torch# 1. 未初始化，都是0x = torch.Tensor(5, 3)# 2. 随机初始化x = torch.rand(5, 3)# 3. 传递参数初始化x = torch.Tensor([1, 2])# 4. 通过Numpy初始化a = np.ones(5)b = torch.from_numpy(a)# 5. 获取size，返回一个tuple [5, 3]print x.size() Tensor也可以通过Numpy来进行创建，或者从Tensor得到一个Numpy。 1234import numpy as npa = np.ones(5)b = torch.from_numpy(a)c = b.numpy() Tensor操作 Tensor的运算也很简单，一般的四则运算都是支持的。 123456789101112x = torch.rand(5, 3)y = torch.rand(5, 3)# 1. 直接相加z = x + y# 2. torch相加z = torch.add(x, y)# 3. 传递参数返回结果result = torch.rand(5, 3)torch.add(x, y, out = result)# 4. 加到自身去，自身y会改变y.add_(x) 其中所有类似于x.add_(y)的操作都会改变自己x，如x.copy_(y) 、x.t_() 。 对于Tensor可以像Numpy那样索引和切片。 改变Tensor和Numpy 改变Tensor后，对应的Numpy也会发生改变 1234567a = torch.ones(5)b = a.numpy()# 改变aa.add_(1)# b也会改变print a # 22222print b # 22222 CUDA Tensors 使用GPU很简单，只需使用.cuda就可以了 1234if torch.cuda.is_available(): x = x.cuda() y = y.cuda() x + y Variable 在神经网络中，最重要的是torch.autograd这个包，而其中最重要的一个类就是Variable。 本质上Variable和Tensor没有区别，不过Variable会放入一个计算图，然后进行前向传播，反向传播和自动求导。这也是PyTorch和Numpy不同的地方。 Variable由data, grad, creator 三部分组成。 data: 包装的Tensor，即数据 grad: 方向传播的梯度缓冲区 creator: 得到这个Variable的操作，如乘法加法等等。 用一个Variable进行计算，返回的也是一个同类型的Variable。 梯度计算例子 线性计算\\(z= 2 \\cdot x + 3 \\cdot y + 4\\) ，求\\(\\frac{ \\partial z}{\\partial x}\\) 和\\(\\frac{ \\partial z}{\\partial y}\\) 。 123456789101112import torchfrom torch.autograd import Variable# 1. 准备式子# 默认求导是falsex = Variable(torch.Tensor([2]), requires_grad = True)y = Variable(torch.Tensor([3]), requires_grad = True)z = 2 * x + 3 * y + 4# 2. z对x和y进行求导z.backward()# 3. 获得z对x和y的导数print x.grad.data # 2print y.grad.data # 3 复杂计算$ y = x + 2$， \\(z = y * y * 3\\)， \\(o = avg(z)\\) ，求\\(\\frac{dz}{dx}\\) 1234567x = Variable(torch.ones(2, 2), requires_grad = True)y = x + 2z = y * y * 3out = z.mean()out.backward()# d(out)/dxprint x.grad 传递梯度 12345x = Variable(torch.Tensor([2]), requires_grad = True)y = x + 2gradients = torch.FloatTensor([0.01, 0.1, 1])y.backward(gradients)print x.grad.data 一些常用的API总结 12345678910111213x = torch.Tensor([2, 2])# 随机创建数字x = torch.randn(3)x = torch.randn(3, 3)# 求平均值x.mean()# 范数x.norm()# torch view。数据相同，改变形状。得到一个Tensorx = torch.randn(4, 4)y = x.view(16)z = x.view(2, 2, 4)z = x.view(2, 2, -1) # 最后-1，会自己适配 神经网络","tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://plmsmile.github.io/tags/PyTorch/"}]},{"title":"神经网络机器翻译","date":"2017-10-02T02:03:31.000Z","path":"2017/10/02/NMT/","text":"在机器翻译、语音识别、文本摘要等领域中，Sequence-to-sequence 模型都取得了了非常好的效果。神经机器翻译(Neural Machine Translation, NMT) 使用seq2seq模型取得了巨大的成功。 本文参考谷歌NMT教程。 Basic 背景知识 传统翻译是以词为核心一词一词翻译的，这样会切断句子本身的意思，翻译出来也很死板，不像我们人类说的话。 Encoder-Decoder 现在采用Encoder-Decoder架构模型。如下图 Encoder会利用整个原始句子生成一个语义向量，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考Encoder-Decoder框架。 举个翻译的例子，原始句子\\(X = (x_1, x_2, \\cdots, x_m)\\) ，翻译成目标句子\\(Y = (y_1, y_2, \\cdots, y_m)\\) 。 Encoder对\\(X\\) 进行非线性变换得到中间语义\\(C\\) \\[ C = \\Gamma(x_1, x_2, \\cdots, x_n) \\] Decoder根据语义\\(C\\) 和生成的历史信息\\(y_1, y_2, \\cdots, y_{i-1}\\) 来生成第\\(i\\) 个单词 \\(y_i\\) \\[ y_i = \\Psi(C, y_1, y_2, \\cdots, y_{i-1}) \\] 当然，在Attention Model 中，Decoder生成Y的时候每个单词对应的\\(C\\)不一样，记作\\(C_j, j \\in [1, n]\\) 。\\(C_j\\) 就是体现了源语句子中不同的单词对目标句子中不同的单词的注意力概率分布。即各个单词的对齐的概率，也就是student对&quot;学生&quot;更重要，而对&quot;我&quot;不那么重要。这个在后续会用到。 Encoder-Decoder是个创新大杀器，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN,RNN,BiRNN,GRU,LSTM, Deep LSTM。比如编码CNN-解码RNN, 编码BiRNN-解码Deep LSTM等等。 上面的内容任意组合，只要得到的效果好，就是一个创新，就可以毕业了。（当然别人没有提出过） NMT模型选择 有3个维度需要选择。 方向性。是单向还是双向 深度：是一层还是多层 网络选择：encoder和decoder具体分别选什么 在本文的实现中，我们选择单向的、多层的、LSTM。基于这篇论文。如下图。","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"机器翻译","slug":"机器翻译","permalink":"http://plmsmile.github.io/tags/机器翻译/"}]},{"title":"条件随机场","date":"2017-09-28T03:17:59.000Z","path":"2017/09/28/crf/","text":"条件随机场(Conditional Random Field, CRF)是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型。常常用于标注问题。隐马尔科夫模型和条件随机场是自然语言处理中最重要的算法。CRF最重要的就是根据观测序列，把标记序列给推测出来。 概率无向图模型 概率无向图模型又称为马尔科夫随机场，是一个可以由无向图表示的联合概率分布。一些类似内容。 有一组随机变量\\(Y \\in \\Gamma\\)，联合概率分布为\\(P(Y)\\)，由图\\(G=(V,E)\\)表示。节点v代表变量\\(Y_v\\)，节点之间的边代表两个变量的概率依赖关系。 定义 马尔可夫性就是说，给定一些条件下，没有连接的节点之间是条件独立的。 成对马尔可夫性 设\\(u\\)和\\(v\\)是两个没有边连接的节点，其它所有节点为\\(O\\)。成对马尔可夫性是说，给定随机变量组\\(Y_O\\)的条件下，随机变量\\(Y_u\\)和\\(Y_v\\)是独立的。即有如下： \\[ P(Y_u, Y_v \\mid Y_O) = P(Y_u \\mid Y_O)P(Y_v \\mid Y_O) \\] 局部马尔可夫性 节点\\(v\\)，\\(W\\)是与\\(v\\)连接的所有节点，\\(O\\)是与\\(v\\)没有连接的节点。局部马尔可夫性认为，给定\\(Y_w\\)的条件下，\\(Y_v\\)和\\(Y_O\\)独立。即有： \\[ P(Y_v, Y_O \\mid Y_W) = P(Y_v \\mid Y_W) P(Y_O \\mid Y_W) \\] 全局马尔可夫性 节点集合\\(A\\)，\\(B\\)被中间节点集合\\(C\\)分隔开，即不相连。全局马尔可夫性认为，给定\\(Y_C\\)的条件下，\\(Y_A\\)和\\(Y_B\\)是独立的。即有： \\[ P(Y_A, Y_B \\mid Y_C) = P(Y_A \\mid Y_C) P(Y_B \\mid Y_C) \\] 上面的3个马尔可夫性的定义是等价的。 概率无向图模型 设有联合概率密度\\(P(Y)\\)，由无向图\\(G=(V,E)\\)表示。节点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率密度\\(P(Y)\\)满足马尔可夫性，那么就称此联合概率分布为概率图模型，或马尔可夫随机场。 实际上我们更关心怎么求联合概率密度，一般是把整体的联合概率写成若干个子联合概率的乘积，即进行因子分解。概率无向图模型最大的优点就是易于因子分解。 概率无向图因子分解 团与最大团 团：无向图中的一个子集，任何两个节点均有边连接。 最大团：无向图中的一个子集，任何两个节点均有边连接。不能再加入一个节点组成更大的团了。 如\\(\\{Y_1, Y_2\\}\\)，\\(\\{Y_1, Y_2, Y_3\\}\\) 都是团，其中后者是最大团。而\\(\\{Y_1, Y_2, Y_3, Y_4\\}\\) 不是团，因为\\(Y_1\\)和\\(Y_4\\)没有边连接。 因子分解 有无向图模型\\(G\\), \\(C\\) 是 \\(G\\) 上的最大团，有很多个。\\(Y_C\\) 是\\(C\\) 对应的随机变量。则联合概率分布\\(P(Y)\\) 可以写成多个最大团\\(C\\) 上的势函数的乘积。 \\[ \\color{blue}{P(Y)} = \\frac {1} {Z} \\prod_C \\Psi_C(Y_C), \\quad Z = \\sum_Y \\prod_C \\Psi_C(Y_C) \\] 其中\\(Z\\)是规范化因子。\\(\\Psi_C(Y_C)\\)是 势函数，是一个严格正函数。等式左右两端都取条件概率也是可以的。下文就是。 \\[ \\color{blue}{\\Psi_C(Y_C)} = \\exp \\left(-E(Y_C) \\right) \\] 其中\\(\\color{blue}{E(Y_C) }\\) 是能量函数。 条件随机场的定义与形式 HMM的问题 这里是HMM的讲解 。HMM有下面几个问题 需要给出隐状态和观察符号的联合概率分布，即发射概率 \\(b_j(k)\\)，是生成式模型，也是它们的通病。 观察符号需要是离散的，可以枚举的，要遍历所有观察符号。如果是一个连续序列，则不行。 观察符号是独立的，没有观察相互之间的依赖关系。如一个句子的前后，都有关联才是。即输出独立性假设问题。 无法考虑除了字词顺序以外的其它特征。比如字母为大小写，包含数字等。 标注偏置问题。 标注偏置问题，举例，是说有两个单词&quot;rib-123&quot;和&quot;rob-456&quot;，&quot;ri&quot;应该标记为&quot;12&quot;，&quot;ro&quot;应该标记为&quot;45&quot;。 \\[ \\begin {align} &amp; P(12 \\mid ri) = P(1 \\mid r)P(2 \\mid i, r=1) = P(1 \\mid r) \\cdot 1 = P(1 \\mid r) \\\\ &amp; P(45 \\mid ro) = P(4 \\mid r)P(5 \\mid o, r=4) = P(4 \\mid r) \\cdot 1 = P(4 \\mid r) \\\\ \\end {align} \\] 由上面计算概率可知，ri标为12和 ro标为45的概率最终变成r标为1和4的概率。但是由于语料库中&quot;rob&quot;的出现次数很多，所以\\(P(4 \\mid r) &gt; P(1 \\mid r)\\) ，所以可能会一直把&quot;rib&quot;中的&quot;i&quot;标记为1，会导致标记出错。这就是标记偏置问题。 定义 条件随机场是给定随机变量\\(X\\)条件下，随机变量\\(Y\\) 的马尔可夫随机场。我们主要关心线性链随机场，它可以用于标注问题。 条件随机场 \\(X\\)与\\(Y\\)是随机变量，条件概率分布\\(P(Y \\mid X)\\)。随机变量\\(Y\\)可以构成一个无向图表示的马尔可夫随机场。任意一节点\\(Y_v\\)，\\(Y_A\\)是与\\(v\\)相连接的节点，\\(Y_B\\)是除了\\(v\\)以外的所有节点。若都有 \\[ P(Y_v \\mid X, Y_B) = P(Y_v \\mid X, Y_A) \\] 则称\\(P(Y \\mid X)\\) 为条件随机场。并不要求\\(X\\)和\\(Y\\) 具有相同的结构。 线性链条件随机场 \\(X\\)和\\(Y\\) 有相同的线性结构。设\\(X = (X_1, X_2, \\cdots, X_n)\\)，\\(Y = (Y_1, Y_2, \\cdots, Y_n)\\)均为线性链表示的随机变量序列。每个最大团包含2个节点。 \\(P(Y \\mid X)\\) 构成条件随机场，即满足马尔可夫性 \\[ P(Y_i \\mid X, Y_1, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_n) = P(Y_i \\mid X, Y_{i-1}, Y_{i+1}), \\quad i=1,\\cdots, n。 \\; (1和n时只考虑单边) \\] 则称\\(P(Y \\mid X)\\)为线性链条件随机场。 HMM，每个观察状态只与当前的隐状态有关系，分离了关系。就像1个字1个字地向后讲。输出观察符号还需要条件独立。 线性链条件随机场， 每个状态都与整个序列有关系。即先想好了整句话，再依照相应的次序去说出来。更加直击语言模型的核心。\\(X_1, X_2\\) 不需要条件独立。 基本形式 两种特征函数 状态转移特征函数t，只依赖与当前和前一个位置，即\\(y_i\\)和\\(y_{i-1}\\)。一般是01函数。 \\[ t(y_{i-1}, y_i, x, i) = \\begin {cases} 1, \\quad &amp; 满足某种条件, i \\in [2, n]. \\;例如y_{i-1}+y_{i}=3 \\\\ 0, \\quad &amp; 其他 \\end {cases} \\] 状态特征函数s，只依赖与当前位置\\(y_i\\) \\[ s(y_i, x, i) = \\begin {cases} 1, \\quad &amp; 满足某种条件, i \\in [1, n]. \\;例如y_{i}是偶数 \\\\ 0, \\quad &amp; 其他 \\end {cases} \\] 基础形式 设有\\(K_1\\)个状态特征转移函数，\\(K_2\\)个状态特征函数。分别对应的权值是\\(\\lambda_{k_1}\\)和\\(\\mu_{k_2}\\)。则线性链条件随机场参数化形式\\(P(y \\mid x)\\) 如下： \\[ P(y \\mid x) = \\frac {1}{Z(x)} \\exp \\left( \\sum_k^{K_1}\\lambda_k \\sum_{i=2}^n t_k(y_{i-1}, y_i, x, i) + \\sum_k^{K_2}\\mu_k \\sum_{i=1}^n s_k(y_i, x, i) \\right) \\] 其中\\(Z(x)\\)是规范化因子，如下 \\[ Z(x) = \\sum_x \\exp \\left( \\sum_k^{K_1}\\lambda_k \\sum_{i=2}^nt_k(y_{i-1}, y_i, x, i) + \\sum_k^{K_2}\\mu_k \\sum_{i=1}^n s_k(y_i, x, i) \\right) \\] 条件随机场完全由特征函数\\(t_{k_1}\\) 、\\(s_{k_2}\\)，和对应的权值\\(\\lambda_{k_1}\\) 和\\(\\mu_{k_2}\\) 决定的。 特征函数实际上也是势函数。 简化形式 有\\(K=K_1 + K_2\\)个特征，特征函数如下： \\[ f_k(y_{i-1}, y_i, i) = \\begin {cases} t_k(y_{i-1}, y_i, x, i) \\quad &amp; k = 1, \\cdots, K_1 \\\\ s_k(y_i, x, i) \\quad &amp; k = K_1 + l; \\; l = 1, \\cdots, K_2 \\\\ \\end {cases} \\] 同一个特征函数，要在整个\\(Y​\\)序列的各个位置进行计算，可以进行求和，即转化为全局特征函数， 新的特征函数\\(f_k (y, x)​\\)如下： \\[ \\color{blue} {f_k (y, x)} = \\sum _{i=1} ^n f_k(y_{i-1}, y_i, i), \\quad k = 1, \\cdots, K \\] \\(f_k (y, x)\\) 对应的新的权值\\(w_k\\)如下 \\[ \\color{blue} {w_k} = \\begin {cases} \\lambda_{k}, \\quad &amp; k = 1, \\cdots, K_1 \\\\ \\mu_{k - K_1}, \\quad &amp; k = K_1 + 1, K_1 + 2, \\cdots, K \\\\ \\end {cases} \\] 所以新的条件随机场形式如下： \\[ P(y \\mid x) = \\frac {1} {Z(x)} \\exp \\sum_{k=1} ^K w_k f_k(y, x) , \\quad Z(x) = \\sum_y \\exp \\sum_{k=1} ^K w_k f_k(y, x) \\] 可以看出，格式和最大熵模型很像。条件随机场最重要的就是，根据观察序列，把标记序列给推测出来。 向量形式 向量化特征函数和权值 \\[ F(y, x) = (f_1(y, x), \\cdots, f_K(y, x))^T, \\quad w = (w_1, \\cdots, w_K)^T \\] 可以写成向量内积的形式 \\[ P_w (y \\mid x) = \\frac{1}{Z(x)} \\exp (w \\cdot F(y, x)) , \\quad Z(x) = \\sum_y \\exp (w \\cdot F(y, x)) \\] 矩阵形式 为状态序列\\(Y\\)设置起点和终点标记，\\(y_0 = start\\) 和\\(y_{n+1} = stop\\)。从\\(0 \\to n+1\\)中，有\\(n+1\\)次的状态转移。我们可以用\\(n+1\\) 个状态转移矩阵来表示状态转移的概率。 设\\(\\color{blue}{M_i(x)}\\) 是\\(i-1 \\to i\\)的转移矩阵，是\\(m\\)阶，\\(m\\) 是\\(y_i\\) 取值的个数。表示了各种取值情况互相转化的概率。 \\[ M_1(x) = \\begin{bmatrix} a_{01} &amp; a_{02} \\\\ 0 &amp; 0 \\\\ \\end{bmatrix} , \\; M_2(x) = \\begin{bmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\\\ \\end{bmatrix} , \\; M_3(x) = \\begin{bmatrix} c_{11} &amp; c_{12} \\\\ c_{21} &amp; c_{22} \\\\ \\end{bmatrix} , \\; M_4(x) = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\] 要求什么样的路径概率，则相乘相应的数值概率即可。那么这些矩阵里面的数值是怎么来的呢。有下面的矩阵定义 ： \\[ \\color{blue} {g(y_{i-1}, y_i, x, i)} = M_i(y_{i-1}, y_i \\mid x) = \\exp \\sum_{k=1}^K f_k(y_{i-1}, y_i, i, x) ,\\quad \\color{blue} {M_i(x)} = [g(y_{i-1}, y_i, x, i) ] \\] 每一步转移的时候，由于\\((y_{i-1}, y_i)\\) 有\\(m^2\\) 种情况，计算\\(g\\)时会得到多个值，即可得到一个矩阵。 其中\\(g(y_{i-1}, y_i, x, i)\\) 在计算的时候，会根据\\(i\\) 的不同，而选择不同的特征函数进行计算。不要忘记了\\(f_k\\)函数的定义。 \\[ \\color{blue} {f_k(y_{i-1}, y_i, i)} = \\begin {cases} t_k(y_{i-1}, y_i, x, i) \\quad &amp; k = 1, \\cdots, K_1 \\\\ s_k(y_i, x, i) \\quad &amp; k = K_1 + l; \\; l = 1, \\cdots, K_2 \\\\ \\end {cases} \\] 所以非规范化条件概率可以通过矩阵的某些元素的乘积表示，有 \\[ P_w( y \\mid x) = \\frac {1}{Z_w(x)} \\prod_{i=1}^{n+1} M_i(y_{i-1}, y_i \\mid x) \\] 其中规范化因子 是\\(n+1\\)的矩阵相乘的结果矩阵中，第\\((start, stop)\\) 元素。例如第\\((0, 0)\\)。其中是\\((start, end)\\) 是矩阵下标对应。 条件随机场的概率计算问题 主要问题是给定条件随机场\\(P(Y \\mid X)\\) ，给定输入序列\\(x\\) 和输出序列\\(y\\)，求\\(P(Y_i = y_i \\mid x)\\) 和\\(P(Y_{i-1} = y_{i-1}, Y_i = y_{i} \\mid X)\\)， 以及相应的期望问题。 关键是求这些特征函数期望值，当模型训练好之后，去验证我们的模型。 前向后向算法 \\(y_i\\) 确定后， \\(\\color{blue} {\\alpha_i(y_i \\mid x) }\\) 表示，从\\(start \\to i\\)，就是$y = (start, y_1, y_2, , y_i) $ 的概率，也就是从前面到位置\\(y_i\\) 的概率。特别地 \\[ \\alpha_0(y \\mid x) = \\begin{cases} 1, \\quad &amp; y=start \\\\ 0, \\quad &amp; 其他 \\\\ \\end{cases} \\] 而\\(y_i​\\) 的取值有\\(m​\\) 种， 所以前向变量 \\(\\color{blue}{\\alpha_i(x)}​\\) 到\\(y​\\) 到第 \\(i​\\) 个位置 的所有概率取值向量。 \\[ \\color{blue} {\\alpha_i^T(x)} = \\alpha_{i-1}^T(x) \\cdot M_i(x) ,\\quad i = 1,2,\\cdots, n+1 \\] \\(y_i\\) 确定后， \\(\\color{blue} {\\beta_i (y_i \\mid x) }\\) 表示，位置\\(i\\)的标记为\\(y_i\\) ，并且后面为\\(y_{i+1}, \\cdots, y_n, stop\\) 的概率。同理一个是概率值。特别地 \\[ \\beta_{n+1}(y \\mid x) = \\begin{cases} 1, \\quad &amp; y=stop \\\\ 0, \\quad &amp; 其他 \\\\ \\end{cases} \\] 后向变量 \\(\\color{blue} {\\beta_i (y \\mid x) }​\\)，是一个m维向量 \\[ \\color{blue} {\\beta_i (x) } = M_{i+1}(x) \\cdot \\beta_{i+1}(x) \\] 可以得到\\(Z(x)\\)： \\[ Z(x) = \\alpha_n^T(x) \\cdot 1 = 1^T \\cdot \\beta_{i+1}(x) \\] 条件概率计算 位置是\\(i\\) 标记\\(y_i\\) 的条件概率\\(P(Y_i = y_i \\mid x)\\)是 \\[ P(Y_i = y_i \\mid x) = \\frac {1}{Z(x)} \\cdot \\alpha_i(y_i \\mid x) \\beta_i(y_i \\mid x) \\] 位置\\(i-1, i\\) 分别标记为\\(y_{i-1}, y_i\\) 的概率是 \\[ P(Y_{i-1} = y_{i-1}, Y_i = y_i \\mid x) = \\frac{1}{Z(x)} \\cdot \\alpha_{i-1}(y_{i-1} \\mid x) M_i(y_{i-1}, y_i \\mid x) \\beta_i (y_i \\mid x) \\] 特征期望值计算 两个期望值和最大熵模型的约束条件等式 有点像。 特征函数\\(f_k\\) 关于条件概率分布\\(P(Y \\mid X)\\) 的概率 \\[ E_{P(Y \\mid X)}(f_k) = \\sum_y P(y \\mid x) f_k(y, x) = \\sum_{i=1}^{n+1}\\sum_{y_{i-1}y_i} f_k(y_{i-1}, y_i, x, i) P(y_{i-1}, y_i \\mid x) \\] 特征函数\\(f_k\\) 关于条件概率分布\\(P(X, Y)\\) 的概率，\\(\\hat P(x)\\) 是经验分布 \\[ E_{P(X, Y)}(f_k) = \\sum_{x, y} P(x, y) \\sum_{i=1}^{n+1} f_k(y_{i-1}, y_i, x, i) = \\sum_{x} \\hat P(x) \\sum_y P(y \\mid x) \\sum_{i=1}^{n+1} f_k(y_{i-1}, y_i, x, i) \\] 通过前向和后向向量可以计算出两个概率，然后可以计算出相应的期望值。就可以与我们训练出的模型进行比较。 学习算法 条件随机场模型实际上是定义在时序数据上的对数线性模型，学习方法有极大似然估计和正则化的极大似然估计。具体的优化实现算法有：改进的迭代尺度法IIS、梯度下降法和拟牛顿法。 改进的迭代尺度法 这里是最大熵模型中的改进的迭代尺度算法。每次更新一个\\(\\delta_i\\) 使得似然函数的该变量的下界增大，即似然函数增大。 已知经验分布\\(\\hat P(X, Y)\\)，和模型如下 \\[ P(y \\mid x) = \\frac {1} {Z(x)} \\exp \\sum_{k=1} ^K w_k f_k(y, x) , \\quad Z(x) = \\sum_y \\exp \\sum_{k=1} ^K w_k f_k(y, x) \\] 对数似然函数和最大熵算法的极大似然函数很相似，如下： \\[ L(w) = L_{\\hat P}(P_w) = \\log \\prod_{x,y} P_w(y \\mid x) ^ {\\widetilde P(x, y)} = \\sum_{x,y} \\widetilde P(x, y) \\log P_w(y \\mid x) \\] 对数似然函数\\(L(w)\\) \\[ L(w) = \\sum_{j=1}^{N} \\sum_{k=1}^K w_k f_k(y_j, x_j) - \\sum_{j=1}^N \\log Z_w(x_j) \\] 数据\\((x, y)\\) 中出现的特征总数\\(T(x, y)\\) ： \\[ T(x, y) = \\sum_k f_k(y, x) = \\sum_{k=1}^K \\sum_{i=1}^{n+1}f_k(y_{i-1}, y_i, x) \\] 输入：特征函数\\(t_1, t_2, \\cdots, t_{K_1}\\)和\\(s_1, s_2, \\cdots, s_{K_2}\\) ；经验分布\\(\\hat P(x, y)\\) 输出：模型参数\\(\\hat w\\)，模型\\(P_{\\hat w}\\) 步骤： 1 赋初值 \\(w_k = 0\\) 2 对所有\\(k\\)，求解方程，解为\\(\\delta_k\\) \\[ \\begin{align} &amp; \\sum_{x, y} \\hat P(x) P(y \\mid x) \\sum_{i=1}^{n+1} t_k(y_{i-1}, y_i, x, i) \\exp (\\delta_k T(x, y)) = E_{\\hat p}[t_k] , \\quad k = 1, 2, \\cdots, K_1 时 \\\\ &amp; \\sum_{x, y} \\hat P(x) P(y \\mid x) \\sum_{i=1}^{n+1} s_l(y_{i-1}, y_i, x, i) \\exp (\\delta_k T(x, y)) = E_{\\hat p}[s_l] , \\quad k = K_1 + l, l = 1, 2, \\cdots, K_2 时 \\\\ \\end{align} \\] 3 更新\\(w_k + \\delta_k \\to w_k\\) ，如果还有\\(w_k\\)未收敛，则继续2 算法S 对于不同的数据\\((x, y)\\)的特征出现次数\\(T(x, y)\\) 可能不同，可以选取一个尽量大的数\\(S\\)作为特征总数，使得所有松弛特征\\(s(x, y) \\ge 0\\) ： \\[ s(x, y) = S - \\sum_{i=1}^{n+1}\\sum_{k=1}^K f_k(y_{i-1}, y_i, x, i) \\] 所以可以直接解得\\(\\delta_k\\) ，当然\\(f_k\\) 要分为\\(t_k\\)和\\(s_k\\)，对应的期望值计算也不一样。具体见书上。 \\[ \\delta_k = \\frac{1}{S} \\log \\frac{E_{ \\hat P}[f_k] } {E_P[f_k]} \\] 算法T 算法S中\\(S\\)会选择很大，导致每一步的迭代增量会加大，算法收敛会变慢，算法T重新选择一个特征总数 T(x) \\[ T(x) = \\max \\limits_y T(x, y) \\] 使用前后向递推公式，可以算得\\(T(x)=t\\) 。 对于\\(k \\in [1, K_1]\\)的\\(t_k\\)关于经验分布的期望： \\[ E_{\\hat P}[t_k] = \\sum_{t=0}^{T_{max}} a_{k,t} \\beta_{k}^t \\] 其中，\\(a_{k,t}\\)是\\(t_k\\)的期待值， \\(\\delta_k = \\log \\beta_k\\) 对于\\(k \\in [1+K_1, K]\\)的\\(s_k\\)关于经验分布的期望： \\[ E_{\\hat P}[s_k] = \\sum_{t=0}^{T_{max}} b_{k,t} \\gamma_{k}^t \\] 其中\\(\\gamma_k^t\\)是特征\\(s_k\\)的期望值，\\(\\delta_k = \\log \\gamma_k\\)。当然，求根也可以使用牛顿法去求解。 拟牛顿法 预测算法 预测问题 给定条件随机场\\(P(Y \\mid X)\\)和输入序列\\(x\\)，求条件概率最大的输出序列（标记序列）\\(y^*\\)，即对观测序列进行标注。 \\[ \\begin{align} y^* &amp; = \\arg \\max \\limits_y P_w(y \\mid x) = \\arg \\max \\limits_y \\frac{\\exp (w \\cdot F(y, x))}{Z_w(x)} \\\\ &amp; = \\arg \\max \\limits_y ( w \\cdot F(y, x)) \\\\ \\end {align} \\] 其中路径\\(y\\)表示标记序列，下面是参数说明 \\[ \\begin {align} &amp; w = (w_1, w_2, \\cdots, w_k)^T \\\\ &amp; F(y, x) = (f_1(y, x), \\cdots, f_K(y, x))^T, \\quad w = (w_1, \\cdots, w_K)^T \\\\ &amp; f_k (y, x) = \\sum _{i=1} ^n f_k(y_{i-1}, y_i, x, i) \\\\ &amp; F_i(y_{i-1}, y_i, x) = \\left(f_1(y_{i-1}, y_i, x, i), f_2(y_{i-1}, y_i, x, i),\\cdots, f_k(y_{i-1}, y_i, x, i) \\right)^T \\end {align} \\] 所以，为了求解最优路径，只需计算非规范化概率，即转换为下面的问题： \\[ \\max \\limits_y \\quad \\sum_{i=1}^n w \\cdot F_i(y_{i-1}, y_i, x) \\] 维特比算法 HMM的维特比算法。 维特比变量\\(\\delta_i(l)\\)，到达位置\\(i\\)， 标记为\\(l \\in [1, m]\\) 的概率 \\[ \\delta_i(l) = \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\}, \\quad j = 1, 2, \\cdots, m \\] 记忆路径\\(\\psi_i(l) = a\\) 当前时刻\\(t\\)标记为l, \\(t-1\\)时刻标记为a \\[ \\psi_i(l) = = \\arg \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\} \\] 算法主体 输入：特征向量\\(F(y, x)\\)和权值向量\\(\\mathbf{w}\\)，观测向量\\(x = (x_1, x_2, \\cdots. x_n)\\) 输出：最优路径\\(y^* = (y_1^*, y_2^*, \\cdots, y_n^*)\\) 步骤如下 初始化 \\[ \\delta_1(j) = w \\cdot F_1(y_0 = start, y_1 = j, x), \\quad j = 1, \\cdots, m \\] 递推 \\[ \\begin{align} &amp; \\delta_i(l) = \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\}, \\quad j = 1, 2, \\cdots, m \\\\ &amp; \\psi_i(l) = \\arg \\max \\limits_{1 \\le j \\le m} \\delta_i(j), \\quad \\text{即上式的参数j} \\\\ \\end{align} \\] 终止 \\[ \\begin{align} &amp; \\max \\limits_y (w \\cdot F(y, x)) = \\max \\limits_{1 \\le j \\le m} \\delta_n(j) \\\\ &amp; y_n^* = \\arg \\max \\limits_{1 \\le j \\le m} \\delta_n(j) \\\\ \\end{align} \\] 返回路径 \\[ y_i^* = \\psi_{i+1} (y_{i+1}^*), \\quad i = n-1, n-2, \\cdots, 1 \\] 求得最优路径\\(y^* = (y_1^*, y_2^*, \\cdots, y_n^*)\\)","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"条件随机场","slug":"条件随机场","permalink":"http://plmsmile.github.io/tags/条件随机场/"}]},{"title":"最大熵模型","date":"2017-09-20T09:39:12.000Z","path":"2017/09/20/maxentmodel/","text":"最大熵原理 预备知识 离散型变量\\(X\\)的概率分布是\\(\\color{blue}{P(X)}\\)。它的熵\\(\\color{blue}{H(X) \\; or \\; H(P)}\\)越大，代表越均匀、越混乱、越不确定。各种熵点这里 \\[ \\color{blue}{H(P)} = \\color{red} {- \\sum_{x \\in X}P(x) \\log P(x)} \\] 熵满足下面不等式 \\[ 0 \\le H(P) \\le \\log |X|, \\quad 其中|X|是X的取值个数 \\] 当前仅当\\(X\\)的分布是均匀分布的时候等号成立。当\\(X\\)服从均匀分布时，熵最大。 最大熵的思想 最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。 事情分为两个部分：确定的部分（约束条件）和不确定的部分。选择模型时要 要满足所有的约束条件，即满足已有的确定的事实 要均分不确定的部分 \\(X\\)有5个取值\\(\\{A, B,C,D,E\\}\\)，取值概率分别为\\(P(A), P(B), P(C), P(D), P(E)\\)。满足以下约束条件 \\[ P(A)+ P(B)+ P(C)+ P(D)+ P(E) = 1 \\] 满足这个条件的模型有很多。再加一个约束条件 \\[ P(A) + P(B) = \\frac{3}{10} \\] 则，满足约束条件，不确定的平分（熵最大）：这样的模型是最好的模型 \\[ P(A) = P(B) = \\frac{3}{20}, \\quad P(C)=P(D)=P(E) = \\frac{7}{30} \\] 即：约束条件，熵最大 最大熵模型 假设分类模型是一个条件概率分布\\(\\color{blue}{P(Y \\mid X)}\\)。(有的不是选择条件模型，如论文里面)。训练数据集\\(N\\)个样本 \\(T = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}\\) 基本概念 联合分布：\\(\\color{blue}{P(X, Y)}\\) 边缘分布：\\(\\color{blue}{P(X)}\\) 联合经验分布：\\(\\color{blue}{\\widetilde{P}(X, Y)} = \\color{red}{\\frac {v(X=x, Y=y)}{N}}\\)，其中\\(v(x, y)\\)为频数 联合边缘分布：\\(\\color{blue}{\\widetilde P(X) = \\color{red} {\\frac{v(X=x)}{N}}}\\) 特征函数\\(\\color{blue}{f(x, y)}\\)用来描述\\(x\\)和\\(y\\)满足的一个事实约束条件： \\[ f(x, y) = \\begin{cases} 1, \\quad &amp; x与y满足一个事实，即约束条件 \\\\ 0, \\quad &amp; 否则 \\end{cases} \\] 如果有\\(n\\)个特征函数\\(\\color{blue}{f_i(x, y), i = 1, 2, \\cdots, n}\\), 就有\\(n\\)个约束条件。 概率期望的计算 \\(X\\)的期望 \\(X\\) 是随机变量，概率分布是\\(P(X)\\) ，或概率密度函数是\\(f(x)\\) \\[ E(X) = \\begin{cases} &amp;\\sum_{i} x_i P(x_i), \\quad &amp; \\text{离散} \\\\ &amp; \\int_{-\\infty}^{+\\infty} {x \\cdot f(x)} \\, {\\rm d}x , \\quad &amp; \\text{连续} \\\\ \\end{cases} \\] 下面只考虑离散型的期望，连续型同理，求积分即可。 一元函数的期望 \\(Y = g(X)\\)，期望是 \\[ E[Y] = E[g(X)] = \\sum_{i}^{\\infty} g(x_i) \\cdot P(x_i) \\] 二元函数的期望 \\(Z = g(X, Y)\\) ，期望是 \\[ E(Z) = \\sum_{x, y} g(x, y) \\cdot p(x, y) = \\sum_{i=1} \\sum_{j=1} g(x_i, y_j) p(x_i, y_j) \\] 期望其实就是\\(E 狗 = \\sum 狗 \\cdot 老概率\\) 。可离散，可连续。 约束条件等式 实际分布期望 特征函数\\(f(x, y)\\)关于经验分布\\(\\color{blue}{\\widetilde P(x, y)}\\)的期望\\(\\color{blue}{E_{\\widetilde P}(f)}\\)，即实际应该有的特征 ，也就是一个给模型加的约束条件 ： \\[ \\color{blue}{E_{\\widetilde P}(f)} = \\sum_{x, y} \\color{red} {\\widetilde P(x,y)} f(x, y) \\] 理论模型期望 特征函数\\(f(x, y)\\) 关于模型\\(\\color{blue}{P(Y\\mid X)}\\)和经验分布\\(\\color{blue}{\\widetilde P(X)}\\)的期望\\(\\color{blue}{E_{P}(f)}\\) ，即理论上模型学得后的期望： \\[ \\color{blue}{E_{ P}(f)} =\\sum_{x, y} \\color{red} { \\widetilde{P}(x) P(y \\mid x)} f(x, y) \\] 要从训练数据中获取信息，特征函数关于实际经验分布和理论模型的两个期望就得相等，即理论模型要满足实际约束条件 \\[ E_{\\widetilde P}(f) = E_{ P}(f) \\] 最大熵模型思想 条件概率分布\\(P(Y \\mid X)\\)的条件熵为\\(\\color{blue}{H(P)}\\)如下，条件熵： \\[ \\color{blue}{H(P)} = \\color{red} {- \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 则满足约束条件\\(\\color{blue}{E_{\\widetilde P}(f) = E_{ P}(f)}\\)的模型中，条件熵\\(\\color{blue}{H(P)}\\)最大的模型就是最大熵模型。 最大熵模型的学习 学习问题 最大熵模型的学习等价于约束最优化问题，这类问题可以用拉格朗日对偶性去求解。 给定数据集\\(T = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}\\)和特征函数\\(\\color{blue}{f_i(x, y), i = 1, 2, \\cdots, n}\\)。 要满足2个约束条件 \\[ \\color{red} {E_{\\widetilde P}(f) = E_{ P}(f), \\quad \\sum_{x, y}P(y \\mid x) = 1 } \\] 要得到最大化熵 \\[ \\max \\limits_{P \\in C} \\; \\color{blue}{H(P)} = \\color{red} {- \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题 ，如下 \\[ \\min \\limits_{P \\in C} \\; \\color{blue}{ - H(P)} = \\color{red} { \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 推导最大熵模型 一般使用拉格朗日对偶性去求解，可以见李航书附录。 引入拉格朗日乘子\\(\\color{blue}{w = (w_0, w_1, \\cdots, w_n)}\\)，即参数向量，构造拉格朗日函数\\(\\color{blue}{L(P, w)}\\) ： \\[ \\color{blue}{L(P, w)} = \\color{red}{ -H(P) + w_0 \\cdot \\left( 1 - \\sum_{x, y} P(y \\mid x)\\right) + \\sum_{i=1}^n w_i \\cdot \\left(E_{\\widetilde P}(f) - E_{ P}(f) \\right) } \\] 由于是凸函数，根据相关性质，所以原始问题和对偶问题同解，原始问题如下： \\[ \\min \\limits_{P \\in C} \\max \\limits_{w} L(P, w) \\] 对应的对偶问题如下： \\[ \\max \\limits_{w} \\min \\limits_{P \\in C} L(P, w) \\] 主要思路是：先固定\\(\\color{blue}{w}\\)，去计算\\(\\color{blue} {\\min \\limits_{P \\in C} L(P, w)}\\)，即去找到一个合适的\\(\\color{blue}{P(Y \\mid X)}\\)。再去找到一个合适的\\(\\color{blue}{w}\\)。 第一步：求解\\(\\color{blue}{P}\\) 。设对偶函数\\(\\color{blue} { \\Psi (w)}\\)如下： \\[ \\color{blue} { \\Psi (w)} = \\color{red} {\\min \\limits_{P \\in C} L(P, w) = L(P_w, w)} \\] 对偶函数的解，即我们找到的\\(P(Y \\mid X)\\)，记作\\(\\color{blue}{P_w}\\)，如下： \\[ \\color{blue}{P_w} = \\arg \\min \\limits_{P \\in C} L(P, w) =\\color{red} {P_w(y \\mid x)} \\] 用\\({L(P, w)}\\)对\\(P\\)进行求偏导，令偏导为0，可以解得\\({P_w}\\)，即最大熵模型 如下： \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 其中\\(\\color{blue}{Z_w(x)}\\)是归一化因子，\\(\\color{blue} {f_i(x, y)}\\)是特征函数，\\(\\color{blue}{w_i}\\)是特征的权值，\\(\\color{blue}{P_w(y \\mid x)}\\) 就是最大熵模型， \\(\\color{blue}{w}\\)是最大熵模型中的参数向量。 第二步：求解\\(\\color{blue}{w}\\)。即求\\(w\\)去最大化对偶函数，设解为\\(\\color{blue} {w^*}\\) 。可以使用最优化算法去求极大化。 \\[ \\color{blue}{w^*} = \\color{red} {\\arg \\max \\limits_{w} \\Psi(w)} \\] 最终，求到的\\(\\color{blue} {P^* = P_{w^*} = P_{w^*}(y \\mid x)}\\)就是学习得到的最大熵模型。 最大熵模型 最大熵模型如下，其中\\(\\color{blue}{Z_w(x)}\\)是归一化因子，\\(\\color{blue} {f_i(x, y)}\\)是特征函数，\\(\\color{blue}{w_i}\\)是特征的权值 。 \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 极大似然估计 其实对偶函数\\(\\color{blue}{\\Psi(w)}\\)的极大化等价于最大熵模型的极大似然估计。 已知训练数据的经验分布\\(\\widetilde{P}(X, Y)\\)，条件概率分布的\\(P(Y \\mid X)\\)的对数似然函数是： \\[ \\begin{align*} \\color{blue} {L_{\\widetilde P}(P_w)} &amp; = \\log \\prod_{x, y} P(y \\mid x) ^ {\\widetilde{P}(X, Y)} = \\sum_{x, y} \\widetilde P(x, y) \\log P(y \\mid x) \\\\ &amp; = \\color{red}{\\sum_{x, y}\\widetilde{P}(X, Y) \\sum_{i=1}^{n} w_i f_i(x, y) - \\sum_{x} \\widetilde{P}(X)\\log Z_w(x) } \\end{align*} \\] 可以证明得到，$L_{P}(P_w) = (w) $，极大似然函数等于对偶函数。 模型学习的最优化算法 逻辑回归、最大熵模型的学习都是以似然函数为目标函数的最优化问题，可以通过迭代算法求解。这个目标函数是个光滑的凸函数。通过很多方法都可以保证找到全局最优解，常用的有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法，其中牛顿法和拟牛顿法一般收敛速度更快。 改进的迭代尺度法 改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习的最优化算法。 已知最大熵模型如下： \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 对数似然函数如下： \\[ \\color{blue} {L_{\\widetilde P}(P_w)} = \\color{red}{\\sum_{x, y}\\widetilde{P}(X, Y) \\sum_{i=1}^{n} w_i f_i(x, y) - \\sum_{x} \\widetilde{P}(X)\\log Z_w(x) } \\] 目标是：通过极大似然估计学习模型参数，即求对数似然函数的极大值\\(\\color{blue} {\\hat w}\\)。 基本思想 当前参数向量\\(w = (w_1, w_2, \\cdots, w_n)^T\\)，找到一个新的参数向量\\(w+\\delta = (w_1 + \\delta_1, w_2 + \\delta_2, \\cdots, w_n + \\delta_n)\\)，使得每次更新都使似然函数值增大。 由于\\(\\delta\\)是一个向量，含有多个变量，不易同时优化。所以IIS 每次只优化其中一个变量\\(\\delta_i\\)，而固定其他变量\\(\\delta_j\\)。 设所有特征在\\((x, y)\\)中的出现次数\\(f^\\#(x, y) = M\\) ： \\[ \\color{blue}{f^\\# (x, y)} = \\sum_i f_i(x, y) \\] 计算每次的改变量： \\[ L(w+\\delta) - L(w) \\ge \\color{blue}{B(\\delta \\mid w)}, \\; 改变量的下界限 \\] 如果找到适当的\\(\\delta\\)使得改变量的下界\\(B(\\delta \\mid w)\\)提高，则对数似然函数也能提高。 计算\\(B(\\delta \\mid w)\\)对\\(\\delta_i\\)求偏导，令偏导等于0，得如下方程： \\[ \\color{red} { \\sum_{x, y} \\widetilde P(x) P_w(y \\mid x) f_i(x, y) \\exp \\left(\\delta_i f^\\#(x, y)\\right) = E_{\\widetilde p}(f_i) }, \\quad 其中\\, E_{\\widetilde p}(f_i) = \\sum_{x, y} \\widetilde P(x, y)f_i(x, y) \\] 然后，依次对\\(\\delta_i​\\)求解该方程，就可以求得\\(\\delta​\\)，也就能够更新\\(w​\\)，即\\(w \\to w+\\delta​\\) 算法步骤 输入：特征函数\\(f_1, \\cdots, f_n\\)；经验分布\\(\\widetilde P(x, y)\\)，模型\\(P_w(y \\mid x)\\) 输出：最优参数值\\(w_i^*\\)；最优模型\\(P_{w^*}\\) 初始化参数，取初值 \\(w_i = 0\\) 求解方程 \\(\\delta_i\\) \\[ \\sum_{x, y} \\widetilde P(x) P_w(y \\mid x) f_i(x, y) \\exp \\left(\\delta_i f^\\#(x, y)\\right) = E_{\\widetilde p}(f_i), \\] 更新参数 \\(w_i + \\delta_i \\to w_i\\) 其中解方程的时候，如果特征出现次数\\(f^\\#(x, y)\\) 是常数\\(M\\)，则可以直接计算\\(\\delta_i\\) ： \\[ \\color{blue}{\\delta_i } = \\color{red} {\\frac{1}{M} \\log \\frac{E_{\\widetilde p}(f_i)}{E_{p}(f_i)} } \\] 如果\\(f^\\#(x, y)\\)不是常数，则必须通过数值计算\\(\\delta_i\\)。最简单就是通过牛顿迭代法去迭代求解\\(\\delta_i^*\\)。以\\(g(\\delta_i) = 0\\) 表示该方程，进行如下迭代： \\[ \\color{blue} {\\delta_i^{(k+1)}} = \\color{red} {\\delta_i^{(k)} - \\frac{g(\\delta_i^{(k)})}{g^\\prime (\\delta_i^{(k)})} } \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"各种熵","slug":"各种熵","permalink":"http://plmsmile.github.io/tags/各种熵/"},{"name":"最大熵模型","slug":"最大熵模型","permalink":"http://plmsmile.github.io/tags/最大熵模型/"},{"name":"IIS","slug":"IIS","permalink":"http://plmsmile.github.io/tags/IIS/"},{"name":"期望","slug":"期望","permalink":"http://plmsmile.github.io/tags/期望/"}]},{"title":"机器学习笔记","date":"2017-08-20T13:38:54.000Z","path":"2017/08/20/ml-ng-notes/","text":"线性回归 有\\(m\\)个样本\\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(m)}, y^{(m)})\\)，假设函数有2个参数\\(\\theta_0, \\theta_1\\)，形式如下： \\[ h_\\theta(x) = \\theta_0 + \\theta_1x \\] 代价函数 代价函数 \\[ \\color{red} {J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2} \\] 目标是要找到合适的参数，去最小化代价函数\\(min\\, J(\\theta_0, \\theta_1)\\)。 假设\\(\\theta_0 = 0\\)，去描绘出\\(J(\\theta_1)\\)和\\(\\theta_1\\)的关系，如下面右图所示。 假设有3个样本\\((1,1), (2,2), (3,3)\\)，图中选取了3个\\(\\theta_1 = 1, 0.5, 0\\)，其中\\(J(\\theta_1)\\)在\\(\\theta_1=1\\)时最小。 那么回到最初的两个参数\\(h_\\theta(x) = \\theta_0 + \\theta_1x\\)，如何去找\\(min\\, J(\\theta_0, \\theta_1)\\)呢？这里绘制一个等高图去表示代价函数，如下面右图所示，其中中间点是代价最小的。 梯度下降 基础说明 上文已经定义了代价函数\\(J(\\theta_0, \\theta_1)\\)，这里要使用梯度下降算法去最小化\\(J(\\theta_0, \\theta_1)\\)，自动寻找出最合适的\\(\\theta\\)。梯度下降算法应用很广泛，很重要。大体步骤如下： 设置初始值\\(\\theta_0, \\theta_1\\) 不停改变\\(\\theta_0, \\theta_1\\)去减少\\(J(\\theta_0, \\theta_1)\\) 当然选择不同的初始值，可能会得到不同的结果，得到局部最优解。 对于所有的参数\\(\\theta_j\\)进行同步更新，式子如下 \\[ \\color{red}{\\theta_j = \\theta_j - \\underbrace{\\alpha \\cdot \\frac{\\partial}{\\partial_{\\theta_j}} J(\\theta_0, \\theta_1)}_{学习率 \\times 偏导}} \\] 上面公式中\\(\\color{blue}{\\alpha}\\)是学习率(learning rate)，是指一次迈多大的步子，一次更新的幅度大小。 例如上面的两个参数，对于一次同步更新(梯度下降) \\[ t_0 = \\theta_0 - \\alpha \\frac{\\partial}{\\partial_{\\theta_0}} J(\\theta_0, \\theta_1), t_1 = \\theta_1 - \\alpha \\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1) \\quad \\to\\quad \\theta_0 = t_0, \\theta_1 = t_1 \\] 也有异步更新(一般指别的算法) \\[ t_0 = \\theta_0 - \\alpha \\frac{\\partial}{\\partial_{\\theta_0}} J(\\theta_0, \\theta_1),\\theta_0 = t_0 \\quad \\to\\quad t_1 = \\theta_1 - \\alpha \\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1), \\theta_1 = t_1 \\] 偏导和学习率 这里先看一个参数的例子，即\\(J(\\theta_1)\\)。\\(\\theta_1 = \\theta_1 - \\alpha \\frac{d}{dx}J(\\theta_1)\\)。当\\(\\theta\\)从左右靠近中间值，导数值(偏导/斜率)分别是负、正，所以从左右两端都会靠近中间值。 当学习率\\(\\alpha\\)太小，梯度下降会很缓慢；\\(\\alpha\\)太大，可能会错过最低点，导致无法收敛。 当已经处于局部最优的时候，导数为0，并不会改变参数的值，如下图 当逐渐靠近局部最优的时候，梯度下降会自动采取小步子到达局部最优点。是因为越接近，导数会越来越小。 在线性回归上使用梯度下降 代价函数 \\[ \\color{blue} {J(\\theta_0, \\theta_1)} = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2 = \\color{red}{\\frac{1}{2m} \\sum_{i=1}^m (\\theta_0 + \\theta_1x^{(i)} -y^{(i)})^2} \\] 分别对\\(\\theta_0\\)和\\(\\theta_1\\)求偏导有 \\[ \\color{blue}{\\frac{\\partial}{\\partial_{\\theta_0}}J(\\theta_0, \\theta_1) }= \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right), \\quad \\color{blue}{\\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1)}= \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)\\cdot x^{(i)} \\] 那么使用梯度下降对\\(\\theta_0 和 \\theta_1\\)进行更新，如下 \\[ \\theta_0 = \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right), \\quad \\theta_1 = \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)\\cdot x^{(i)} \\] 当前代价函数实际上是一个凸函数，如下图所示。它只有全局最优，没有局部最优。 通过不断地改变参数减小代价函数\\(\\color{blue} {J(\\theta_0, \\theta_1)}\\)，逼近最优解，最终会得到一组比较好的参数，正好拟合了我们的训练数据，就可以进行新的值预测。 梯度下降技巧 特征缩放 不同的特征的单位的数值变化范围不一样，比如\\(x_1 \\in (0,2000), x_2 \\in (1,5)\\)，这样会导致代价函数\\(J(\\theta)\\)特别的偏，椭圆。这样来进行梯度下降会特别的慢，会来回震荡。 所以特征缩放是把所有的特征缩放到相同的规模上。得到的\\(J(\\theta)\\)就会比较圆，梯度下降能很快地找到一条通往全局最小的捷径。 特征缩放的数据规模不能太小或者太大，如下面可以的规模是 \\[ [-1, 1], [0, 3], [-2, 0.5], [-3, 3], [-\\frac{1}{3}, \\frac{1}{3}] 都是可以的。而[-100, 100], [-0.0001, 0.0001]是不可以的 \\] 有一些常见的缩放方法 \\(x_i = \\frac{x_i - \\mu}{max - min}\\), \\(x_i = \\frac{x_i - \\mu}{s}\\)，其中\\(\\mu\\)是均值，\\(s\\)是标准差 \\(x_i = \\frac{x_i - min} {max - min}\\) \\(x_i = \\frac{x_i}{max}\\) 学习率的选择 当梯度下降正确运行的时候，每一次迭代\\(J(\\theta)\\)都会减少，但是减少到什么时候合适呢？当然最好的办法就是画图去观察，当然也可以设定减小的最小值来判断。下图中， 迭代次数到达400的时候就已经收敛。不同的算法，收敛次数不一样。 当图像呈现如下的形状，就需要使用更小的学习率。理论上讲，只要使用足够小的学习率，\\(J(\\theta)\\)每次都会减少。但是太小的话，梯度下降会太慢，难以收敛。 学习率总结 学习率太小，慢收敛 学习率太大，\\(J(\\theta)\\)可能不会每次迭代都减小，甚至不会收敛 这样去选择学习率调试： \\(\\ldots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \\ldots\\) 多变量线性回归 数据有\\(n\\)个特征，如\\(x^{(i)} = (1, x_1, x_2, \\cdots, x_n)\\)，其中\\(x_0 = 1\\)。则假设函数有\\(n+1\\)个参数，形式如下 \\[ \\color{blue}{h_\\theta(x)} = \\theta^Tx = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\] 代价函数 \\[ \\color{blue}{J(\\theta)} = \\frac{1}{2m} \\sum_{i=1}^m\\left( h_\\theta(x^{(i)}) - y^{(i)}\\right) ^ 2 \\] 梯度下降，更新每个参数\\(\\theta_j\\) \\[ \\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial_{\\theta_j}} =\\color{red}{ \\theta_j -\\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left(h_\\theta(x^{(i)})-y^{(i)} \\right) \\cdot x_j^{(i)}} \\] 多项式回归 有时候，线性回归并不能很好地拟合数据，所以我们需要曲线来适应我们的数据。比如一个二次方模型 \\[ h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x^2_2 \\] 当然可以用\\(x_2 = x^2_2, x_3 = x_3^3\\)来转化为多变量线性回归。如果使用多项式回归，那么在梯度下降之前，就必须要使用特征缩放。 正规方程 对于一些线性回归问题，使用正规方程方法求解参数\\(\\theta\\)，比用梯度下降更好些。代价函数如下 \\[ \\color{red} {J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2} \\] 正规方程的思想是函数\\(J(\\theta)\\)对每个\\(\\theta_j\\)求偏导令其等于0，就能得到所有的参数。即\\(\\frac{\\partial J}{\\partial \\theta_j} = 0\\)。 那么设\\(X_{m\\times(n+1)}\\)为数据矩阵（其中包括\\(x_0=1\\)），\\(y\\)为标签向量。则通过如下方程可以求得\\(\\theta\\) \\[ \\theta = (X^TX)^{-1}X^Ty \\] 正规方程和梯度下降的比较 梯度下降 正规方程 需要特征缩放 不需要特征缩放 需要选择学习率\\(\\alpha\\) 不虚选择学习率 需要多次迭代计算 一次运算出结果 特征数量\\(n\\)很大时，依然适用 \\(n\\)太大，求矩阵逆运算代价太大，复杂度为\\(O(n^3)\\)。\\(n\\leq10000\\)可以接受 适用于各种模型 只适用于线性模型，不适合逻辑回归和其他模型 逻辑回归 1&lt;img src=\"\" style=\"display:block; margin:auto\" width=\"60%\"&gt; 线性回归有2个不好的问题：直线难以拟合很多数据；数据标签一般是\\(0, 1\\)，但是\\(h_\\theta(x)\\)却可能远大于1或者远小于0。如下图。 基本模型 逻辑回归是一种分类算法，使得输出预测值永远在0和1之间，是使用最广泛的分类算法。模型如下 \\[ h_\\theta(x) = g(\\theta^Tx), \\quad g(z) = \\frac{1}{1+e^{-z}} \\] \\(g(z)\\)的图像如下，也称作Sigmoid函数或者Logistic函数，是S形函数。 将上面的公式整理后得到逻辑回归的模型 \\[ \\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}, \\quad 其中\\; \\color{red}{0 \\le h_\\theta(x) \\le 1} \\] 模型的意义是给出分类为1的概率，即\\(h_\\theta(x) = P(y=1\\mid x; \\theta)\\)。例如\\(h_\\theta(x)=0.7\\)，则分类为1的概率是0.7，分类为0的概率是\\(1-0.7=0.3\\)。 \\[ x的分类预测, y = \\begin{cases} 1, \\; &amp; h_\\theta(x) \\ge 0.5, \\;即\\; \\theta^Tx \\ge 0\\\\ 0, \\; &amp; h_\\theta(x) &lt; 0.5, \\; 即 \\; \\theta^Tx &lt; 0 \\\\ \\end{cases} \\] 决策边界 线性边界 假设我们有一个模型\\(h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2)\\)，已经确定参数\\(\\theta = (-3, 1, 1)\\)，即模型\\(h_\\theta(x) = g(-3+x_1+x_2)\\)，数据和模型如下图所示 由上可知，分类结果如下 \\[ y = \\begin{cases} 1, \\, &amp; x_1+ x_2 \\ge 3 \\\\ 0, \\, &amp; x_1 + x_2 &lt; 3 \\\\ \\end{cases} \\] 那么直线\\(x_1+x_2=3\\)就称作模型的决策边界，将预测为1的区域和预测为0的区域分隔开来。gg 非线性边界 先看下面的数据 使用这样的模型去拟合数据 \\[ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_1^2 + \\theta_4x_2^2) , \\; \\theta = (-1, 0, 0, 1, 1), \\; 即\\, \\color{red}{h_\\theta(x) = g(-1+x_1^2 + x_2^2)} \\] 对于更复杂的情况，可以用更复杂的模型去拟合，如\\(x_1x_2, x_1x_2^2\\)等 代价函数和梯度下降 我们知道线性回归中的代价函数是\\(J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2\\)，但是由于逻辑回归的模型是\\(\\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}\\)，所以代价函数关于\\(\\theta\\)的图像就是一个非凸函数，容易达到局部收敛，如下图左边所示。而右边，则是一个凸函数，有全局最小值。 代价函数 逻辑回归的代价函数 \\[ \\color{red}{ Cost(h_\\theta(x), y) = \\begin{cases} -\\log(h_\\theta(x)),\\; &amp; y=1 \\\\ -\\log(1-h_\\theta(x)), \\; &amp; y=0 \\\\ \\end{cases} } \\] 当实际上\\(y=1\\)时，若预测为0，则代价会无穷大。当实际上\\(y=0\\)时，若预测为1，则代价会无穷大。 整理代价函数如下 \\[ \\color{red}{Cost(h_\\theta(x), y) = -y \\cdot \\log(h_\\theta(x)) - (1-y) \\cdot \\log(1- h_\\theta(x))} \\] 得到所有的\\(\\color{red}{J(\\theta)}\\) \\[ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)}\\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right) \\] 梯度下降 逻辑回归的假设函数估计\\(y=1\\)的概率 \\(\\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}\\)。 代价函数\\(\\color{red}{J(\\theta)}\\)，求参数\\(\\theta\\)去\\(\\color{red}{\\min \\limits_{\\theta} J(\\theta)}\\) 对每个参数\\(\\theta_j\\)，依次更新参数 \\[ \\color{red} {\\theta_j = \\theta_j -\\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left(h_\\theta(x^{(i)})-y^{(i)} \\right) \\cdot x_j^{(i)}} \\] 逻辑回归虽然梯度下降的式子和线性回归看起来一样，但是实际上\\(h_\\theta(x)\\)和\\(J(\\theta)\\)都不一样，所以是不一样的。 gg","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://plmsmile.github.io/tags/逻辑回归/"},{"name":"梯度下降","slug":"梯度下降","permalink":"http://plmsmile.github.io/tags/梯度下降/"}]},{"title":"最大期望算法","date":"2017-08-13T10:37:48.000Z","path":"2017/08/13/em/","text":"EM算法定义 背景 如果概率模型的变量都是观测变量，那么可以直接使用极大似然估计法或贝叶斯估计法去估计模型参数。 如果模型既有观测变量又有隐变量，就不能简单使用上述方法。 EM算法，期望极大算法，就是含有隐变量的概率模型参数的极大似然估计法或极大后验概率估计法，是一种迭代算法。每次迭代分为如下两步 E步：求期望(expectation) M步：求极大(maximization) 三硬币模型 有3枚硬币，记做ABC，每次出现正面的概率分别是\\(\\pi, p, q\\)。先掷A，正面选B，反面选C。再掷B/C，得到正面1或反面0作为一次的结果。问：去估计参数\\(\\theta = (\\pi, p, q)\\) 观察变量：一次 实验得到的结果1或0，记做\\(y\\) 隐变量：A的结果，即掷的是B还是C，记做\\(z\\) 对于一次实验，求出\\(y\\)的概率分布 \\[ \\begin{align*} \\color{blue}{P(y \\mid \\theta)} &amp;= \\underbrace{\\sum_zP(y, z \\mid \\theta)}_{\\color{red}{把所有z的y加起来}} = \\sum_z \\underbrace{P(z \\mid \\theta)P(y \\mid z, \\theta)}_{\\color{red}{贝叶斯公式}} = \\underbrace{\\pi p^y(1-p)^{1-y}}_{\\color{red}{z=1时}}+\\underbrace{(1-\\pi)q^y(1-q)^{1-y}}_{\\color{red}{z=0时}} \\end{align*} \\] 设观察序列\\(Y=(y_1, y_2, \\cdots, y_n)^T\\)，隐藏数据\\(Z=(z_1, z_2, \\cdots, z_n)^T\\)，则观测数据的似然函数为 \\[ \\begin{align*} \\color{blue}{P(Y \\mid \\theta)} &amp;= \\sum_Z \\color{red}{P(Z \\mid \\theta)P(Y \\mid Z, \\theta)} \\\\ &amp;= \\prod_{i=1}^n[\\pi p^{y_i}(1-p)^{1-y_i} + (1-\\pi)q^{y_i}(1-q)^{1-y_i}] \\end{align*} \\] 求模型参数\\(\\theta = (\\pi, p, q)\\)的最大似然估计，即 \\[ \\hat\\theta = arg max_\\theta \\log P(Y \\mid \\theta) \\] 这个问题不能直接求解，只有通过迭代的方法求解。EM算法就是解决这种问题的一种迭代算法。先给\\(\\theta^{(0)}\\) 选择初始值，然后去迭代。每次迭代分为E步和M步。 EM算法 基本概念 一些概念如下 \\(Y\\) 观测变量，\\(Z\\) 隐变量 不完全数据：\\(Y\\)；概率分布：\\(P(Y \\mid \\theta)\\)；对数似然函数：\\(\\color{red} {L(\\theta) = \\log P(Y \\mid \\theta)}\\) 完全数据：\\(Y\\)和\\(Z\\)合在一起；概率分布：\\(P(Y, Z \\mid \\theta)\\)；对数似然函数：\\(\\log P(Y, Z \\mid \\theta)\\) EM算法通过迭代求\\(L(\\theta) = \\log P(Y \\mid \\theta)\\)的极大似然估计。 概率论函数的期望 设\\(Y\\)是随机变量\\(X\\)的函数，\\(Y = g(X)\\)，\\(g\\)是连续函数，那么 \\(X\\)是离散型变量，\\(X\\)的分布律为\\(P(X = x_i) = p_i, \\; i=1,2,3\\cdots\\)，则有 \\[ E(Y) = E(g(X)) = \\sum_{i=1}^{\\infty}g(x_i)p_i, \\quad 左式收敛时成立 \\] \\(X\\)是 连续型变量，\\(X\\)的概率密度为\\(f(x)\\)，则有 \\[ E(Y) = E(g(X)) = \\int_{-\\infty}^{+\\infty} {g(x)f(x)} \\, {\\rm d}x, \\quad 左式绝对收敛成立 \\] Q函数 \\(\\color{blue}{Q(\\theta, \\theta^{(i)})}\\)是EM算法的核心。它是完全数据的对数似然函数\\(\\log P(Y,Z \\mid \\theta)\\)的期望，是关于未观测数据\\(Z\\)的条件概率分布\\(P(Z \\mid Y, \\theta^{(i)})\\)，而\\(Z\\)的条件是在给定观测数据\\(Y\\)和当前参数\\(\\theta^{(i)}\\)。（都是后置定语，不通顺） \\[ \\begin {align*} \\color{blue}{Q(\\theta, \\theta^{(i)})} &amp;= E_Z[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}] = \\sum_Z \\color{red}{\\log P(Y, Z \\mid \\theta)P(Z \\mid Y, \\theta^{(i)})} \\end {align*} \\] 下面是我具体的理解 \\(\\color{blue}{\\theta^{(i)}}\\)是第\\(i\\)次迭代参数\\(\\theta\\)的估计值 \\(P(Z \\mid Y, \\theta^{(i)})\\)是以\\(Y\\)和当前参数\\(\\theta^{(i)}\\)的条件下的分布律，简写为\\(P(Z)\\)。类似于上面的\\(X\\) \\(P(Y, Z \\mid \\theta)\\) 是在以\\(\\theta\\)为参数的分布的联合概率密度，简写为\\(P(Y, Z)\\)。类似于上面的\\(Y=g(X)\\) 求对数似然函数\\(\\log P(Y, Z)\\)的期望，转移到隐变量\\(Z\\)上 把目标函数映射到\\(Z\\)上，\\(g(z) = \\log P(Y, Z)\\)，\\(E(g(z)) = \\sum_z \\log P(Y, Z) P(Z)\\) \\(\\color{blue}{Q(\\theta, \\theta^{(i)})}\\) 是因为要找到一个新的\\(\\theta\\)优于之前的\\(\\theta^{(i)}\\)，是代表的分布优于 EM算法步骤 输入：\\(\\color{blue}{Y}\\) 观测变量，\\(\\color{blue}{Z}\\) 隐藏变量，\\(\\color{blue}{P(Y, Z \\mid \\theta)}\\) 联合分布，条件分布 \\(\\color{blue}{P(Z \\mid Y, \\theta)}\\) 输出：模型参数\\(\\color{blue}{\\theta}\\) 步骤 选择参数初始值\\(\\color{blue}{\\theta^{(0)}}\\)，开始迭代 E步：第\\(i+1\\)次迭代， 求 \\(\\color{blue}{Q(\\theta, \\theta^{(i)})} = \\sum_Z \\color{red}{\\log P(Y, Z \\mid \\theta)P(Z \\mid Y, \\theta^{(i)})}\\) M步：求使\\(Q(\\theta, \\theta^{(i)})\\)极大化的\\(\\theta\\)，得到\\(i+1\\)次迭代新的估计值\\(\\color{blue}{\\theta^{(i+1)}} = \\color{red}{arg \\, max_\\theta (Q(\\theta, \\theta^{(i)}))}\\) 重复E和M步，直到收敛 Jensen不等式 凸函数与凹函数 从图像上讲，在函数上两点连接一条直线。直线完全在图像上面，就是凸函数 convex；完全在下面，就是凹函数 concave。 \\(f^{\\prime\\prime}(x) \\ge 0 \\implies f(x) 是凸函数; \\quad f^{\\prime\\prime}(x) \\le 0 \\implies f(x)是凹函数\\) Jensen不等式 函数\\(f(x)​\\)，上有两点\\(x_1, x_2​\\)，对于任意\\(\\lambda \\in [0,1]​\\) 如果\\(f(x)\\)是凸函数，\\(f(\\lambda x_1+(1-\\lambda)x_2) \\le \\lambda f(x_1) + (1-\\lambda)f(x_2)\\) 如果\\(f(x)\\)是凹函数，\\(f(\\lambda x_1+(1-\\lambda)x_2) \\ge \\lambda f(x_1) + (1-\\lambda)f(x_2)\\) 一般地，\\(n\\)个点\\(x_1, x_2, \\cdots, x_n\\)和参数\\(\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n = 1\\)，对于凸函数，则有 \\[ f(\\underbrace{\\lambda_1x_1 + \\lambda_2x_2 + \\cdots + \\lambda_nx_n}_{\\color{red}{E[X], 总体是f(E[X])}}) \\le \\underbrace{f(\\lambda_1x_1) + f(\\lambda_2x_2) + \\cdots + f(\\lambda_nx_n)}_{\\color{red}{E[f(X)]}}, \\;即\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\leq \\sum_{i=1}^nf(\\lambda_ix_i)} \\] 琴声不等式 凸函数 \\(f(E[X]) \\le E[f(X)]\\)，即\\(\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\leq \\sum_{i=1}^nf(\\lambda_ix_i)}\\) 凹函数 \\(f(E[X]) \\ge E[f(X)]\\)，即\\(\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\geq \\sum_{i=1}^nf(\\lambda_ix_i)}\\)","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"最大期望算法","slug":"最大期望算法","permalink":"http://plmsmile.github.io/tags/最大期望算法/"},{"name":"Jesen不等式","slug":"Jesen不等式","permalink":"http://plmsmile.github.io/tags/Jesen不等式/"}]},{"title":"概率图模型(一)","date":"2017-08-04T03:06:41.000Z","path":"2017/08/04/pgm-01/","text":"概述 产生式和判别式 判别方法 由数据直接去学习决策函数\\(Y=f(X)\\) 或者\\(P(Y \\mid X)\\)作为预测模型 ，即判别模型 生成方法 先求出联合概率密度\\(P(X, Y)\\)，然后求出条件概率密度\\(P(Y \\mid X)\\)。即生成模型\\(P(Y \\mid X) = \\frac {P(X, Y)} {P(X)}\\) 判别式 生成式 原理 直接求\\(Y=f(X)\\) 或\\(P(Y \\mid X)\\) 先求\\(P(X,Y)\\)，然后 \\(P(Y \\mid X) = \\frac {P(X, Y)} {P(X)}\\) 差别 只关心差别，根据差别分类 关心数据怎么生成的，然后进行分类 应用 k近邻、感知机、决策树、LR、SVM 朴素贝叶斯、隐马尔可夫模型 概率图模型 概率图模型(probabilistic graphical models) 在概率模型的基础上，使用了基于图的方法来表示概率分布。节点表示变量，边表示变量之间的概率关系 概率图模型便于理解、降低参数、简化计算，在下文的贝叶斯网络中会进行说明。 贝叶斯网络 贝叶斯网络 又称为信度网络或者信念网络（belief networks），实际上就是一个有向无环图。 节点表示随机变量；边表示条件依存关系。没有边说明两个变量在某些情况下条件独立或者说是计算独立，有边说明任何条件下都不条件独立。 如上图所示，要表示上述情况的概率只需要求出\\(4*2*2*2*2-1=63\\)个参数的联合概率密度就行了，实际上这个太难以求得。我们可以考虑一下独立关系\\((F \\perp H \\mid S) \\,\\,\\, 表示在S确定的情况下，F和H独立\\)，所以有以下独立关系： \\[ (F \\perp H \\mid S)、\\, (C \\perp S \\mid F,H)、\\, (M \\perp H, C \\mid F)、 \\, (M \\perp C | F) \\] 所以我们得到如下的计算独立假设： \\[ P(C \\mid FHS) = P(C \\mid FH)，即假设C只与FH有关，而与S无关 \\] 又由\\(P(AB)=P(A|B)P(B)\\)，所以得到联合概率分布： \\[ \\begin{align*} P(SFHMC) &amp;= P(M \\mid SHFC) \\cdot P(SHFC) = \\underbrace {P(M \\mid F)}_{\\color {red}{计算独立性}} \\cdot \\underbrace {P(C \\mid SHF) \\cdot P(SHF)}_{\\color{red}{继续分解}} \\\\ &amp;= P(M \\mid F) \\cdot P(C \\mid FH) \\cdot P(F \\mid S) \\cdot P(H \\mid S) \\cdot P(S) \\end{align*} \\] \\(P(S)\\) 4个季节，需要3个参数；\\(P(H \\mid S)\\)时，\\(P(Y \\mid Spring)\\) 和 \\(P(N \\mid Spring)\\)只需要一个参数，所以\\(P(H \\mid S)\\)只需要4个参数即可，其他同理。 所以联合概率密度就转化成了上述公式中的5个乘积项，其中每一项需要的参数个数分别是2、4、4、4、3，所以一共只需要17个参数，这就大大降低了参数的个数。 马尔可夫模型 简介 马尔可夫模型(Markov Model) 描述了一类重要的随机过程，未来只依赖于现在，不依赖于过去。这样的特性的称为马尔可夫性，具有这样特性的过程称为马尔可夫过程。 时间和状态都是离散的马尔可夫过程称为马尔可夫链，简称马氏链，关键定义如下 系统有\\(N\\)个状态\\(S = \\{ s_1, s_2, \\cdots, s_N\\}\\)，随着时间的推移，系统将从某一状态转移到另一状态 设\\(q_t \\in S\\)是系统在\\(t\\)时刻的状态，\\(Q = \\{q_q, q_2, \\cdots, q_T \\}\\)系统时间的随机变量序列 一般地，系统在时间\\(t\\)时的状态\\(s_j\\)取决于\\([1, t-1]\\)的所有状态\\(\\{q_1, q_2, \\cdots, q_{t-1}\\}\\)，则当前时间的概率是 \\[ P(q_t = s_j \\mid q_{t-1} = s_i, q_{t-2} = s_k, \\cdots) \\] 在时刻\\(m\\)处于\\(s_i\\)状态，那么在时刻\\(m+n\\)转移到状态\\(s_j\\)的概率称为转移概率，即从时刻\\(m \\to m+n\\)： \\[ \\color{blue} {P_{ij}(m, m+n)} = P(q_{m+n} = s_j \\mid q_m = s_i) \\] 如果\\(P_{ij}(m, m+n)\\)只与状态\\(i, j\\)和步长\\(n\\)有关，而与起始时间\\(m\\)无关，则记为\\(\\color {blue} {P_{ij}(n)}\\),称为n步转移概率。 并且称此转移概率具有平稳性，且称此链是齐次的，称为齐次马氏链，我们重点研究齐次马氏链。\\(P(n) = [P_{ij}(n)]\\)称为n步转移矩阵。 \\[ P_{ij}(m, m+n) =\\color {blue} {P_{ij}(n)} = P(q_{m+n} = s_j \\mid q_m = s_i) \\] 特别地，\\(n = 1\\)时，有一步转移概率如下 \\[ p_{ij} = P_{ij}(1) = P(q_{m+1} \\mid q_{m}) = a_{ij} \\] 一阶马尔可夫 特别地，如果\\(t\\)时刻状态只与\\(t-1\\)时刻状态有关，那么下有离散的一阶马尔可夫链如下： \\[ P(q_t = s_j \\mid q_{t-1} = s_i, q_{t-2} = s_k, \\cdots) = P(q_t = s_j\\mid q_{t-1} = s_i) \\] 其中\\(t-1​\\)的状态\\(s_i​\\)转移到\\(t​\\)的状态\\(s_j​\\)的概率定义如下： \\[ P(q_t = s_j\\mid q_{t-1} = s_i) = \\color{blue} {a_{ij}}，其中i, j \\in [1, N]，a_{ij} \\ge 0，\\sum_{j=1}^Na_{ij} = 1 \\] 显然，\\(N​\\)个状态的一阶马尔可夫链有\\(N^2​\\)次状态转移，这些概率\\(a_{ij}​\\)构成了状态转移矩阵。 \\[ A = [a_{ij}] = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\\\ \\end{bmatrix} \\] 设系统在初始状态的概率向量是 \\(\\color{blue} {\\pi_i} \\ge 0\\) ，其中，\\(\\sum_{i=1}^{N}\\pi_i = 1\\) 那么时间序列\\(Q = \\{q_1, q_2, \\cdots, q_T \\}\\)出现的概率是 \\[ \\color{blue} {P(q_1, q_2, \\cdots, q_T) } = P(q_1) P(q2 \\mid q_1) P(q_3 \\mid q_2) \\cdots P(q_T \\mid q_{T-1}) = \\color{red} {\\underbrace {\\pi_{q_1}}_{初态概率} \\prod_{t=1}^{T-1} a_{q_tq_{t+1}}} \\] 下图是一个例子 多步转移概率 对于齐次马氏链，多步转移概率就是\\(u+v\\)时间段的状态转移，可以分解为先转移\\(u\\)步，再转移\\(v\\)步。则有CK方程的矩阵形式 \\[ P(u+v) = P(u)P(v) \\] 由此得到\\(n\\)步转移概率矩阵是一次转移概率矩阵的\\(n\\)次方 \\[ P(n) = P(1) P(n-1) = PP(n-1) \\implies P(n) = P^n \\] 对于求矩阵的幂\\(A^n\\)，则最好使用相似对角化来进行矩阵连乘。 存在一个可逆矩阵P，使得\\(P^{-1}AP = \\Lambda，A = P \\Lambda P^{-1}\\)，其中\\(\\Lambda\\)是矩阵\\(A\\)的特征值矩阵 \\[ \\Lambda = \\begin{bmatrix} \\lambda_1 &amp; &amp; &amp; \\\\ &amp;\\lambda_2 &amp; &amp; \\\\ &amp;&amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\lambda_n \\\\ \\end{bmatrix} ，其中\\lambda是矩阵A的特征值 \\] 则有\\(A^n = P\\Lambda ^ {n}P^{-1}\\) 遍历性 齐次马氏链，状态\\(i\\)向状态\\(j\\)转移，经过无穷步，任何状态\\(s_i\\)经过无穷步转移到状态\\(s_j\\)的概率收敛于一个定值\\(\\pi_j\\)，即\\(\\lim_{n \\to \\infty} P_{ij}(n) = \\pi_j \\; (与i无关)\\) 则称此链具有遍历性。若\\(\\sum_{j=1}^N \\pi_j = 1\\)，则称\\(\\vec{\\pi} = (\\pi_1, \\pi_2, \\cdots)\\)为链的极限分布。 遍历性的充分条件：如果存在正整数\\(m\\)(步数)，使得对于任意的，都有如下（转移概率大于0），则该马氏链具有遍历性 \\[ P_{ij}(m) &gt; 0, \\quad i, j =1, 2, \\cdots, N, \\quad s_i,s_j \\in S \\; \\] 那么它的极限分布\\(\\vec{\\pi} = (\\pi_1, \\pi_2, \\cdots, \\pi_N)​\\)，它是下面方程组的唯一解 \\[ \\pi = \\pi P, \\quad 即\\pi_j = \\sum_{i=1}^{N} \\pi_i p_{ij}, \\quad 其中\\pi_j &gt; 0, \\sum_{j=1}^N \\pi_j = 1 \\] PageRank应用 有很多应用，压缩算法、排队论等统计建模、语音识别、基因预测、搜索引擎鉴别网页质量-PR值。 Page Rank算法 这是Google最核心的算法，用于给每个网页价值评分，是Google“在垃圾中找黄金”的关键算法。 大致思想是要为搜索引擎返回最相关的页面。页面相关度是由和当前网页相关的一些页面决定的。 当前页面会把自己的importance平均传递给它所指向的页面，若有\\(k\\)个，则为每个传递\\(\\frac 1 k\\) 如果有很多页面都指向当前页面，则当前页面很重要，相关度高 当前页面有一些来自官方页面的backlink，当前页面很重要 例如有4个页面，分别如下 矩阵\\(\\color {blue }A\\)是页面跳转的一次转移矩阵，\\(\\color {blue }q\\)是当前时间每个页面的相关度向量，即PageRank vector。 \\[ A = \\begin{bmatrix} 0 &amp; 0 &amp;1 &amp; \\frac {1}{2} \\\\ \\frac {1}{3}&amp; 0 &amp; 0 &amp; 0 \\\\ \\frac {1}{3}&amp; \\frac {1}{2} &amp; 0 &amp; \\frac {1}{2} \\\\ \\frac {1}{3} &amp; \\frac {1}{2} &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\quad 初始时刻，q = \\begin {bmatrix} \\frac1 4 \\\\ \\frac1 4 \\\\ \\frac1 4 \\\\ \\frac1 4 \\\\ \\end {bmatrix} \\] \\(A\\)的一列是当前页面出去的所有页面，一行是进入当前页面的所有页面。设\\(u\\)表示第\\(A\\)的第\\(i\\)行，那么\\(u*q\\)就表示当页面\\(i\\)接受当前\\(q\\)的更新后的rank值。 定义矩阵\\(\\color {blue} {G} = \\color{red} {\\alpha A + (1-\\alpha) \\frac {1} {n} U}\\)，对\\(A\\)进行修正，\\(G\\)所有元素大于0，具有遍历性 \\(\\alpha \\in[0, 1] \\; (\\alpha = 0.85)\\) 阻尼因子 \\(A\\) 一步转移矩阵 \\(n\\) 页面数量 \\(U\\) 元素全为\\(1\\)的矩阵 使用\\(G\\)进行迭代的好处 解决了很多\\(A\\)元素为0导致的问题，如没有超链接的节点，不连接的图等 \\(A\\)所有元素大于0，具有遍历性，具有极限分布，即它的极限分布\\(q\\)会收敛 那么通过迭代就可以求出PR向量\\(\\color {red} {q^{next} = G q^{cur}}\\)，实际上\\(q\\)是\\(G\\)的特征值为1的特征向量。 迭代具体计算如下图(下图没有使用G，是使用A去算的，这是网上找的图[捂脸]) 随着迭代，\\(q\\)会收敛，那么称为\\(q\\)就是PageRank vector。 我们知道节点1有2个backlink，3有3个backlink。但是节点1却比3更加相关，这是为什么呢？因为节点3虽然有3个backlink，但是却只有1个outgoing，只指向了页面1。这样的话它就把它所有的importance都传递给了1，所以页面1也就比页面3的相关度高。 隐马尔可夫模型 定义 隐马尔可夫模型（Hidden Markov Model， HMM）是统计模型，它用来描述含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数，然后利用这些参数来做进一步的分析。大概形状如下 一个HMM由以下5个部分构成。 隐藏状态 模型的状态，隐蔽不可观察 有\\(N\\)种，隐状态种类集合\\(\\color {blue} {S = \\{s_1, s_2, \\cdots, s_N\\}}\\)会相 隐藏状态互相互转换，一步转移。\\(s_i\\)转移到\\(s_j\\)的概率 \\(\\color {red} {a_{ij} = P(q_t= s_j \\mid q_{t-1}=s_i)}\\) \\(q_t = s_i\\) 代表在\\(t\\)时刻，系统隐藏状态\\(q_t\\)是\\(s_i\\) 隐状态时间序列 \\(\\color{blue}{Q = \\{q_1, q_2, \\cdots, q_t, q_{t+1}\\cdots \\}}\\) 观察状态 模型可以显示观察到的状态 有\\(M\\)种，显状态种类集合\\(\\color{blue} {K = \\{v_1, v_2, \\cdots, v_M\\}}\\)。不能相互转换，只能由隐状态产生(发射) \\(o_t = v_k​\\) 代表在\\(t​\\)时刻，系统的观察状态\\(o_t​\\)是\\(v_k​\\) 每一个隐藏状态会发射一个观察状态。\\(s_j\\)发射符号\\(v_k\\)的概率\\(\\color {red} {b_j (k) = P(o_t = v_k \\mid s_j)}\\) 显状态时间序列 \\(\\color{blue} {O = \\{o_1, o_2, \\cdots, o_ t\\}}\\) 状态转移矩阵A (隐--隐) 从一个隐状\\(s_i\\)转移到另一个隐状\\(s_j\\)的概率。\\(A = \\{a_{ij}\\}\\) \\(\\color {red} {a_{ij} = P(q_t= s_j \\mid q_{t-1}=s_i)}\\)，其中 \\(1 \\leq i, j \\leq N, \\; a_{ij} \\geq 0, \\; \\sum_{j=1}^N a_{ij}=1\\) 发射概率矩阵B (隐--显) 一个隐状\\(s_j\\)发射出一个显状\\(v_k\\)的概率。\\(B = \\{b_j(k)\\}\\) \\(\\color {red} {b_j(k) = P(o_t = v_k \\mid s_j)}\\)，其中\\(1 \\leq j \\leq N; \\; 1 \\leq k \\leq M; \\; b_{jk} \\ge 0; \\; \\sum_{k=1}^Mb_{jk}=1\\) 初始状态概率分布 \\(\\pi\\) 最初的隐状态\\(q_1=s_i\\)的概率是\\(\\pi_i = P(q_1 = s_i)\\) 其中\\(1 \\leq i \\leq N, \\; \\pi_i \\ge 0, \\; \\sum_{i=1}^N \\pi_i = 1\\) 一般地，一个HMM记作一个五元组\\(\\mu = (S, K, A, B, \\pi)\\)，有时也简单记作\\(\\mu = (A, B, \\pi)\\)。一般，当考虑潜在事件随机生成表面事件的时候，HMM是非常有用的。 HMM中的三个问题 观察序列概率 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求当前观察序列\\(O\\)的出现概率\\(P(O \\mid \\mu)\\) 状态序列概率 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求一个最优的状态序列\\(Q=\\{q_1, q_2, \\cdots, q_T\\}\\)的出现概率，使得最好解释当前观察序列\\(O\\) 训练问题或参数估计问题 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)，调节模型\\(\\mu = (A, B, \\pi)\\)参数，使得\\(P(O \\mid u)\\)最大 前后向算法 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求给定模型\\(\\mu\\)的情况下观察序列\\(O\\)的出现概率。这是解码问题。如果直接去求，计算量会出现指数爆炸，那么会很不好求。我们这里使用前向算法和后向算法进行求解。 前向算法 前向变量\\(\\color {blue} {\\alpha_t(i)}\\)是系统在\\(t\\)时刻，观察序列为\\(O=o_1o_2\\cdots o_t\\)并且隐状态为\\(q_t = s_i\\)的概率，即 \\[ \\color {red} {\\alpha_t(i) = P(o_1o_2\\cdots o_t, q_t = s_i \\mid \\mu)} \\] \\(\\color {blue} {P(O \\mid \\mu)}\\) 是在\\(t\\)时刻，状态\\(q_t=\\) 所有隐状态的情况下，输出序列\\(O\\)的概率之和 \\[ \\color {blue} {P(O \\mid \\mu)} = \\sum_{i=1}^N P(O, q_t = s_i \\mid \\mu) = \\color {red} {\\sum_{i=1}^{N}\\alpha_t(i)} \\] 接下来就是计算\\(\\color {blue} {\\alpha_t(i)}\\)，其实是有动态规划的思想，有如下递推公式 \\[ \\color {blue} {\\alpha_{t+1}(j)} = \\color{red}{\\underbrace{\\left( \\sum_{i=1}^N \\alpha_t(i)a_{ij} \\right)}_{所有状态i转为j的概率} \\underbrace {b_j(o_{ t+1})}_{状态j发射o_{t+1}}} \\] 上述计算，其实是分为了下面3步 从1到达时间\\(t\\)，状态为\\(s_i\\)，输出\\(o_1o_2 \\cdots o_t\\)。\\(\\color{blue}{\\alpha_t(i)}\\) 从\\(t\\)到达\\(t+1\\)，状态变化\\(s_i \\to s_j \\text{。} \\quad\\color{blue}{a_{ij}}\\) 在\\(t+1\\)时刻，输出\\(o_{t+1}\\)。\\(\\color{blue}{b_j(o_{ t+1})}\\) 算法的步骤如下 初始化 \\(\\color {blue} {\\alpha_1(i)} = \\color {red} {\\pi_ib_i(o_1)}, \\; 1 \\leq i \\leq N\\) 归纳计算 \\(\\color {blue} {\\alpha_{t+1}(j)} = \\color{red} {\\left( \\sum_{i=1}^N \\alpha_t(i)a_{ij} \\right) b_j(o_{ t+1})}, \\; 1 \\leq t \\leq T-1\\) 求和终结 \\(\\color {blue} {P(O \\mid \\mu)} = \\color {red} {\\sum_{i=1}^{N}\\alpha_T(i)}\\) 在每个时刻\\(t\\)，需要考虑\\(N\\)个状态转移到\\(s_{j}\\)的可能性，同时也需要计算\\(\\alpha_t(1), \\cdots , \\alpha_t(N)\\)，所以时间复杂度为\\(O(N^2)\\)。同时在系统中有\\(T\\)个时间，所以总的复杂度为\\(O(N^2T)\\)。 后向算法 后向变量 \\(\\color {blue} {\\beta_{t}(i)}\\) 是系统在\\(t\\)时刻，状态为\\(s_i\\)的条件下，输出为\\(o_{t+1}o_{t+2}\\cdots o_T\\)的概率，即 \\[ \\color {red} {\\beta_t(i) = P(o_{t+1}o_{t+2}\\cdots o_T \\mid q_t = s_i , \\mu)} \\] 递推 \\(\\color {blue} {\\beta_{t}(i)}\\)的思路及公式如下 从\\(t \\to t+1\\)，状态变化\\(s_i \\to s_j\\)，并从\\(s_j \\implies o_{t+1}\\)，发射\\(o_{t+1}\\) 在\\(q_{t+1}=s_j\\)的条件下，输出序列\\(o_{t+2}\\cdots o_T\\) \\[ \\color {blue} {\\beta_{t}(i)} = \\sum_{j=1}^N\\color{red}{\\underbrace {a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}_{s_i转s_j \\; s_j发o_{t+1} \\; t+1时s_j后面\\{o_{t+2}, \\cdots\\}} } \\] 上面的公式个人的思路解释如下(不明白公式再看) 其实要从\\(\\beta_{t+1}(j) \\to \\beta_{t}(i)\\) \\(\\beta_{t+1}(j)\\)是\\(t+1\\)时刻状态为\\(s_j\\)，后面的观察序列为\\(o_{t+2}, \\cdots, o_{T}\\) \\(\\beta_{t}(i)\\)是\\(t\\)时刻状态为\\(s_i\\)，后面的观察序列为\\(\\color{red}{o_{t+1}}, o_{t+2}, \\cdots, o_{T}\\) \\(t \\to t+1\\) \\(s_i\\)会变成各种\\(s_j\\)，\\(\\beta_t(i)\\)只关心t+1时刻的显示状态为\\(o_{t+1}\\)，而不关心隐状态，所以是所有隐状态发射\\(o_{t+1}\\)的概率和 \\(\\color{red} {a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}\\)，\\(s_i\\)转为\\(s_j\\)的概率，在t+1时刻\\(s_j\\)发射\\(o_{t+1}\\)的概率，t+1时刻状态为\\(s_j\\) 观察序列为\\(o_{t+2}, \\cdots, o_{T}\\)的概率 把上述概率加起来，就得到了t时刻为\\(s_i\\),后面的观察为\\(o_{t+1}, o_{t+2}, \\cdots, o_{T}\\)的概率\\(\\beta_{t}(i)\\) 上式是把所有从\\(t+1 \\to t\\)的概率加起来，得到\\(t\\)的概率。算法步骤如下 初始化 \\(\\color {blue} {\\beta_T(i) = 1}, \\; 1 \\leq i \\leq N\\) 归纳计算 \\(\\color {blue} {\\beta_{t}(i)} = \\sum_{j=1}^N\\color{red}{a_{ij}b_j(o_{t+1})\\beta_{t+1}(j) }, \\quad 1 \\leq t \\leq T-1; \\; 1 \\leq i \\leq N\\) 求和终结 \\(\\color {blue} {P(O \\mid \\mu)} = \\sum_{i=1}^{N} \\color{red} {\\pi_i b_i(o_1)\\beta_1(i)}\\) 前后向算法结合 模型\\(\\mu\\)，观察序列\\(O=\\{o_1, o_2, \\cdots, o_t, o_{t+1}\\cdots, o_T\\}\\)，\\(t\\)时刻状态为\\(q_t=s_i\\)的概率如下 \\[ \\color {blue} {P(O, q_t = s_i \\mid \\mu)} = \\color{red} {\\alpha_t(i) \\times \\beta_t(i)} \\] 推导过程如下 \\[ \\begin{align*} P(O, q_t = s_i \\mid \\mu) &amp;= P(o_1\\cdots o_T, q_t=s_i \\mid \\mu) =P(o_1 \\cdots o_t, q_t=s_i, o_{t+1} \\cdots o_T \\mid \\mu) \\\\ &amp;= P(o_1 \\cdots o_t, q_t=s_i \\mid \\mu) \\times P(o_{t+1} \\cdots o_T \\mid o_1 \\cdots o_t, q_t=s_i, \\mu) \\\\ &amp;= \\alpha_t(i) \\times P((o_{t+1} \\cdots o_T \\mid q_t=s_i, \\mu) \\quad (显然o_1 \\cdots o_t是显然成立的，概率为1，条件忽略)\\\\ &amp;= \\alpha_t(i) \\times \\beta_t(i) \\end{align*} \\] 所以，把\\(q_t\\)等于所有\\(s_i\\)的概率加起来就可以得到观察概率\\(\\color{blue} {P(O \\mid \\mu)}\\) \\[ \\color{blue} {P(O \\mid \\mu)} = \\sum_{i=1}^N\\ \\color{red} {\\alpha_t(i) \\times \\beta_t(i)}, \\quad 1 \\leq t \\leq T \\] 维特比算法 维特比(Viterbi)算法用于求解HMM的第二个问题状态序列问题。即给定观察序列\\(O=o_1o_2\\cdots o_T\\)和模型\\(\\mu = (A, B, \\pi)\\)，求一个最优的状态序列\\(Q=q_1q_2 \\cdots q_T\\)。 有两种理解最优的思路。 使该状态序列中每一个状态都单独地具有最大概率，即\\(\\gamma_t(i) = P(q_t = s_i \\mid O,\\mu)\\)最大。但可能出现\\(a_{q_tq_{t+1}}=0\\)的情况 另一种是，使整个状态序列概率最大，即\\(P(Q \\mid O, \\mu)\\)最大。\\(\\hat{Q} = arg \\max \\limits_Q P(Q \\mid O, \\mu)\\) 维特比变量 \\(\\color{blue}{\\delta_t(i)}\\)是，在\\(t\\)时刻，\\(q_t = s_i\\) ，HMM沿着某一条路径到达状态\\(s_i\\)，并输出观察序列\\(o_1o_2 \\cdots o_t\\)的概率。 \\[ \\color{blue}{\\delta_t(i)} = \\arg \\max \\limits_{q_1\\cdots q_{t-1}} P(q_1 \\cdots q_{t-1}, q_t = s_i, o_1 \\cdots o_t \\mid \\mu) \\] 递推关系 \\[ \\color{blue}{\\delta_{t+1}(i)} = \\max \\limits_j [\\delta_t(j) \\cdot a_{ji}] \\cdot b_i(o_{t+1}) \\] 路径记忆变量 \\(\\color{blue}{\\psi_t(i) = k}\\) 表示\\(q_t = s_i, q_{t-1} = s_k\\)，即表示在该路径上状态\\(q_t=s_i\\)的前一个状态\\(q_{t-1} = s_k\\)。 维特比算法步骤 初始化 \\(\\delta_1(i) = \\pi_ib_i(o_1), \\; 1 \\le i \\le N\\)，路径变量\\(\\psi_1(i) = 0\\) 归纳计算 维特比变量 \\(\\delta_t(j) = \\max \\limits_{1 \\le i \\le N} [\\delta_{t-1}(i) \\cdot a_{ij}] \\cdot b_j(o_t), \\quad 2 \\le t \\le T; 1 \\le j \\le N\\) 记忆路径(记住参数\\(i\\)就行) \\(\\psi_t(j) = \\arg \\max \\limits_{1 \\le i \\le N} [\\delta_{t-1}(i) \\cdot a_{ij}] \\cdot b_j(o_t), \\quad 2 \\le t \\le T; 1 \\le j \\le N\\) 终结 \\[ \\hat{Q_T} = \\arg \\max \\limits_{1 \\le i \\le N} [\\delta_T(i)], \\quad \\hat P(\\hat{Q_T}) = \\max \\limits_{1 \\le i \\le N} [\\delta_T(i)] \\] 路径（状态序列）回溯 \\(\\hat{q_t} = \\psi_{t+1}(\\hat{q}_{t+1}), \\quad t = T-1, T-2, \\cdots, 1\\) Baum-Welch算法 Baum-Welch算法用于解决HMM的第3个问题，参数估计问题，给定一个观察序列\\(O= o_1 o_2 \\cdots o_T\\)，去调节模型\\(\\mu = (A, B, \\pi)\\)的参数使得\\(P(O\\mid \\mu)\\)最大化，即\\(\\mathop{argmax} \\limits_{\\mu} P(O_{training} \\mid \\mu)\\)。模型参数主要是\\(a_{ij}, b_j(k) \\text{和}\\pi_i\\)，详细信息见上文。 有完整语料库 如果我们知道观察序列\\(\\color{blue}{O= o_1 o_2 \\cdots o_T}\\)和状态序列\\(\\color{blue}{Q = q_1 q_2 \\cdots q_T}\\)，那么我们可以根据最大似然估计去计算HMM的参数。 设\\(\\delta(x, y)\\)是克罗耐克函数，当\\(x==y\\)时为1，否则为0。计算步骤如下 \\[ \\begin{align*} &amp; 初始概率\\quad \\color{blue}{\\bar\\pi_i} = \\delta(q_1, s_1) \\\\ &amp; 转移概率\\quad \\color{blue}{\\bar {a}_{ij}} = \\frac{s_i \\to s_j的次数}{s_i \\to all的次数} = \\frac {\\sum_{t=1}^{T-1} \\delta(q_t, s_i) \\times \\delta(q_{t+1}, s_j)} { \\sum_{t=1}^{T-1}\\delta(q_t, s_i)} \\\\ &amp; 发射概率 \\quad \\color{blue}{\\bar{b}_j(k)} = \\frac{s_j \\to v_k 的次数}{Q到达q_j的次数} = \\frac {\\sum_{t=1}^T\\delta(q_t, s_i) \\times \\delta(o_t, v_k)}{ \\sum_{t=1}^{T}\\delta(q_t, s_j)} \\end{align*} \\] 但是一般情况下是不知道隐藏状态序列\\(Q​\\)的，还好我们可以使用期望最大算法去进行含有隐变量的参数估计。主要思路如下。 我们可以给定初始值模型\\(\\mu_0\\)，然后通过EM算法去估计隐变量\\(Q\\)的期望来代替实际出现的次数，再通过上式去进行计算新的参数得到新的模型\\(\\mu_1\\)，再如此迭代直到参数收敛。 这种迭代爬山算法可以局部地使\\(P(O \\mid \\mu)\\)最大化，BW算法就是具体实现这种EM算法。 Baum-Welch算法 给定HMM的参数\\(\\mu\\)和观察序列\\(O= o_1 o_2 \\cdots o_T\\)。 定义t时刻状态为\\(s_i\\)和t+1时刻状态为\\(s_j\\)的概率是\\(\\color{blue}{\\xi_t(i, j)} = P(q_t =s_i, q_{t+1}=s_j \\mid O, \\mu)\\) \\[ \\begin{align} \\color{blue}{\\xi_t(i, j)} &amp;= \\frac{P(q_t =s_i, q_{t+1}=s_j, O \\mid \\mu)}{P(O \\mid \\mu)} = \\color{red}{\\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{P(O \\mid \\mu)}} = \\frac{\\overbrace{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}^{o_1\\cdots o_t, \\; o_{t+1}, \\; o_{t+2}\\cdots o_T}} {\\underbrace{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}_{\\xi_t(i, j)对ij求和，只留下P(O\\mid \\mu)}} \\\\ \\end{align} \\] 定义\\(t\\)时刻状态为\\(s_i\\)的概率是\\(\\color{blue}{\\gamma_t(i)} = P(q_t = s_i \\mid O, \\mu)\\) \\[ \\color{blue}{\\gamma_t(i)} = \\color{red}{\\sum_{j=1}^N \\xi_t(i, j)} \\] 那么有算法步骤如下（也称作前向后向算法） 1初始化 随机地给参数\\(\\color{blue}{a_{ij}, b_j(k), \\pi_i}\\)赋值，当然要满足一些基本条件，各个概率和为1。得到模型\\(\\mu_0\\)，令\\(i=0\\)，执行下面步骤 2EM步骤 2.1E步骤 使用模型\\(\\mu_i\\)计算\\(\\color{blue}{\\xi_t(i, j)}和\\color{blue}{\\gamma_t(i)}\\) \\[ \\color{blue}{\\xi_t(i, j)} = \\color{red}{\\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}} , \\; \\color{blue}{\\gamma_t(i)} = \\color{red}{\\sum_{j=1}^N \\xi_t(i, j)} \\] 2.2M步骤 用上面算得的期望去估计参数 \\[ \\begin{align*} &amp; 初始概率\\quad \\color{blue}{\\bar\\pi_i} = P(q_1=s_i \\mid O, \\mu) = \\gamma_1(i) \\\\ &amp; 转移概率\\quad \\color{blue}{\\bar {a}_{ij}} = \\frac{\\sum_{t=1}^{T-1}\\xi_t(i, j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)} \\\\ &amp; 发射概率 \\quad \\color{blue}{\\bar{b}_j(k)} = \\frac{\\sum_{t=1}^T \\gamma_t(j) \\times \\delta(o_t, v_k)}{\\sum_{t=1}^T \\gamma_t(j)} \\end{align*} \\] 3循环计算 令\\(i=i+1\\)，直到参数收敛","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"概率图模型","slug":"概率图模型","permalink":"http://plmsmile.github.io/tags/概率图模型/"},{"name":"马尔可夫链","slug":"马尔可夫链","permalink":"http://plmsmile.github.io/tags/马尔可夫链/"},{"name":"隐马尔科夫模型","slug":"隐马尔科夫模型","permalink":"http://plmsmile.github.io/tags/隐马尔科夫模型/"},{"name":"维特比算法","slug":"维特比算法","permalink":"http://plmsmile.github.io/tags/维特比算法/"},{"name":"前向算法","slug":"前向算法","permalink":"http://plmsmile.github.io/tags/前向算法/"},{"name":"后向算法，BW算法","slug":"后向算法，BW算法","permalink":"http://plmsmile.github.io/tags/后向算法，BW算法/"}]},{"title":"语言模型和平滑方法","date":"2017-07-31T00:57:52.000Z","path":"2017/07/31/nlp-notes/","text":"语言模型 二元语法$ $ 对于一个句子\\(s=w_1 \\cdots w_n\\)，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 \\[ \\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \\cdots p(w_n|w_{l-1})= \\color {red} {\\prod_{i=1}^l {p(w_i|w_{i-1})}} \\] 设\\(\\color {blue} {c(w_{i-1}w_i)}\\) 表示二元语法\\(\\color {green} {w_{i-1}w_i}\\)在给定文本中的出现次数，则上一个词是\\(w_{i-1}\\)下一个词是\\(w_i\\)的概率\\(\\color {blue} {p(w_i \\mid w_{i-1})}\\)是当前语法\\(\\color {green} {w_{i-1}w_i}\\)出现的次数比上所有形似\\(\\color {green} {w_{i-1}}w\\)的二元语法的出现次数 \\[ \\color {blue} {p(w_i \\mid w_{i-1})} =\\color {red} {\\frac {c(w_{i-1}w_i)} {\\sum_{w} {c(w_{i-1}w)}}}，w是变量 \\] \\(n\\)元语法 认为一个词出现的概率和它前面的n个词有关系。则对于句子\\(s=w_1w_2 \\cdots w_l\\)，其概率计算公式为如下： \\[ \\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_1w_2) \\cdots p(w_n|w_1w_2 \\cdots w_{l-1})=\\color {red} {\\prod_{i=1}^n{p(w_i|w_1 \\cdots w_{i-1})}} \\] 上述公式需要大量的概率计算，太理想了。一般取\\(n=2\\)或者\\(n=3\\)。 对于\\(n&gt;2\\)的\\(n\\)元语法模型，条件概率要考虑前面\\(n-1\\)个词的概率，设\\(w_i^j\\)表示\\(w_i\\cdots w_j\\)，则有 \\[ \\color{blue} {p(s)} = \\prod_{i=1}^{l+1}p(w_i \\mid w_{i-n+1}^{i})，\\color {blue} {p(w_i \\mid w_{i-n+1}^{i})}=\\frac { \\overbrace {c(w_{i-n+1}^i)}^{\\color{red}{具体以w_i结尾的词串w[i-n+1, i]}}} { \\underbrace{\\sum_{w_i}{c(w_{i-n+1}^i)}}_{\\color{red}{所有以w_i结尾的词串w[i-n+1, i]}}} \\] 实际例子 假设语料库\\(S\\)是由下面3个句子组成，所求的句子t在其后： 12s = ['brown read holy bible', 'plm see a text book', 'he read a book by david']t = 'brown read a book' 那么求句子\\(t\\)出现的概率是 \\[\\color{blue}{p(t)}=p(\\color{green}{brown\\,read\\, a\\, book})=p(brown|BOS)p(read|brown)p(a|read)p(book|a)p(eos|book)\\approx0.06\\] \\(n\\)元文法的一些应用如下 语音识别歧义消除 如给了一个拼音 \\(\\color{green}{ta\\,shi \\,yan \\,jiu \\,sheng\\, wu\\, de}\\)，得到了很多可能的汉字串：踏实研究生物的，他实验救生物的，他是研究生物的 ，那么求出\\(arg_{str}maxP(str|pinyin)\\)，即返回最大概率的句子 汉语分词问题 给定汉字串他是研究生物的。可能的汉字串 他 是 研究生 物 的和他 是 研究 生物 的，这也是求最大句子的概率 开发自然语言处理的统计方法的一般步骤 收集大量语料（基础工作，工作量最大，很重要） 对语料进行统计分析，得出知识（如n元文法，一堆概率） 针对场景建立算法，如计算概率可能也用很多复杂的算法或者直接标注 解释或者应用结果 模型评估参数 基础 评价目标：语言模型计 算出的概率分布与“真实的”理想模型是否接近 难点：无法知道“真实的”理想模型的分布 常用指标：交叉熵，困惑度 信息量和信息熵 \\(X\\)是一个离散随机变量，取值空间为\\(R\\)，其概率分布是\\(p(x)=P(X=x), x \\in R\\)。 信息量 概率是对事件确定性的度量，那么信息就是对不确定性的度量。信息量 \\(\\color {blue} {I(X)}\\)代表特征的不确定性，定义如下 \\[ \\color {blue} {I(X)}= \\color {red} {-\\log {p(x)}} \\] 信息熵 信息熵\\(\\color{blue}{H(x)}\\)是特征不确定性的平均值，用表示，定义如下 \\[ \\color{blue}{H(X)}=\\sum_{x \\in R}{p(x)log\\frac 1 {p(x)}}=\\color {red} {-\\sum_{x \\in R} {p(x) \\log p(x)}} \\] 一般是\\(log_2{p(x)}\\)，单位是比特。若是\\(\\ln {p(x)}\\)，单位是奈特。 信息熵的本质是信息量的期望 信息熵是对不确定性的度量 随机变量\\(X\\)的熵越大，说明不确定性也大；若\\(X\\)为定值，则熵为0 平均分布是&quot;最不确定”的分布 联合熵和条件熵 \\(X, Y\\)是一对离散型随机变量，并且\\(\\color{blue}{X,Y \\sim p(x,y)}\\)。 联合熵 联合熵实际上描述的是一对随机变量平均所需要的信息量，定义如下。 \\[ \\color{blue}{H(X, Y)} = \\color{red} {- \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x, y)} \\] 条件熵 给定\\(X\\)的情况下，\\(Y\\)的条件熵为 \\[ \\color{blue}{H(Y \\mid X)} = \\color{red}{ - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y \\mid x)} \\] 其中可以推导出：\\(H(X, Y) = H(X) + H(Y \\mid X)\\)。 相对熵和交叉熵 相对熵 随机变量\\(X\\)的状态空间\\(\\Omega {x}\\)上有两个概率分布\\(p(x)\\)和\\(q(x)\\)。一般p是真实分布，q是预测分布。 相对熵也称为KL距离，用来衡量相同事件空间里两个概率分布的差异。 \\(p\\)和\\(q\\)的相对熵\\(\\color{blue}{D(p\\mid\\mid q)}\\)用来度量它们之间的差异，如下 \\[ \\color{blue}{D(p\\mid\\mid q)} =\\color{red}{\\sum_{x\\in X} {p(x)\\log{\\frac {p(x)}{q(x)}}}} = E_p(\\log \\frac{p(X)}{q(X)}) \\; (期望) \\] 特别地，若\\(p==q\\)，则相对熵为0；若差别增加，相对熵的值也增加。简单理解“相对”如下： \\[ D(p \\mid\\mid q)=\\sum_{x \\in X}{p(x)(\\log p(x) - \\log q(x))} = \\underbrace{\\left(-\\sum_{x \\in X}{ \\color{red}{p(x)\\log q(x)}}\\right)}_{\\color {red}{以q去近似p的熵=交叉熵}} - \\underbrace{\\left(-\\sum_{x \\in X} {\\color{red}{p(x)\\log p(x)}}\\right)}_{\\color{red} {p本身的熵}} \\] 交叉熵 交叉熵用来衡量估计模型与真实概率分布之间的差异。 随机变量\\(X \\sim p(x)\\)，\\(q(x)\\)近似于\\(p(x)\\)。 则随机变量\\(X\\)和模型\\(q\\)之间的交叉熵\\(\\color {blue} {H(X, q)}\\)如下：以\\(q\\)去近似\\(p\\)的熵 \\[ \\color {blue} {H(p, q)} = H(X) + D(p \\mid\\mid q) = \\color {red} {-\\sum_{x \\in X}{p(x)\\log q(x)}} \\] 实际应用 交叉熵的实际应用，设\\(y\\)是预测的概率分布，\\(y^\\prime\\)为真实的概率分布。则用交叉熵去判断估计的准确程度 \\[ H(y^{\\prime}, y)= - \\sum_i y_i^{\\prime}\\log y_i = \\color {red} {-\\sum_i y_{真实} \\log y_{预测}} \\] n元文法模型的交叉熵 设测试集\\(T=(t_1, t_2, \\ldots, t_l)\\)包含\\(l\\)个句子，则定义测试集的概率\\(\\color {blue} {p(T)}\\)为多个句子概率的乘积 \\[ \\color {blue} {p(T)} = \\prod_{i=1}^{l} p(t_i)， \\, \\text{其中}\\color{blue}{p(t_i)}=\\color{red}{\\prod_{i=1}^{l_w} {p(w_i|w_{i-n+1}^{i-1})}}, \\text{见上面} \\] 其中\\(w_i^j\\)表示词\\(w_i\\cdots w_j\\)，\\(\\sum_{w}{c(read \\, w)}\\)是查找出所有以\\(read\\)开头的二元组的出现次数。 则在数据\\(T\\)上n元模型\\(\\color {green} {p(w_i|w_{i-n+1}^{i-1})}\\)的交叉熵\\(\\color {blue} {H_p(T)}\\)定义如下 \\[ \\color {blue} {H_p(T)} = \\color {red} {-\\frac {1} {W_T} \\log _2 p(T)}，其中W_T是文本T中基元(词或字)的长度 \\] 公式的推导过程如下 \\[ -\\sum_{x \\in X}{p(x)\\log q(x)} \\implies \\underbrace { -{\\frac{1} {W_T}}\\sum \\log q(x)}_{\\color{red}{使用均匀分布代替p(x)}} \\implies -{\\frac{1} {W_T}} \\log {\\prod r(w_i|w_{i-n+1}^{i-1})} \\implies -{\\frac{1} {W_T}} \\log_2p(T) \\] 可以这么理解：利用模型\\(p\\)对\\(W_T\\)个词进行编码，每一个编码所需要的平均比特位数。 困惑度 困惑度是评估语言的基本准则人，也是对测试集T中每一个词汇的概率的几何平均值的倒数。 \\[ \\color{blue}{PP_T(T)} =\\color{red}{ 2^{H_p(T)}= \\frac {1} {\\sqrt [W_T]{p(T)}}} = 2 ^{\\text{交叉熵}} \\] 当然，交叉熵和困惑度越小越好。语言模型设计的任务就是要找出困惑度最小的模型。 在英语中，n元语法模型的困惑度是\\(50 \\sim 1000\\)，交叉熵是\\(6 \\sim 10\\)个比特位。 数据平滑 问题的提出 按照上面提出的语言模型，有的句子就没有概率，但是这是不合理的，因为总有出现的可能，概率应该大于0。设\\(\\color {blue}{c(w)}\\)是\\(w\\)在语料库中的出现次数。 \\[ p(\\color{green} {read \\mid plm}) = \\frac {c(plm \\mid read)} {\\sum_{w_i}{c(plm | w_i})} = \\frac {\\color{red}{0}} {1}=\\color{red}{0， \\, 这是不对的} \\] 因此，必须分配给所有可能出现的字符串一个非0的概率值来避免这种错误的发送。 平滑技术就是用来解决这种零概率问题的。平滑指的是为了产生更准确的概率来调整最大似然估计的一种技术，也称作数据平滑。思想是劫富济贫，即提高低概率、降低高概率，尽量是概率分布趋于均匀。 数据平滑是语言模型中的核心问题 加法平滑 其实为了避免0概率，最简单的就是给统计次数加1。这里我们可以为每个单词的出现次数加上\\(\\delta，\\delta \\in [0, 1]\\)，设\\(V\\)是所有词汇的单词表，\\(|V|\\)是单词表的词汇个数，则有概率： \\[ p_{add}(w_i \\mid w_{i-n+1}^{i-1}) = \\frac {\\delta + c(w_{i-n+1}^i)} {\\sum_{w_i}{(\\delta*|V| + c(w_{i-n+1}^i)})}=\\frac {\\delta + \\overbrace {c(w_{i-n+1}^i)}^{\\color{red}{具体词串[i-n+1, i]}}} {\\delta*|V| + \\underbrace{\\sum_{w_i}{c(w_{i-n+1}^i)}}_{\\color{red}{所有以w_i结尾的词串[i-n+1, i]}}} \\] 注：这个方法很原始。 Good-Turing Good-Turing也称作古德-图灵方法，这是很多平滑技术的核心。 主要思想是重新分配概率，会得到一个剩余概率量\\(\\color {blue} {p_0}= \\color {red} {\\frac {n_1} N}\\)，设\\(n_0\\)为未出现的单词的个数，然后由这\\(n_0\\)个单词去平均分配得到\\(p_0\\)，即每个未出现的单词的概率为\\(\\frac {p_0} {n_0}\\)。 对于一个\\(n\\)元语法，设\\(\\color {blue} n_r\\)恰好出现\\(r\\)次的\\(n\\)元语法的数目，下面是一些新的定义 出现次数为\\(r\\)的\\(n\\)元语法 新的出现次数\\(\\color {blue} {r^*} = \\color {red} {(r+1)\\frac{n_{r+1}}{n_r}}\\) 设\\(N = \\sum_{r=0}^{\\infty}n_r r^* = \\sum_{r=1}^{\\infty} n_r r\\)，即\\(N\\)是这个分布中最初的所有文法出现的次数，例如所有以\\(read\\)开始的总次数 出现次数为\\(r\\)的修正概率 \\(\\color {blue}p_r = \\color {red} {\\frac {r^*} {N}}\\) 剩余概率量\\(\\color {blue} {p_0}= \\color {red} {\\frac {n_1} N}\\)的推导 \\[ 总的概率 = \\sum_{r&gt;0}{n_r p_r} = \\sum_{r&gt;0}{n_r (r+1)\\frac{n_{r+1}}{n_r N}} = \\frac {1}{N} (\\sum_{r&gt;0} (r+1)n_{r+1} = \\frac {1}{N} (\\sum_{r&gt;0} (r n_r - n_1) = 1 - \\frac {n_1} N &lt; 1 \\] 然后把\\(p_0\\)平均分配给所有未见事件(r=0的事件)。 缺点 若出现次数最大为\\(k\\)，则无法计算\\(r=k\\)的新的次数\\(r^*\\)和修正概率\\(p_r\\) 高低阶模型的结合通常能获得较好的平滑效果，但是Good-Turing不能高低阶模型结合 Jelinek-Mercer 问题引入 假如\\(c(send \\, the)=c(send \\, thou)=0\\)，则通过GT方法有\\(p(the \\mid send)=p(thou \\mid send)\\)，但是实际上却应该是\\(p(the \\mid send)&gt;p(thou \\mid send)\\)。 所以我们需要在二元语法模型中加入一个一元模型 \\[ p_{ML}(w_i) = \\frac {c(w_i)}{\\sum_w{c(w)}} \\] 二元线性插值 使用\\(r\\)将二元文法模型和一元文法模型进行线性插值 \\[ p(w_i \\mid w_{i-1}) = \\lambda p_{ML}(w_i | w_{i-1}) + (1-\\lambda)p_{ML}(w_i)，\\lambda \\in [0, 1] \\] 所以可以得到\\(p(the \\mid send)&gt;p(thou \\mid send)\\)","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"各种熵","slug":"各种熵","permalink":"http://plmsmile.github.io/tags/各种熵/"},{"name":"语言模型","slug":"语言模型","permalink":"http://plmsmile.github.io/tags/语言模型/"},{"name":"数据平滑","slug":"数据平滑","permalink":"http://plmsmile.github.io/tags/数据平滑/"}]},{"title":"剑指Offer算法题","date":"2017-07-29T03:42:07.000Z","path":"2017/07/29/aim2offer/","text":"数组中重复的数字-03 题目1 找到数组中重复的数字 一个数组存放n个数字，所有数字在[0, n-1]范围内。某些数字是随机重复的。请找出任意一个重复的数字。例如[2,3,1,0,2,5,3]，输出2或3 思路1 对数组进行排序，然后可以找出重复的数字。但是排序的时间复杂度是O(nlogn) 思路2 使用哈希表，每次存放的时候检查是否在哈希表中，如果已经存在，那么就重复了。时间复杂度O(n)，空间复杂度O(n) 最优思路 利用下标和值的关系，从头到尾依次扫描这个数组。扫描到下标为i，值为m。尽量把m放到a[m]的位置上。修改了原来的数组。 12345678910while (a[i] != i) if m == i: # 扫描下一个数字 else: # 把m和a[m]进行比较 if m == a[m]: # 找到一个相同的数字 else: # 把m放到a[m]的位置，交换 m &lt;--&gt; a[m] 尽管有两重循环，但每个数字最多交换两次就能找到自己的位置，所以总的时间复杂度是O(n)，空间复杂度为O(1) 关键代码 123456789101112131415161718192021222324252627// 找到数组中重复的值// Args:// a: 数组// alen: 数组长度// dup: 用于返回重复的数值// Returns:// True：数据合法(长度和值)并且有重复的数字，否则返回False// bool duplicate(int a[], int alen, int *dup) &#123; // 遍历数组，把i都放到a[i]上 for (int i = 0; i &lt; alen; i++) &#123; while (a[i] != i) &#123; int m = a[i]; if (a[m] == m) &#123; // a[m]已经有m *dup = m; return true; &#125; else &#123; // 把m放到a[m]上 int t = a[m]; a[m] = m; a[i] = t; &#125; &#125; &#125; return false;&#125; 题目2 不修改数组找出重复的数字 数组，长度为n+1，数字范围[1, n]，数组中至少有一个是重复的，找出任意一个重复的数字，但是不能修改数组 思路1 创建一个新数组b存放原数组a。遍历原数组，当前是m，如果b[m]已经没有值，则存放；如果有值，则重复。但是需要O(n)的辅助空间 最优思路 把\\(a[1, n]\\)的个数字，分为两部分。\\(a[1, m]\\)和\\(a[m+1, n]\\)。 在\\(a[1, m]\\)中，统计数字\\(1,2\\cdots, m\\)在\\(a[1,m]\\)中出现的次数count 如果是m次，则\\(a[1,m]\\)每个数字独一无二，重复的区间在a[m+1, n]中。则\\(\\rm{start}=m+1\\)，继续查找。 否则不独一无二，则重复在\\(a[1, m]\\)中 。则\\(\\rm{end}=m\\)， 继续查找。 直到\\(\\rm{start} == \\rm{end}\\) 。count &gt; 1，则start重复，否则没有重复的。 关键代码 1234567891011121314151617181920212223242526272829303132// 二分查找数组中重复的值// Args:// a: 数组// alen: 数组长度// Returns:// dup: 重复的数值; 没有重复时返回-1int get_duplication(const int *a, int alen) &#123; if (a == nullptr || alen &lt;= 0) &#123; return -1; &#125; int start = 1; int end = alen - 1; while (start &lt;= end) &#123; int m = ((end - start) &gt;&gt; 1) + start; int count = count_range(a, alen, start, m); // last if (start == end) &#123; if (count &gt; 1) &#123; return start; &#125; else &#123; break; &#125; &#125; // continue if (count == m - start + 1) &#123; start = m + 1; &#125; else &#123; end = m; &#125; &#125; return 0;&#125; 二维数组查找-04 ​ 一个二维数组，每一行从左到右递增，每一列，从上到下递增。输入一个整数，判断二维数组中是否有这个数字 错误思路 全盘扫描肯定不行，从左上角最小的开始也不行，应该从最大角的地方开始 思路 一行的最大元素在最右边，一列的最小元素在上边。所以从右上角开始查找最好。即向左查、向下查，这样每次都能够剔除一行或者一列。 1234567891011while# 当期右上角值是a[i, j]=m，查找的值是tif t == a[i,j]: done # 查找成功else if t &gt; a[i,j]: # 删除当前行 m = a[i+1, j]else if t &lt; a[i, j]: # 删除当前列 m = a[i, j-1]# 继续查找 关键代码 12345678910111213141516171819202122232425262728// 查找一个数，是否在一个矩阵中// Args:// target: 要查找的数字// array: 矩阵// Returns:// exists: true or falsebool find(int target, std::vector&lt;std::vector&lt;int&gt;&gt; array) &#123; int col = array.size(); int row = array[0].size(); bool exist = false; int i = 0; int j = col - 1; // 注意i,j的范围 while (exist == false &amp;&amp; (i &lt; row &amp;&amp; i &gt;= 0 &amp;&amp; j &lt; col &amp;&amp; j &gt;= 0)) &#123; int t = array[i][j]; if (target == t) &#123; exist = true; break; &#125; else if (target &lt; t) &#123; // to left --j; &#125; else if(target &gt; t) &#123; // to down ++i; &#125; &#125; return exist;&#125; 字符串替换空格-05 把字符串中的每个空格替换成&quot;%20&quot; 如果在原来的字符串上修改，则会覆盖原来字符串后面的内存 如果创建新的字符串，则要分配足够的内存 C/C++中字符串最后一个字符是\\0 不好思路 从前向后扫描，遇到一个空格替换一个。但是每次都需要大量移动后面的元素，所以时间复杂度是\\(O(n^2)\\) 最优思路 从后向前替换。使用两个指针p1和p2。先计算出替换后的长度，p2指向替换后的长度的末尾指针。p1指向之前的字符串的指针。 从p1开始向前移动 当前是普通字符，则复制到p2，p2向前移动 当前是空格，则在p2加入“%20”，p2向前移动 如果p1==p2，那么移动完毕 总的来说，先找到最终的长度，从后向后拉。时间复杂度是\\(O(n)\\) 技巧 合并两个数组/字符串，如果从前往后，则需要移动多次。从后向前，能够减少移动次数，提高效率 关键代码 1234567891011121314151617181920212223242526272829303132// 替换字符串中的空格字符，每个空格用'02%'替换// 直接修改原字符串// Args:// str: 字符串// len: 长度// Returns:// Nonevoid replace_space(char *str, int len) &#123; // 统计空格的个数 int count = 0; for (int i = 0; i &lt; len; i++) if (str[i] == ' ') ++count; int newlen = (len - count) + count * 3; int i = len - 1, j = newlen - 1; while (i &gt;= 0 &amp;&amp; j &gt;= 0) &#123; if (str[i] == ' ') &#123; // 在j处添加替换字符 str[j--] = '0'; str[j--] = '2'; str[j--] = '%'; // 向前移动 --i; &#125; else &#123; // 字符复制到后面 str[j--] = str[i--]; &#125; &#125; // 字符串结尾 str[newlen] = '0';&#125; 逆序打印链表-06 链表基础考点 链表是面试中最频繁的数据结构。动态结构很灵活，考指针、考编程功底。 链表创建 ： 链表插入： 为新节点分配内存，调整指针的指向。 删除链表中的节点 从尾到头打印链表 链表中倒数第k个节点 反转链表 合并两个排序的链表 两个链表的第一个公共节点 环形链表 ：尾节点指针指向头结点。题目62：圆圈中最后剩下的数字 双向链表 ：题目36，二叉搜索树与双向链表 复杂链表 ：指向下一个，指向任意节点的指针 从尾到头打印链表 本质上是先进后出， 可以用栈和递归。显然，栈的效率高。 关键代码 1234567891011121314151617181920212223// 使用栈逆序打印链表// Args:// head: 头指针// Returns:// res: vector&lt;int&gt;，逆序值vector&lt;int&gt; get_reverse_by_stack(ListNode *head) &#123; ListNode* pnode = head; stack&lt;int&gt; st; int count = 0; while (pnode != nullptr) &#123; st.push(pnode-&gt;val); pnode = pnode-&gt;next; count++; &#125; // 分配定长的vector，不用 vector&lt;int&gt; res(count); for (int i = 0; i &lt; count &amp;&amp; st.empty() == false; i++) &#123; res[i] = st.top(); st.pop(); &#125; return res;&#125; 重建二叉树-07 树的考点 树的遍历 叉树涉及指针，比较难。最常问遍历。需要对下面7种了如指掌。 前序 中序 后序 层次遍历 递归 无递归 循环 考题 题26，树的子结构 题34，二叉树中和为某一值的路径 题55，二叉树的深度 题7，重建二叉树 题33，二叉搜索树的后序遍历序列 题32，从上到下打印二叉树（层次遍历） 特别的二叉树 二叉搜索树 ：左节点小于根节点，根节点小于右节点。查找搜索时间复杂度\\(O(\\log n)\\)。 题36，二叉搜索树与双向链表；题68：树中两个节点的最低公共祖先。 堆 ：最大堆和最小堆。找最大值和最小值。 红黑树 ： 节点定义为红黑两种颜色。根节点到叶节点的最长路径不超过最短路径的两倍。 前序中序建立二叉树 前序序列：1, 2, 4, 7, 3, 5, 6, 8。 根 左 右。 中序序列：4, 7, 2, 1, 5, 3, 8, 6。 左 根 右。 使用递归，先找到根节点，找到左右子树，为左右子树分别创建各自的前序和中序序列，再进行递归创建左右子树。 关键是要构建下面的序列，注意下标值。 前序 中序 左子树 2, 4, 7 4, 7, 2 右子树 3, 5, 6, 8 5, 3, 8, 6 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 递归利用先序和中序重建二叉树// Args:// vpre: 先序序列// vin: 中序序列// Returns:// root: treeTreeNode * reconstruct_binary_tree(vector&lt;int&gt;vpre, vector&lt;int&gt; vin) &#123; // 1. 为空，停止递归 if (vpre.size() == 0 || vin.size() == 0) &#123; return NULL; &#125; // 2. 构建根节点 TreeNode *root = new TreeNode(vpre[0]); // 3. 找到根节点在中序中的位置 int root_index = -1; for (int i = 0; i &lt; vin.size(); i++) &#123; // cout &lt;&lt; vin[i] &lt;&lt; \" \" &lt;&lt; vpre[0] &lt;&lt; endl; if (vin[i] == vpre[0]) &#123; root_index = i; break; &#125; &#125; // 简单判断一下 if (root_index == -1) &#123; cout &lt;&lt; \"root_index is -1\" &lt;&lt; endl; return NULL; &#125; // 4. 生成左右子树的先序序列、中序序列 int leftlen = root_index; int rightlen = vin.size() - leftlen - 1; vector&lt;int&gt; leftvpre(leftlen), leftvin(leftlen); vector&lt;int&gt; rightvpre(rightlen), rightvin(rightlen); // 重点在这里，用实际例子去对照看 for (int i = 0; i &lt; vin.size(); i++) &#123; if (i &lt; root_index) &#123; // 左子树 leftvin[i] = vin[i]; leftvpre[i] = vpre[i+1]; &#125; else if (i &gt; root_index)&#123; // 右子树，条件特别重要 int right_idx = i - root_index - 1; rightvin[right_idx] = vin[i]; rightvpre[right_idx] = vpre[leftlen + 1 + right_idx]; &#125; &#125; // 5. 递归生成左右子树 root-&gt;left = reconstruct_binary_tree(leftvpre, leftvin); root-&gt;right = reconstruct_binary_tree(rightvpre, rightvin); return root;&#125; 二叉树的下一个节点-08 二叉树：值，左孩子，右孩子，父亲节点指针。 给一个节点，找出中序序列的该节点的下一个节点。重在分析中序序列。 12345678910if \"有右子树\": # 向左走 while (\"p有左孩子\") p = \"左孩子\" t = pelse: # 向上走 while (\"p有父节点 &amp;&amp; p是父节点的右节点\"): p = \"父节点\" t = p 关键代码 123456789101112131415161718192021222324252627// 找到中序遍历的下一个节点// Args:// pnode: 当前节点// Returns:// pnext: 中序中，pnode的下一个节点TreeNode* get_next_inorder(TreeNode* pnode) &#123; if (pnode == nullptr) &#123; return nullptr; &#125; TreeNode* pnext = nullptr; if (pnode-&gt;right != nullptr) &#123; TreeNode* p = pnode-&gt;right; while (p-&gt;left != nullptr) &#123; p = p-&gt;left; &#125; pnext = p; &#125; else &#123; TreeNode* p = pnode; while (p-&gt;parent != nullptr &amp;&amp; p == p-&gt;parent-&gt;right) &#123; p = p-&gt;parent; &#125; if (p-&gt;parent != nullptr) &#123; pnext = p-&gt;parent; &#125; &#125; return pnext;&#125; 两个栈实现队列-09 栈和队列 栈 ：后进先出 题31：栈的压入、弹出序列 \\(O(n)\\) 找到最大最小元素。若\\(O(1)\\) ，则题30：包含min函数的栈 队列 ：先进先出 树的层次遍历，题32： 从上到下打印二叉树 用两个栈实现队列 分为入栈（栈A）和出栈（栈B）。 入队时：直接进入入栈 出队时：若出栈为空，则把入栈里的内容放入出栈；再从出栈里面出一个元素。 [关键代码] 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; private: stack&lt;int&gt; stackIn; stack&lt;int&gt; stackOut; public: // 入队 void push(int node) &#123; stackIn.push(node); &#125; // 出队 int pop() &#123; if (this-&gt;empty()) &#123; cout &lt;&lt; \"empty queue\" &lt;&lt; endl; return -1; &#125; int node = -1; if (stackOut.empty() == true) &#123; while (stackIn.empty() == false) &#123; node = stackIn.top(); stackIn.pop(); stackOut.push(node); &#125; &#125; node = stackOut.top(); stackOut.pop(); return node; &#125; bool empty() &#123; return stackIn.empty() == true &amp;&amp; stackOut.empty() == true; &#125;&#125;; 两个队列实现栈 分为空队列和非空队列 入栈：进入非空队列 出栈：非空队列中中前n-1个进入空队列，出非空队列最后一个元素（最新进来的） 关键代码 123456789101112131415161718192021222324int pop() &#123; if (this-&gt;empty()) &#123; return -1; &#125; // 找到哪个队列有元素，注意使用指针 queue&lt;int&gt;* qout; queue&lt;int&gt;* qin; if (q1.empty() == true) &#123; qout = &amp;q2; qin = &amp;q1; &#125; else &#123; qout = &amp;q1; qin = &amp;q2; &#125; // qout的前n-1个元素放到qin中 while (qout-&gt;size() &gt; 1) &#123; qin-&gt;push(qout-&gt;front()); qout-&gt;pop(); &#125; int res = qout-&gt;back(); qout-&gt;pop(); return res;&#125; 算法和数据操作 总览 类型 题型 备注 递归和循环 树的遍历 递归简介，循环效率高 排序和查找 二分查找、归并排序、快速排序 正确、完整写出代码 二维数组 迷宫、棋盘 回溯法；栈模拟递归 最优解 动态规划，问题分解为多个子问题 自上而下递归分析；自下而上循环代码实现，数组保存 最优解 贪心算法 分解时是否存在某个特殊选择：贪心得到最优解 与、或、异或、左移、右移 递归效率低的原因：函数调用自身，函数调用是由时间和空间的消耗；会在内存栈中分配空间以保存参数，返回地址和临时变量，往栈里弹入和弹出都需要时间。 递归和循环：题10，斐波那契数列；题60，n个骰子的点数。 动态规划 递归思路分析，递归分解的子问题中存在着大量的重复。用自下而上的循环来实现代码。题14 剪绳子， 题47礼物的最大价值 ， 题48最长不含重复字符的子字符串 斐波那契数列-递归循环-10 斐波那契数列 数列定义 \\[ f(n) = \\begin{cases} &amp;0 &amp; n=0 \\\\ &amp;1 &amp; n=1 \\\\ &amp;f(n-1) + f(n-2) &amp; n \\ge 1 \\end{cases} \\] 递归和循环两种实现策略 关键代码 1234567891011121314151617181920long long fibonacci_recursion(unsigned int n) &#123; if (n &lt;= 0) return 0; if (n == 1) return 1; return fibonacci_recursion(n-1) + fibonacci_recursion(n-2);&#125;long long fibonacci_loop(unsigned int n) &#123; if (n &lt;= 0) return 0; if (n == 1) return 1; long long f1 = 0; long long f2 = 1; long long fn = 0; for (unsigned int i = 2; i &lt;= n; i++) &#123; fn = f1 + f2; f1 = f2; f2 = fn; &#125; return fn;&#125; 青蛙跳台阶 青蛙可以一次跳1个台阶，一次跳2个台阶。问青蛙跳n个台阶有多少种跳法。 青蛙跳到第n个台阶有两种跳法：跳1个和2个。所以\\(f(n)=f(n-1)+f(n-2)\\) 。是斐波那契数列。 扩展 青蛙一次可以跳1个台阶、2个台阶、n个台阶。问有多少种跳法？ 数学归纳法证得：\\(f(n) = 2^{n-1}\\) 矩阵覆盖问题 \\(2\\times1\\)矩阵去覆盖\\(2 \\times 8\\) 矩阵，可以横着竖着覆盖，问多少种覆盖方法？ 同理，最后一个横着放或者竖着放。\\(f(8)=f(7)+f(6)\\)","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"},{"name":"排序","slug":"排序","permalink":"http://plmsmile.github.io/tags/排序/"}]},{"title":"简单的卷积神经网络","date":"2017-07-18T11:23:56.000Z","path":"2017/07/18/cnn-mnist/","text":"卷积神经网络 概要 卷积神经网络(Convolutional Neural Network, CNN)是人工神经网络中的一种，是一种特殊的对图像识别的方式，属于非常有效的带有前向反馈的网络。也用于音频信号、文本数据、人脸识别等等。$ $ CNN不需要把特征提取和分类训练两个过程分开，在训练的时候就提取了最有效的特征，降低了对图形数据预处理的要求。 卷积神经网络的核心思想是将输入信息切分成一个个子采样层进行采样，然后将提取的特征和权重值作为参数，传导到下一层 主要思路 每一个卷积操作只处理一小块图像，提取出最有效的特征，传给紧接着的池化层 池化层用来降采样，求局部平均和二次采样 循环上面两种操作 不停地对基础特征进行组合和抽象，得到更高阶的特征 主要特点 局部连接：减少了连接数量 权值共享：大幅度减少参数数量，防止过拟合又降低了复杂度 降采样紧跟卷积层：对样本有较高的畸变容忍能力 特征分区提取、时间或空间采样等规则 理论上具有对图像缩放、平移和旋转的不变性，有着很强的泛化性 网络结构 传统网路的问题 传统网络是全连接的，假如图像比较大是[200, 200, 3]，则会有\\(200*200*3=12000\\)个神经元。这样庞大的神经元做全连接结构是非常浪费的，并且有大量的参数会导致过拟合。 卷积神经网络 每一个像素点在空间上和周围的像素点实际上是有紧密联系的，但是和太遥远的像素点就不一定有什么联系了。这也是视觉感受野的概念，每一个感受野只接受一小块区域的信号。 一个卷积神经网络通常由多个卷积层组成，每一个卷积操作只处理一小块图像（也称作卷积核滤波），提取出最有效的特征传递给后面，主要操作如下： 一个卷积核可以提取出一种特征。一个图像经历一个卷积核之后，会输出一个新的图像，称作特征图谱(Feature Map, FM)。其实卷积操作也是\\(w * x + \\vec b\\)，只不过对于单次的卷积操作\\(w\\)和\\(\\vec b\\)是不变的，而\\(x\\)是会变的，每次取图片的一小块，会得到很多的结果，拼凑起来就是一张新的FM 将前面卷积的滤波结果FM，进行非线性的激活函数处理。之前是sigmoid，现在是Relu函数，比较完美解决梯度弥散的问题。 对激活函数的结果进行降采样（池化操作），比如将[2,2]将为[1,1]的图片。常用的有最大值合并、平均值合并和随机合并。思路和卷积差不多，只不过结果不用乘加，只是选择最大的就行了。 最后一个子采样层一般会全连接一个或多个全连接层，全连接层的输出就是最后的输出。一般是softmax","tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://plmsmile.github.io/tags/tensorflow/"},{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"卷积","slug":"卷积","permalink":"http://plmsmile.github.io/tags/卷积/"}]},{"title":"利用tensorflow实现简版word2vec","date":"2017-07-14T12:17:50.000Z","path":"2017/07/14/word2vec/","text":"相关知识 传统方法 One-Hot Encoder 是一个词对应一个向量，向量中只有一个是1，其余是0，离散表达。$ $ Bag of Words 标识当前单词那一位不是1，而是变成了当前单词的出现次数。 存在的问题 需要大量的维数去表示，编码随机的，没有任何关联的信息。 向量空间模型 Vector Space Models可以把字词转化为连续值，并将意思相近的词被映射到向量空间相近的位置。 VSM在NLP中的一个重要假设是：在相同语境中出现的词，语义也相近。 有如下两种模型 计数模型 统计语料库中相邻出现的词的频率，再把这些计数结果转为小而稠密的矩阵。 预测模型 根据一个词周围相邻的词汇推测出这个词。 Word2Vec Word2Vec是一种计算非常高效的、可以从原始语料中学习字词空间向量的预测模型。 有如下两种模型 CBOW Continuous Bag of Words 从语境推测目标词汇，适合小型数据。如“中国的首都是__”推测出“北京”。把一整段上下文信息当做一个观察对象 Skip-Gram 从目标词汇推测语境，适合大型语料。 把每一对上下文-目标词汇当做一个观察对象 Word2Vec的一些优点 连续的词向量能够捕捉到更多的语义和关联信息 意思相近的词语在向量空间中的位置也会比较近。如北京-成都、狗-猫等词汇会分别聚集在一起。 能学会一些高阶语言的抽象概念。如&quot;man-woman&quot;和&quot;king-queen&quot;的向量很相似。 Word2Vec学习的抽象概念 噪声对比训练 神经概率化语言模型通常使用极大似然法进行训练，再使用Softmax函数得到在给出上下文单词\\(h\\)的情况下，目标词\\(w_t\\)出现的最大概率，设为\\(P(w_t|h)\\)。 设\\(score(w_t, h)\\)为当前词\\(w_t\\)和上下文单词\\(h\\)的相容性，通常使用向量积获得。 \\[ P(w_t|h) = Softmax(score(w_i, h))=\\frac{e^{score(w_i, h)}} {\\sum_{i=1}^v {e^{score(w_i, h)}}} \\] 通过对数似然函数max likelihood来进行训练 \\[ J_{ml}=\\ln{P(w_t|h)}=score(w_t,h)-\\ln{\\sum_{i=1}^v e^{score(w_i, h)}} \\] 这个方法看起来可行，但是消耗太大了，因为要对当前\\(h\\)与所有单词\\(w\\)的相容性\\(score(w, h)\\)。 在使用word2vec模型中，我们并不需要对所有的特征进行学习。所以在CBOW模型和Skip-Gram模型中，会构造\\(k\\)个噪声单词，而我们只需要从这k个中找出真正目标单词\\(w_t\\)即可，使用了一个二分类器（lr）。下面是CBOW模型，对于Skip-Gram模型只需要相反就行了。 设\\(Q_\\theta(D=1|w, h)\\)是二元逻辑回归的概率，即在当前条件下出现词语\\(w\\)的概率。 \\(\\theta\\) 输入的embedding vector \\(h\\) 当前上下文 \\(d\\) 输入数据集 \\(w\\) 目标词汇（就是他出现的概率） 此时，最大化目标函数如下： \\[ J_{NEG}=\\ln{Q_\\theta(D=1|w_t, h)} + \\frac {\\sum_{i=1}^{k}{\\ln Q_\\theta(D=0|w_I, h)}} {k} \\] 前半部分为词\\(w\\)出现的概率，后面为\\(k\\)个噪声概率的期望值（如果写法有错误，希望提出，再改啦），有点像蒙特卡洛。 负采样Negative Sampling 当模型预测的真实目标词汇\\(w_t\\)的概率越高，其他噪声词汇概率越低，模型就得到优化了 用编造的噪声词汇进行训练 计算loss效率非常高，只需要随机选择\\(k\\)个，而不是全部词汇 实现Skip-Gram模型 数据说明 Skip-Gram模型是通过目标词汇预测语境词汇。如数据集如下 1I hope you always find a reason to smile 从中我们可以得到很多目标单词和所对应的上下文信息（多个单词）。如假设设左右词的窗口距离为1，那么相应的信息如下 1&#123;'hope':['i', 'you'], 'you':['hope', 'alawys']...&#125; 训练时，希望给出目标词汇就能够预测出语境词汇，所以需要这样的训练数据 12345# 前面是目标单词，后面是语境词汇，实际上相当于数据的label('hope', 'i')('hope', 'you')('you', 'hope')('you', 'always') 同时在训练时，制造一些随机单词作为负样本（噪声）。我们希望预测的概率分布在正样本上尽可能大，在负样本上尽可能小。 使用随机梯度下降算法(SGD)来进行最优化求解，并且使用mini-batch的方法，这样来更新embedding中的参数\\(\\theta\\)，让损失函数(NCE loss)尽可能小。这样，每个单词的词向量就会在训练的过程中不断调整，最后会处在一个最合适的语料空间位置。 例如，假设训练第\\(t\\)步，输入目标单词hope，希望预测出you，选择一个噪声词汇reason。则目标函数如下 \\[ J_{NEG}^{(t)}=\\ln {Q_\\theta(D=1|hope, you)} + \\ln{Q_\\theta(D=0|hope, reason)} \\] 目标是更新embedding的参数\\(\\theta\\)以增大目标值，更新方式是计算损失函数对参数\\(\\theta\\)的导数，使得参数\\(\\theta\\)朝梯度方向进行调整。多次以后，模型就能够很好区别出真实语境单词和噪声词。 构建数据集 先来分析数据，对所有的词汇进行编码。对高频词汇给一个id，对于出现次数很少词汇，id就设置为0。高频是选择出现频率最高的50000个词汇。 1234567891011121314151617181920212223242526272829303132def build_dataset(self, words): ''' 构建数据集 Args: words: 单词列表 Returns: word_code: 所有word的编码，top的单词：数量；其余的：0 topword_id: topword-id id_topword: id-word topcount: 包含所有word的一个Counter对象 ''' # 获取top50000频数的单词 unk = 'UNK' topcount = [[unk, -1]] topcount.extend( collections.Counter(words).most_common( self.__vocab_size - 1)) topword_id = &#123;&#125; for word, _ in topcount: topword_id[word] = len(topword_id) # 构建单词的编码。top单词：出现次数；其余单词：0 word_code = [] unk_count = 0 for w in words: if w in topword_id: c = topword_id[w] else: c = 0 unk_count += 1 word_code.append(c) topcount[0][1] = unk_count id_topword = dict(zip(topword_id.values(), topword_id.keys())) return word_code, topword_id, id_topword, topcount 产生batch训练样本 由于是使用mini-batch的训练方法，所以每次要产生一些样本。对于每个单词，要确定要产生多少个语境单词，和最多可以左右选择多远。 12345678910111213141516171819202122232425262728293031323334353637383940414243def generate_batch(self, batch_size, single_num, skip_window, word_code): '''产生训练样本。Skip-Gram模型，从当前推测上下文 如 i love you. (love, i), (love, you) Args: batch_size: 每一个batch的大小，即多少个() single_num: 对单个单词生成多少个样本 skip_window: 单词最远可以联系的距离 word_code: 所有单词，单词以code形式表示 Returns: batch: 目标单词 labels: 语境单词 ''' # 条件判断 # 确保每个batch包含了一个词汇对应的所有样本 assert batch_size % single_num == 0 # 样本数量限制 assert single_num &lt;= 2 * skip_window # batch label batch = np.ndarray(shape=(batch_size), dtype=np.int32) labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # 目标单词和相关单词 span = 2 * skip_window + 1 word_buffer = collections.deque(maxlen=span) for _ in range(span): word_buffer.append(word_code[self.__data_index]) self.__data_index = (self.__data_index + 1) % len(word_code) # 遍历batchsize/samplenums次，每次一个目标词汇，一次samplenums个语境词汇 for i in range(batch_size // single_num): target = skip_window # 当前的单词 targets_to_void = [skip_window] # 已经选过的单词+自己本身 # 为当前单词选取样本 for j in range(single_num): while target in targets_to_void: target = random.randint(0, span - 1) targets_to_void.append(target) batch[i * single_num + j] = word_buffer[skip_window] labels[i * single_num + j, 0] = word_buffer[target] # 当前单词已经选择完毕，输入下一个单词，skip_window单词也成为下一个 self.__data_index = (self.__data_index + 1) % len(word_code) word_buffer.append(word_code[self.__data_index]) return batch, labels 一些配置信息 12345678910111213141516171819# 频率top50000个单词vocab_size = 50000# 一批样本的数量batch_size = 128# 将单词转化为稠密向量的维度embedding_size = 128# 为单词找相邻单词，向左向右最多能取得范围skip_window = 1# 每个单词的语境单词数量single_num = 2# 验证单词的数量valid_size = 16# 验证单词从频数最高的100个单词中抽取valid_window = 100# 从100个中随机选择16个valid_examples = np.random.choice(valid_window, valid_size, replace=False)# 负样本的噪声数量noise_num = 64 计算图 1234567891011121314151617181920212223242526272829303132333435363738394041424344graph = tf.Graph()with graph.as_default(): # 输入数据 train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) valid_dataset = tf.constant(valid_examples, dtype=tf.int32) with tf.device('/cpu:0'): # 随机生成单词的词向量，50000*128 embeddings = tf.Variable( tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0)) # 查找输入inputs对应的向量 embed = tf.nn.embedding_lookup(embeddings, train_inputs) nce_weights = tf.Variable( tf.truncated_normal([vocab_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) nce_biases = tf.Variable(tf.zeros([vocab_size])) # 为每个batch计算nceloss loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels = train_labels, inputs=embed, num_sampled=noise_num, num_classes=vocab_size)) # sgd optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss) # 计算embeddings的L2范式，各元素的平方和然后求平方根，防止过拟合 norm = tf.sqrt( tf.reduce_sum( tf.square(embeddings), axis=1, keep_dims=True)) # 标准化词向量 normalized_embeddings = embeddings / norm valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset) # valid单词和所有单词的相似度计算，向量相乘 similarity = tf.matmul( valid_embeddings, normalized_embeddings, transpose_b=True) init = tf.global_variables_initializer() 训练过程 123456789101112131415161718192021222324252627282930313233num_steps = 100001with tf.Session(graph=graph) as sess: init.run() print('Initialized') avg_loss = 0 for step in range(num_steps): batch_inputs, batch_labels = wu.generate_batch( batch_size, single_num, skip_window, word_code) feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125; _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict) avg_loss += loss_val if step % 2000 == 0: if step &gt; 0: avg_loss /= 2000 print (\"avg loss at step %s : %s\" % (step, avg_loss)) avg_loss = 0 if step % 10000 == 0: # 相似度，16*50000 sim = similarity.eval() for i in range(valid_size): valid_word = id_topword[valid_examples[i]] # 选相似的前8个 top_k = 8 # 排序，获得id nearest = (-sim[i, :]).argsort()[1:top_k+1] log_str = \"Nearest to %s: \" % valid_word for k in range(top_k): close_word = id_topword[nearest[k]] log_str = \"%s %s,\" % (log_str, close_word) print log_str final_embeddings = normalized_embeddings.eval()","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"}]},{"title":"机器学习-朴素贝叶斯","date":"2017-05-06T06:36:58.000Z","path":"2017/05/06/ml-ch03-bayes/","text":"条件概率 基础知识 条件概率 在\\(B\\)发生的情况下\\(A\\)的概率$ $ ​ \\(P(A|B) = \\frac{P(AB)}{P(B)}\\) ​ \\(P(c_i|x)=\\frac{P(c_ix)}{P(x)}\\)。\\(c_i\\)是类别，\\(x\\)是一个向量。\\(x\\)属于类别\\(c_i\\)的概率。 贝叶斯准则 交换条件概率中的条件与结果，得到想要的值。 \\(P(A|B) = \\frac{P(AB)}{P(B)}\\), \\(P(B|A) = \\frac{P(AB)}{P(A)}\\) \\(\\to\\) \\(P(B|A)=\\frac{P(A|B)P(B)}{P(A)}\\) 所以可以得到\\(\\color{red}{P(c_i|x)}=\\frac{P(x|c_i)P(c_i)}{P(x)}\\) 条件概率分类 贝叶斯决策理论 计算两个概率\\(x\\)属于类别1和类别2的概率\\(p_1(x)\\)和\\(p_2(x)\\)。 如果\\(p_1(x) &gt; p_2(x)\\)，则\\(x\\)属于类别1 如果\\(p_2(x) &gt; p_1(x)\\)，则\\(x\\)属于类别2 贝叶斯准则 \\(x\\)属于类别\\(c_i\\)的概率是\\(\\color{red}{P(c_i|x)}\\)。 如果\\(P(c_1|x) &gt; P(c_2|x)\\)，则\\(x\\)属于\\(c_1\\) 如果\\(P(c_2|x) &gt; P(c_1|x)\\)，则\\(x\\)属于\\(c_2\\) 朴素贝叶斯文档分类 简介 机器学习的一个重要应用就是文档的自动分类。我们可以观察文档中出现的词，并把每个词出现与否或者出现次数作为一个特征。朴素贝叶斯就是用于文档分类的常用算法，当然它可以用于任意场景的分类。 向量\\(\\color{red}{\\vec{w}}={(w_1,w_2,...,w_n)}\\)代表一篇文章。其中\\(w_i=0,1\\)，代表词汇表中第\\(i\\)个词汇出现与否。词汇表是指一个总体的全局词汇表。文章\\(\\vec{w}\\)属于第\\(i\\)类的概率\\(\\color{red}{P(c_i|\\vec{w})}=\\frac{P(\\vec{w}|c_i)P(c_i)}{P(\\vec{w})}\\)。 朴素贝叶斯分类器的两个假设： 特征之间相互独立 每个特征同等重要 尽管这有瑕疵，但是朴素贝叶斯的实际效果却很好了。 朴素贝叶斯分类器的两种实现： 伯努利模型：只考虑出现或者不出现 多项式模型：考虑词在文档中的出现次数 文档分类中的独立：每个单词出现的可能性和其他单词没有关系。独立的好处在下面概率计算中会体现出来。 概率计算 对每一个文章的各个分类概率计算，其实只需要计算上式的分母就行了。 对于\\(P(c_i)=\\frac{c_i数量}{总数量}\\)，即\\(c_i\\)类文章的数量除以所有类别的文章的总数量。 对于\\(P(\\vec{w}|c_i)\\)，要稍微复杂一些。由于各个特征（单词出现否）独立，则有如下推导公式： \\[P(\\vec{w}|c_i)=P(w_1,w_2,...,w_n|c_i)=P(w_1|c_i)P(w_2|c_i)\\cdots P(w_n|c_i)\\] 其中\\(\\color{red}{P(w_i|c_i)}\\)代表第\\(i\\)个单词在\\(c_i\\)类别文章的总词汇里出现的概率。 实际操作的一个小技巧，由于概率都很小多个小值做乘法会导致下溢出，所以决定对概率取对数做加法，最后再比较对数的大小。 \\[\\ln(P(\\vec{w}|c_i))=\\ln(P(w_1|c_i))+\\ln(P(w_2|c_i))+\\dots+\\ln(P(w_n|c_i))\\] 如上，可以求得每个单词在各个类别文章里出现的概率。用\\(\\color{red}{\\vec{wp_0}}\\)、\\(\\color{red}{\\vec{wp_1}}\\)来分别表示所有单词在类别0、类别1中总词汇中的概率。当然，在程序中实际上这个概率是取对数了的。 当要求一篇新的文章\\(\\color{red}{\\vec{w}}={(0,1,0,0,\\dots)}\\)，此时为出现或者不出现，当然也可以统计出现次数，属于哪个类别的时候，要先求出\\(\\color{red}{P(w|c_0)}\\)和\\(\\color{red}{P(w|c_1)}\\)，然后根据贝叶斯准则选择概率大的分类为结果。 \\[P(w|c_0)=\\vec{w}\\cdot\\vec{wp_0}, P(w|c_1)=\\vec{w}\\cdot\\vec{wp_1}\\] 程序实现 朴素贝叶斯的实例应有很多，这里主要是介绍垃圾邮件分类。数据集中的邮件有两种：垃圾邮件和正常邮件。每个类型都有25个样本，一共是50个样本。我们对数据集进行划分为训练集和测试集。训练集用来训练获得\\(\\vec{wp_0}\\)、\\(\\vec{wp_1}\\)和\\(p(c_1)\\)。然后用测试集去进行朴素贝叶斯分类，计算错误率，查看效果。 加载数据 数据是存放在两个文件夹中的，以txt格式的形式存储。取出来后要进行单词切割。然后得到邮件列表email_list和它对应的分类列表class_list。 1234567891011121314151617181920212223242526272829303132333435def parse_str(big_str): ''' 解析文本为单词列表 Args: big_str: 长文本 Returns: 单词列表 ''' # 以任何非单词字符切割 word_list = re.split(r'\\W*', big_str) # 只保留长度大于3的单词，并且全部转化为小写 return [word.lower() for word in word_list if len(word) &gt; 2]def load_dataset(spam_dir, ham_dir): ''' 从文件夹中加载文件 Args: spam_dir: 垃圾邮件文件夹 ham_dir: 正常邮件文件夹 Returns: email_list: 邮件列表 class_list: 分类好的列表 ''' email_list = [] class_list = [] txt_num = 25 # 每个文件夹有25个文件 for i in range(1, txt_num + 1): for j in range(2): file_dir = spam_dir if j == 1 else ham_dir f = open(('&#123;&#125;/&#123;&#125;.txt').format(file_dir, i)) f_str = f.read() f.close() words = parse_str(f_str) email_list.append(words) # 邮件列表 class_list.append(j) # 分类标签，1垃圾邮件，0非垃圾邮件 return email_list, class_list 划分数据集 由于前面email_list包含所有的邮件，下标是从0-49，所以我们划分数据集只需要获得对应的索引集合就可以了。 123456789101112131415def get_train_test_indices(data_num): ''' 划分训练集和测试集 Args: data_num: 数据集的数量 Returns: train_indices: 训练集的索引列表 test_indices: 测试集的索引列表 ''' train_indices = range(data_num) test_ratio = 0.3 # 测试数据的比例 test_num = int(data_num * test_ratio) test_indices = random.sample(train_indices, test_num) # 随机抽样选择 for i in test_indices: train_indices.remove(i) return train_indices, test_indices 获得训练矩阵 获得训练数据之后，要把训练数据转化为训练矩阵。 获得所有的词汇 1234567891011def get_vocab_list(post_list): ''' 从数据集中获取所有的不重复的词汇列表 Args: post_list: 多个文章的列表，一篇文章：由单词组成的list Returns: vocab_list: 单词列表 ''' vocab_set = set([]) for post in post_list: vocab_set = vocab_set | set(post) return list(vocab_set) 获得一篇文章的文档向量 12345678910111213141516171819202122def get_doc_vec(doc, vocab_list, is_bag = False): ''' 获得一篇doc的文档向量 词集模型：每个词出现为1，不出现为0。每个词出现1次 词袋模型：每个词出现次数，可以多次出现。 Args: vocab_list: 总的词汇表 doc: 一篇文档，由word组成的list is_bag: 是否是词袋模型，默认为Fasle Returns: doc_vec: 文档向量，1出现，0未出现 ''' doc_vec = [0] * len(vocab_list) for word in doc: if word in vocab_list: idx = vocab_list.index(word) if is_bag == False: # 词集模型 doc_vec[idx] = 1 else: doc_vec[idx] += 1 # 词袋模型 else: print '词汇表中没有 %s ' % word return doc_vec 获得训练矩阵 1234567891011121314151617181920def go_bayes_email(): ''' 贝叶斯垃圾邮件过滤主程序 Returns: error_rate: 错误率 ''' # 源数据 email_list, class_list = load_dataset('email/spam', 'email/ham') # 总的词汇表 vocab_list = bys.get_vocab_list(email_list) # 训练数据，测试数据的索引列表 data_num = len(email_list) train_indices, test_indices = get_train_test_indices(data_num) # 训练数据的矩阵和分类列表 train_mat = [] train_class = [] for i in train_indices: vec = bys.get_doc_vec(email_list[i], vocab_list) train_mat.append(vec) train_class.append(class_list[i]) # 后续还有训练数据和测试数据，在下文给出 贝叶斯算法 贝叶斯训练算法 通过训练数据去计算上文提到的\\(\\vec{wp_0}\\)、\\(\\vec{wp_1}\\)和\\(p(c_1)\\)。 1234567891011121314151617181920212223242526272829303132333435def train_nb0(train_mat, class_list): ''' 朴素贝叶斯训练算法，二分类问题 Args: train_mat: 训练矩阵，文档向量组成的矩阵 class_list: 每一篇文档对应的分类结果 Returns: p0_vec: c0中各个word占c0总词汇的概率 p1_vec: c1中各个word占c1总词汇的概率 p1: 文章是c1的概率 ''' # 文档数目，单词数目 doc_num = len(train_mat) word_num = len(train_mat[0]) # 两个类别的总单词数量 c0_word_count = 2.0 c1_word_count = 2.0 # 向量累加 c0_vec_sum = np.ones(word_num) c1_vec_sum = np.ones(word_num) for i in range(doc_num): if class_list[i] == 0: c0_word_count += sum(train_mat[i]) c0_vec_sum += train_mat[i] else: c1_word_count += sum(train_mat[i]) c1_vec_sum += train_mat[i] c1_num = sum(class_list) p1 = c1_num / float(doc_num) p0_vec = c0_vec_sum / c0_word_count p1_vec = c1_vec_sum / c1_word_count # 由于后面做乘法会下溢出，所以取对数做加法 for i in range(word_num): p0_vec[i] = math.log(p0_vec[i]) p1_vec[i] = math.log(p1_vec[i]) return p0_vec, p1_vec, p1 贝叶斯分类 123456789101112131415def classify_nb(w_vec, p0_vec, p1_vec, p1): ''' 使用朴素贝叶斯分类 Args: w_vec: 要测试的向量 p0_vec: c0中所有词汇占c0的总词汇的概率 p1_vec: c1中所有词汇占c1的总词汇的概率 p1: 文章为类型1的概率，即P(c1) ''' # P(w|c0)*P(c0) = P(w1|c0)*...*P(wn|c0)*P(c0) # 由于下溢出，所以上文取了对数，来做加法 w_p0 = sum(w_vec * p0_vec) + math.log(1 - p1) w_p1 = sum(w_vec * p1_vec) + math.log(p1) if w_p0 &gt; w_p1: return 0 return 1 训练数据 1p0_vec, p1_vec, p1 = bys.train_nb0(train_mat, train_class) 测试数据 一次执行 12345678910111213141516def go_bayes_email(): # 此处省略上文的部分内容 # 训练数据 p0_vec, p1_vec, p1 = bys.train_nb0(train_mat, train_class) # 测试数据 error_count = 0 for i in test_indices: vec = bys.get_doc_vec(email_list[i], vocab_list) res = bys.classify_nb(vec, p0_vec, p1_vec, p1) if res != class_list[i]: error_count += 1 error_rate = error_count / float(data_num) print 'error=%d, rate=%s, test=%d, all=%d' % (error_count, error_rate, len(test_indices), data_num) return error_rate 多次执行，取平均值 1234567def test_bayes_email(): ''' 执行多次go_bayes_email，计算平均错误率 ''' times = 100 error_rate_sum = 0.0 for i in range(10): error_rate_sum += go_bayes_email() print 'average_rate = %s' % (error_rate_sum / 10) 源代码","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","permalink":"http://plmsmile.github.io/tags/朴素贝叶斯/"}]},{"title":"NumPy","date":"2017-04-15T07:32:52.000Z","path":"2017/04/15/NumPy/","text":"NumPy教程 基础 简单demo NumPy中最重要的对象是ndarray，是一个N维数组。它存储着相同类型的元素集合。通过dtype来获取类型，索引来获取值。 通过numpy.array来创建ndarray。 123456789101112# 1. numpy定义numpy.array(object, dtype = None, copy = True, order = None, subok = False, ndmin = 0)# 2. demoimport numpy as npa = np.array([[1, 2], [3, 4]])# 3. 使用dtype#int8, int16, int32, int64 可替换为等价的字符串 'i1', 'i2', 'i4', 以及其他。dt = np.dtype(np.int32)student = np.dtype([('name','S20'), ('age', 'i1'), ('marks', 'f4')])a = np.array([('tom', 23, 89), ('sara', 22, 97)], dtype=student) ndarray.shape 和reshape 获取数组维度 ，也可以调整大小 123456789a = np.array([[1,2,3], [4,5,6]]) print a.shape# (2, 3)b = a.reshape(3,2) b.shapeprint b[[1, 2] [3, 4] [5, 6]] ndarray.ndim 数组的维数 123import numpy as npa = np.arange(24).reshape(2, 12) # 2b = a.reshape(2, 3, 4) # 3 创建数组 输入数组建立 1a = np.array([[1,2,3], [4,5,6]]) zeros, ones, empty 123456# zeros创建0矩阵np.zeros((3, 4))# ones创建1矩阵np.ones((2, 3, 4), dtype=np.int16)# empty不初始化数组，值随机np.empty((2, 3)) arange, linspace 创建随机数，整数和浮点数，步长 12345678# 创建[0, n-1]的数组np.arange(3)# 创建1-10范围类，3个数np.arange(1, 10, 3)# 取均值步长np.linspace(0, 1.5, 3)# array([ 0. , 0.75, 1.5 ]) 基本操作 基本数学操作 1234567891011121314151617181920212223242526272829303132333435363738394041a = np.array([20, 30, 40, 50])b = np.arrange(4)# 减法c = b - a # 乘法b * 2 # 新建一个矩阵b *= 2 # 直接改变b，不会新建一个矩阵 a += 2 同理# 次方b ** 2# 判断a &lt; 30 # array([ True, False, False, False], dtype=bool)10 * np.sin(a)# 矩阵乘法A = np.array([[1, 1], [0, 1]])B = np.array([[2, 0], [3, 4]])A.dot(B)B.dot(A)np.dot(A, B)# sum, max, mina = np.arange(12).reshape(3, 4)array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])# 所有元素sum, min, maxa.sum()a.max()# 使用axis=0按列, axis=1按行a.sum(axis=0)array([12, 15, 18, 21])a.sum(axis=1)array([ 6, 22, 38])# 通用函数B = np.arange(3)np.exp(B) # 求e的次方np.sqrt(B) # 开方C = np.array([2, -1, 4])np.add(B, C) # 相加 访问元素，index, slice, iterator 12345678910111213141516171819202122232425262728293031# 1. 一维数组a = np.arange(4)**2 # array([0, 1, 4, 9])# 下标访问a[1] # 从0开始 # 1# 切片，同python切片a[1:3] # array([1, 4])# 迭代for i in a: print (i*2) # 2. 多维数组a = np.fromfunction(lambda i, j: i + j, (3, 3), dtype=int) # 对下标进行操作array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])a[1, 2] # 访问到 3# 访问第2列a[0:3, 1] # array([1, 2, 3])a[:, 1] # array([1, 2, 3]) # 第i+1行a[1] # 第2行a[-1] # 最后一行a[1, ...] # 第2行，多维的时候这样写# 第2、3行a[1:3, :]a[1:3]for row in a: print rowfor e in a.flat: print e 切片(start, end, step) 123456789101112131415161718# 1. (start, end, step)a = np.arange(10)s = slice(2, 7, 2) b = a[s]# 2. 1-7, step=3, 不包括7b = a[1:7:3]# 3. 从idx开始向后切，包括idxb = a[2:]# 4. start, end, 不包括endb = a[2:5]# 5. a = np.array([[1,2,3],[3,4,5],[4,5,6]]) a[..., 1] #第2列 [2 4 5]a[1, ...] #第2行 [3 4 5]a[...,2:] #第2列及其剩余元素[[2 3] [4 5] [5 6]] 索引 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 1. 一维时a = np.arange(5)**2 # array([ 0, 1, 4, 9, 16])# 索引1i = np.array([1, 3, 4]) # idx 为1,3,4的元素a[i] # 访问元素 array([ 1, 9, 16])# 索引2j = np.array([ [1, 2], [3, 4]])a[j]array([[ 1, 4], [ 9, 16]])# 2. 二维时x = np.array([[1, 2], [3, 4], [5, 6]]) [[1 2] [3 4] [5 6]]# 索引y = x[ [0, 1, 2], [0, 1, 0]] # [0, 1, 2]是行, [0, 1, 0]是对应行的列[1 4 5]# 3. 没看懂x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]# 索引 rows = np.array([ [0,0], [3,3] ])cols = np.array([ [0,2], [0,2] ])i = [rows, cols]y = x[i][[ 0 2] [ 9 11]]# 4. 切片+索引x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) # 切片, 1-3行, 1-2列z = x[1:4,1:3] [[ 4 5] [ 7 8] [10 11]]# 高级索引来切片, 1-3行,1、2列y = x[1:4, [1,2]][[ 4 5] [ 7 8] [10 11]] Shape Manipulation 改变形状 1234567891011121314151617a = np.floor(10*np.random.random((3,4))) # &lt;1的小数*10，取整array([[ 9., 6., 3., 8.], [ 2., 8., 4., 2.], [ 5., 3., 3., 1.]])# 形状a.shape(3, 4)# 打平，返回arraya.ravel()array([ 9., 6., 3., 8., 2., 8., 4., 2., 5., 3., 3., 1.])# reshape 生成新的a.reshape(2, 6) a.reshape(3, -1) # 给定一个，自动计算另外的# resize 改变自己a.resize(4, 3)# 转置a.T 堆积不同的阵列 123456789101112131415161718192021a = np.floor(10*np.random.random((2, 2)))b = np.floor(10*np.random.random((2, 2)))# 垂直堆积 (4, 2)np.vstack((a, b))[[ 1., 7.], [ 9., 8.], [ 9., 0.], [ 8., 6.]]# 水平堆积 (2, 4)np.hstack((a, b))array([[ 1., 7., 9., 0.], [ 9., 8., 8., 6.]])# 特别的，针对一维的列堆积a = np.array((1,2,3))b = np.array((2,3,4))np.column_stack((a,b))[[1, 2], [2, 3], [3, 4]]","tags":[{"name":"Python","slug":"Python","permalink":"http://plmsmile.github.io/tags/Python/"},{"name":"NumPy","slug":"NumPy","permalink":"http://plmsmile.github.io/tags/NumPy/"}]},{"title":"机器学习-西瓜书-第一章习题","date":"2017-04-04T04:07:22.000Z","path":"2017/04/04/ml-watermelon-chap1/","text":"$ $ 1.1版本空间 题目 表1.1中若只包含编号为1和4的两个用例，试给出相应的版本空间。 背景知识 假设空间：假设数据有\\(n\\)中属性，第\\(i\\)个属性可能的取值有\\(t_i\\)种，加上该属性的泛化取值(*)，所以可能的假设有\\(\\prod_{i=1}^n {(t_i+1)}\\)种。再用空集表示没有正例，所以假设空间一共有\\(\\prod_{i=1}^n {(t_i+1)} + 1\\)种假设。 学习过程：在假设空间中进行搜索去找到与训练集匹配的假设。即能够将训练集中的瓜判断正确的假设。 版本空间：多个与训练集一致的假设组成的集合。 解答 西瓜数据集，本题只取1和4。 编号 色泽 根蒂 敲声 好瓜 1 青绿 蜷缩 浊响 是 2 乌黑 蜷缩 浊响 是 3 青绿 硬挺 清脆 否 4 乌黑 稍蜷 沉闷 否 获得好瓜的布尔表达式是：\\(好瓜\\leftrightarrow (色泽=?)\\wedge(根蒂=?)\\wedge(敲声=?)\\)。 三个特征的取值分别是： 色泽：青绿，乌黑，* (*是说什么色泽都行，下面同理) 根蒂：蜷缩，稍蜷，* 敲声，浊响，沉闷，* 当然也可能本身没有“好瓜”这种东西，我们用\\(\\emptyset\\)来表示。 综上，一共有 \\(3\\times3\\times3+1=27\\)种假设。所以假设空间如下(先对正样本最大泛化)： 编号 色泽 根蒂 敲声 符合正样本？ 1 青绿 蜷缩 浊响 是 2 青绿 蜷缩 沉闷 3 青绿 蜷缩 * 是 4 青绿 稍蜷 浊响 5 青绿 稍蜷 沉闷 6 青绿 稍蜷 * 7 青绿 * 浊响 是 8 青绿 * 沉闷 9 青绿 * * 是 10 乌黑 蜷缩 浊响 11 乌黑 蜷缩 沉闷 12 乌黑 蜷缩 * 13 乌黑 稍蜷 浊响 14 乌黑 稍蜷 沉闷 15 乌黑 稍蜷 * 16 乌黑 * 浊响 17 乌黑 * 沉闷 18 乌黑 * * 19 * 蜷缩 浊响 是 20 * 蜷缩 沉闷 21 * 蜷缩 * 22 * 稍蜷 浊响 23 * 稍蜷 沉闷 24 * 稍蜷 * 是 25 * * 浊响 是 26 * * 沉闷 27 * * * 28 \\(\\emptyset\\) 所以版本空间为如下，共7个。 编号 色泽 根蒂 敲声 好瓜 1 青绿 蜷缩 浊响 是 3 青绿 蜷缩 * 是 7 青绿 * 浊响 是 9 青绿 * * 是 19 * 蜷缩 浊响 是 24 * 稍蜷 * 是 25 * * 浊响 是 1.2析合范式 题目 1.2 与使用单个合取式来进行假设表示相比，使用“析合范式”将使得假设空间具有更强的表示能力。例如： \\[ 好瓜\\leftrightarrow\\left((色泽=*) \\wedge(根蒂=蜷缩)\\wedge(敲声=*)\\right)\\vee\\left((色泽=乌黑)\\wedge(根蒂=*)\\wedge(敲声=沉闷)\\right) \\] 若使用包含\\(k\\)个合取式的析合范式来表达表1.1(上文中有)西瓜分类问题的假设空间，试估算共有多少种假设的可能。 解答 表1.1中4个样例，3个属性。假设空间中共有\\(3\\times4\\times4+1=49\\)种假设。 不考虑冗余 在假设空间中选取\\(k\\)个来组成析合范式，则有\\(\\sum_{k=1}^n {C_{49}^K}=\\color{red}{2^{49}}\\)种可能。但是其中包含了很多冗余的情况。 考虑冗余(忽略空集) 48种假设中： 具体假设：\\(2\\times3\\times3=18\\)种 1个属性泛化假设：\\(1\\times3\\times3+2\\times1\\times3+2\\times3\\times1=21\\)种 2个属性泛化假设：\\(2\\times1\\times1+1\\times3\\times1+1\\times1\\times3=8\\)种 3个属性泛化假设：\\(1\\times1\\times1=1\\)种 \\(k\\)的范围是：\\(1\\leq k \\leq18\\)。取1个或者所有的具体假设。 当\\(k=1\\)时，即只选取一种假设，这样不会有冗余情况，有\\(\\color {red} {48}\\)种可能。 当\\(k=18\\)时，即所有的具体假设，只有\\(\\color {red} {1}\\)种可能。 当\\(1&lt;k&lt;18\\)时，进行编程去循环遍历，按照：3属性泛化、2属性泛化、1属性泛化、具体属性排序，去遍历枚举。具体参见。 1.3归纳偏好 题目 若数据包含噪声，则假设空间中有可能不存在与所有训练样本都一致的假设。在此情形下，试设计一种归纳偏好用于假设选择。即不存在训练错误为0的假设。 解答 通常认为两个数据的属性越相近，则更倾向于把它们分为同一类。若相同属性出现了两种不同的分类，则认为它属于与它最邻近几个数据的属性。也可以考虑同时去掉所有具有相同属性而不同分类的数据，留下的就是没有误差的数据，但是可能会丢失部分信息。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"周志华","slug":"周志华","permalink":"http://plmsmile.github.io/tags/周志华/"},{"name":"西瓜书","slug":"西瓜书","permalink":"http://plmsmile.github.io/tags/西瓜书/"}]},{"title":"Spark-Programming","date":"2017-03-25T10:07:35.000Z","path":"2017/03/25/Spark-Programming/","text":"总览 Spark程序 有一个驱动程序，会运行用户的主要功能，并且在集群上执行各种并行操作。 RDD RDD是跨集群节点分区的、并且可以并行计算的分布式数据集合。可以通过外部文件系统或者内部集合来创建。可以在内存中持久化一个RDD，并且在并行计算中有效地重用。RDD可以从节点故障中自动恢复。 共享变量 当一组任务在不同的节点上并行运行一个函数时，Spark会为函数中的每个变量发送一个副本到各个任务中去(低效)。有时，变量需要在任务与任务、任务与驱动程序间共享。Spark有两种共享变量。 累加器：将工作节点中的值聚合到驱动程序中 广播变量：在各个节点中cache一个只读变量 SparkContext Spark的主要入口点。使用它可以连接到集群、创建RDD和广播变量。 RDD RDD是Spark中最核心的概念。 这是一个分布式的、容忍错误的、能并行操作的数据集合。 RDD是一个分布式的不可变的对象集合，可以包含任意对象。 每个RDD都会被分为多个分区，这些分区运行在不同的节点上。 Spark会自动把RDD的数据分发到集群上，并且并行化执行相关操作。 记录如何转化、计算数据的指令列表。 Spark中对数据的所有操作都是创建RDD、转化已有RDD、调用RDD操作进行求值。 创建RDD 创建RDD有两种方式：驱动程序内部的集合，外部系统的数据集(如HDFS, HBase等)。 集合 从集合中创建RDD，会把集合中的元素复制去创建一个可以并行执行的分布式数据集。 Spark可以对这些并行集合进行分区，把这些数据切割到多个分区。Spark会为集群的每个分区运行一个Task。一般，我们需要为集群中的每个CPU分配2-4个分区。默认，Spark会根据集群尝试自动设置分区数。但我们也可以手动地设置分区数。(有的代码中也称partition为slice) 123rdd = sc.parallelize([1, 2, 3, 4])rdd.reduce(lambda x, y: x + y) # 求和rdd2 = sc.parallelize(['Spark', 'Hadoop', 'ML', 'Python', 'Data'], 2) # 设置2个分区 外部数据集 Spark可以从本地文件系统、HDFS、Cassandra、HBase、Amazon S3等创建数据。支持Text、SequenceFile和任何其他Hadoop的Input Format。 Spark读取文件textFile的一些说明： 本地文件使用本地路径读取文件时，该文件也得在其它的worker node的相同路径上访问到。可以把文件复制过去或者使用network-mounted的文件共享系统。 支持文件 、文件夹、通配符、压缩文件(.gz)。 可以设置分区数。默认，Spark为文件的每一个块创建一个分区。(HDFS的block是128MB)。可以传递一个更大的值来请求更多的分区。 RDD操作 RDD主要有2个操作。 转化操作：由一个RDD生成一个新的RDD(Dataset)。惰性求值。 行动操作：会对RDD(Dataset)计算出一个结果或者写到外部系统。会触发实际的计算。 Spark会惰性计算这些RDD，只有第一次在一个行动操作中用到时才会真正计算。 一般，Spark会在每次行动操作时重新计算转换RDD。如果想复用，则用persist把RDD持久化缓存下来。可以持久化到内存、到磁盘、在多个节点上进行复制。这样，在下次查询时，集群可以更快地访问。 Spark程序大体步骤如下。 从外部数据创建输入RDD。如textFile 使用转化操作得到新的RDD。如map，filter 对重用的中间结果RDD进行持久化。如persist 使用行动操作来触发一次并行计算。如count, first 123456789# 从外部创建一个rdd。此时并没有把数据加载到内存中。lines只是一个指向文件的指针lines = sc.textFile(\"data.txt\")# 转化。没有进行真实的计算，因为惰性求值lineLengths = lines.map(lambda s: len(s))# 持久化lineLengths.persist()# 行动。Spark把计算分解为一些任务，这些任务在单独的机器上进行运算。# 每个机器只做属于自己map的部分，并且在本地reduce。返一个结果给DriverProgramtotalLength = lineLengths.reduce(lambda a, b: a + b) 传递函数给Spark Spark的API很多都依赖于传递函数来在集群上面运行。有下面3种方式可以使用： Lambda表达式：简单功能。不支持多语句函数、不支持没有返回值的语句。 本地def函数，调用spark。 模块的顶级函数。 代码较多时 1234def my_func(s): words = s.split(\" \") return len(words)len_rdd = sc.textFile(\"word.txt\").map(my_func) 对象方法时 千万不要引用self，这样会把整个对象序列化发送过去。而我们其实只需要一个方法或者属性就可以了，我们可以copy一份局部变量传递过去。 123456789101112131415161718class SearchFunctions(object): def __init__(self, query): self.query = query def is_match(self, s): return self.query in s def get_matches_func_ref(self, rdd): \"\"\"问题: self.is_match引用了整个self \"\"\" return rdd.filter(self.is_match) def get_matches_attr_ref(self, rdd): \"\"\"问题：self.query引用了整个self \"\"\" return rdd.filter(lambda s: self.query in s) def get_matches_no_ref(self, rdd): \"\"\"正确做法：使用局部变量 \"\"\" query = self.query return rdd.filter(lambda s: query in s) 理解闭包 当在集群上面执行代码时，理解变量和方法的范围和生命周期是很重要并且困难的。先看一段代码。 12345678910counter = 0rdd = sc.parallelize(data)# Wrong: Don't do this!!请使用Accumulatordef increment_counter(x): global counter counter += xrdd.foreach(increment_counter)print(\"Counter value: \", counter) 执行job的时候，Spark会把处理RDD的操作分解为多个任务，每个任务会由一个执行器executor执行。执行前，Spark会计算任务的闭包。闭包其实就是一些变量和方法，为了计算RDD，它们对于执行器是可见的。Spark会把闭包序列化并且发送到每一个执行器。 发送给执行器的闭包里的变量其实是一个副本，这些执行器程序却看不到驱动器程序节点的内存中的变量(counter)，只能看到自己的副本。当foreach函数引用counter的时候，它使用的不是驱动器程序中的counter，而是自己的副本。 本地执行时，有时候foreach函数会在和driver同一个JVM里面执行，那么访问的就是最初的counter，也会对其进行修改。 一般，我们可以使用累加器Accumulator，它可以安全地修改一个变量。闭包不应该修改全局变量。如果要进行全局聚合，则应该使用累加器。 在本地模式，rdd.foreach(println)的时候，会打印出所有的RDD。但是在集群模式的时候，执行器会打印出它自己的那一部分，在driver中并没有。如果要在driver中打印，则需要collect().foreach()，但是只适用于数据量小的情况。因为collect会拿出所有的数据。 键值对RDD 详细的知识参见Spark的键值对RDD。 Shuffle操作 shuffle说明 Shuffle是Spark中重新分布数据的机制，因此它在分区之间分组也不同。主要是复制数据到执行器和机器上，这个很复杂而且很耗费。 以reduceByKey为例，一个key的所有value不一定在同一个partition甚至不在同一个machine，但是却需要把这些values放在一起进行计算。单个任务会在单个分区上执行。为了reduceByKey的reduce任务，需要获得所有的数据。Spark执行一个all-to-all操作，会在所有分区上，查找所有key的所有value，然后跨越分区汇总，去执行reduce任务。这就是shuffle。 shuffle后，分区的顺序和分区里的元素是确定的，但是分区里元素的顺序却不是确定的。可以去设置确定顺序。 性能影响 Shuffle涉及到磁盘IO、数据序列化、网络IO。组织data：一系列map任务；shuffle这些data；聚合data：一系列reduce任务。 一些map的结果会写到内存里，当太大时，会以分区排好序，然后写到单个文件里。在reduce端，task会读取相关的有序的block。 Shuffle操作会占用大量的堆内存，在传输data之前或者之后，都会使用内存中的数据结构去组织这些record。也就是说，在map端，会创建这些structures，在reduce端会生成这些structures。在内存中存不下时，就会写到磁盘中。 Shuffle操作会在磁盘上生成大量的中间文件，并且在RDD不再被使用并且被垃圾回收之前，这些文件都将被一直保留。因为lineage(血统,DAG图)要被重新计算的话，就不会再次shuffle了。如果保留RDD的引用或者垃圾回收不频繁，那么Spark会占用大量的磁盘空间。文件目录可由spark.local.dir配置。 我们可以在Spark的配配置指南中配置各种参数。 RDD持久化 介绍 Spark一个重要的特性是可以在操作的时候持久化缓存RDD到内存中。Persist一个RDD后，每个节点都会将这个RDD计算的所有分区存储在内存中，并且会在后续的计算中进行复用。这可以让future actions快很多(一般是10倍)。缓存是迭代算法和快速交互使用的关键工具。 持久化RDD可以使用persist或cache方法。会先进行行动操作计算，然后缓存到各个节点的内存中。Spark的缓存是fault-tolerant的，如果RDD的某些分区丢失了，它会自动使用产生这个RDD的transformation进行重新计算。 类别 出于不同的目的，持久化可以设置不同的级别。例如可以缓存到磁盘，缓存到内存(以序列化对象存储，节省空间)等，然后会复制到其他节点上。可以对persist传递StorageLevel对象进行设置缓存级别，而cache方法默认的是MEMORY_ONLY，下面是几个常用的。 MEMORY_ONLY(default): RDD作为反序列化的Java对象存储在JVM中。如果not fit in memory，那么一些分区就不会存储，并且会在每次使用的时候重新计算。CPU时间快，但耗内存。 MEMORY_ONLY_SER: RDD作为序列化的Java对象存储在JVM中，每个分区一个字节数组。很省内存，可以选择一个快速的序列化器。CPU计算时间多。只是Java和Scala。 MEMORY_AND_DISK: 反序列化的Java对象存在内存中。如果not fit in memory，那么把不适合在磁盘中存放的分区存放在内存中。 MEMORY_AND_DISK_SER: 和MEMORY_ONLY_SER差不多，只是存不下的再存储到磁盘中，而不是再重新计算。只是Java和Scala。 名字 占用空间 CPU时间 在内存 在磁盘 MEMORY_ONLY 高 低 是 否 MEMORY_ONLY_SER 低 高 是 否 MEMORY_AND_DISK 高 中等 部分 部分 MEMORY_AND_DISK_SER 低 高 部分 部分 所有的类别都通过重新计算丢失的数据来保证容错能力。完整的配置见官方RDD持久化。 在Python中，我们会始终序列化要存储的数据，使用的是Pickle，所以不用担心选择serialized level。 在shuffle中，Spark会自动持久化一些中间结果，即使用户没有使用persist。这样是因为，如果一个节点failed，可以避免重新计算整个input。如果要reuse一个RDD的话，推荐使用persist这个RDD。 选择 Spark的不同storage level是为了在CPU和内存的效率之间不同的权衡，按照如下去选择： 如果适合MEMORY_ONLY，那么就这个。CPU效率最高了。RDD的操作速度会很快！ 如果不适合MEMORY_ONLY，则尽量使用MEMORY_ONLY_SER，然后选个快速序列化库。这样更加节省空间，理论上也能够快速访问。 不要溢写到磁盘。只有这两种才溢写到磁盘：计算数据集非常耗费资源；会过滤掉大量的数据。 如果要快速故障恢复，那么使用复制的storage level。虽然有容错能力，但是复制了，却可以直接继续执行任务，而不需要等待重新计算丢失的分区。 移除数据 Spark会自动监视每个节点上的缓存使用情况，并且以LRU最近最少使用的策略把最老的分区从内存中移除。当然也可以使用rdd.unpersist手动移除。 内存策略：移除分区，再次使用的时候，就需要重新计算。 内存和磁盘策略：移除的分区会写入磁盘。 共享变量 一般，把一个函数f传给Spark的操作，f会在远程集群节点上执行。当函数f在节点上执行的时候，会对所有的变量复制一份副本到该节点，然后利用这些副本单独地工作。对这些副本变量的更新修改不会传回驱动程序，只是修改这些副本。如果要在任务之间支持一般读写共享的变量是很低效的。 Spark支持两种共享变量： 广播变量：用来高效地分发较大的只读对象 累加器：用来对信息进行聚合 广播变量 简介 广播变量可以让程序高效地向所有工作节点发送一个较大的只读值，供一个或多个Spark操作共同使用。 例如较大的只读查询表、机器学习中的一个很大的特征向量，使用广播变量就很方便。这会在每台机器上cache这个变量，而不是发送一个副本。 Spark的Action操作由一组stage组成，由分布式的&quot;shuffle&quot;操作隔离。Spark会自动广播每个stage的tasks需要的common data。这种广播的数据，是以序列化格式缓存的，并且会在每个任务运行之前反序列化。 创建广播变量只有下面两种情况有用： 多个stage的task需要相同的数据 以反序列化形式缓存数据很重要 存在的问题： Spark会自动把闭包中引用到的变量发送到工作节点。方便但是低效。 可能在并行操作中使用同一个变量，但是Spark会为每个操作都发送一次这个变量。 有的变量可能很大，为每个任务都发送一次代价很大。后面再用的话，则还要重新发送。 广播变量来解决： 其实就是一个类型为spark.broadcast.BroadCast[T]的变量。 可以在Task中进行访问。 广播变量只会发送到节点一次，只读。 一种高效地类似BitTorrent的通信机制。 使用方法 对于一个类型为T的对象，使用SparkContext.broadcast创建一个BroadCast[T]。要可以序列化 通过value属性访问值 变量作为只读值会发送到各个节点一次，在自己的节点上修改不会影响到其他变量。 累加器 简介 累加器可以把工作节点中的数据聚合到驱动程序中。类似于reduce，但是更简单。常用作对事件进行计数。累加器仅仅通过关联和交换的操作来实现累加。可以有效地支持并行操作。Spark本身支持数值类型的累加器，我们也可以添加新的类型。 用法 在驱动器程序中，调用SparkContext.accumulator(initialValue)创建一个有初始值的累加器。返回值为org.apache.spark.Accumulator[T] Spark的闭包里的执行器代码可以用累加器的+=来累加。 驱动器程序中，调用累加器的value属性来访问累加器的值 工作节点上的任务不能访问累加器的值 例子 累加空行 123456789101112131415file = sc.textFile(\"callsign_file\")# 创建累加器Accumulator[Int]并且赋初值0blank_line_count = sc.accumulator(0)def extract_callsigns(line): \"\"\"提取callsigns\"\"\" global blank_line_count # 访问全局变量 if line == \"\": blank_line_count += 1 # 累加 return line.split(\" \")callsigns = file.flatMap(extract_callsigns)callsigns.saveAsTextFile(output_dir + \"/callsigns\")# 读取累加器的值 由于惰性求值，只有callsigns的action发生后，才能读取到值print \"Blank lines count: %d\" % blank_line_count.value 进行错误计数 1234567891011121314151617181920212223# 创建用来验证呼号的累加器valid_signcount = sc.accumulator(0)invalid_signcount = sc.accumulator(0)def valid_datesign(sign): global valid_signcount, invalid_sign_count if re.match(r\"\\A\\d?[a-zA-Z]&#123;1,2&#125;\\d&#123;1,4&#125;[a-zA-Z]&#123;1, 3&#125;\\Z\", sign): valid_signcount += 1 return True else: invalid_signcount += 1 return False# 对每个呼号的联系次数进行计数valid_signs = callsigns.filter(valid_datesign)contact_count = valid_signs.map(lambda sign: (sign, 1)).reduceByKey(lambda (x, y): x+y)# 强制求值计算计数contact_count.count()if invalid_signcount.value &lt; 0.1 * valid_signcount.value: contact_count.saveAsTextFile(output_dir + \"/contactcount\")else: print \"Too many errors: %d in %d\" % (invalid_signcount.value, valid_signcount.value) 12345678sign_prefixes = sc.broadcast(load_callsign_table())def process_sign_count(sign_count, sign_prefixes): country = lookup_country(sign_count[0], sign_prefixes.value) count = sign_count[1] return (country, count)country_contack_counts =","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"RDD","slug":"RDD","permalink":"http://plmsmile.github.io/tags/RDD/"}]},{"title":"Spark-SQL","date":"2017-03-19T14:34:44.000Z","path":"2017/03/19/Spark-SQL/","text":"基础 概念 Spark SQL是用来处理结构化数据的模块。与基本RDD相比，Spark SQL提供了更多关于数据结构和计算方面的信息(在内部有优化效果)。通常通过SQL和Dataset API来和Spark SQL进行交互。 SQL: 进行SQL查询，从各种结构化数据源(Json, Hive, Parquet)读取数据。返回Dataset/DataFrame。 Dataset: 分布式的数据集合。 DataFrame 是一个组织有列名的Dataset。类似于关系型数据库中的表。 可以使用结构化数据文件、Hive表、外部数据库、RDD等创建。 在Scala和Java中，DataFrame由Rows和Dataset组成。在Scala中，DataFrame只是Dataset[Row]的类型别名。在Java中，用Dataset表示DataFrame 开始 SparkSession SparkSession是Spark所有功能的入口点，用SparkSession.builder()就可以。 1234567from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL basic example\") \\ .config(\"spark.some.config.option\", \"some-value\") \\ .getOrCreate() DataFrameReader 从外部系统加载数据，返回DataFrame。例如文件系统、键值对等等。通过spark.read来获取Reader。 123456789101112131415# 1. json 键值对df1 = spark.read.json(\"python/test_support/sql/people.json\")df1.dtypes# [('age', 'bigint'), ('name', 'string')]df2 = sc.textFile(\"python/test_support/sql/people.json\")# df1.dtypes 和 df2.dtypes是一样的# 2. text 文本文件 # 每一行就是一个Row，默认的列名是Valuedf = spark.read.text(\"python/test_support/sql/text-test.txt\")df.collect()# [Row(value=u'hello'), Row(value=u'this')]# 3. load# 从数据源中读取数据 创建DataFrames 从RDD、Hive Table、Spark data source、外部文件中都可以创建DataFrames。 通过DataFrameReader，读取外部文件 1234# spark.read返回一个DataFrameReaderdf = spark.read.json(\"examples/src/main/resources/people.json\")df.show()df.dtypes 通过spark.createDataFrame()，读取RDD、List或pandas.DataFrame 12345678910111213141516171819202122232425262728person_list = [('AA', 18), ('PLM', 23)]rdd = sc.parallelize(person_list) # 1. listdf = spark.createDataFrame(person_list) # 没有指定列名，默认为_1 _2df = spark.createDataFrame(person_list, ['name', 'age']) # 指定了列名df.collect() # df.show()#[Row(name='AA', age=18), Row(name='PLM', age=23)]# 2. RDDrdd = sc.parallelize(person_list)df = spark.createDataFrame(rdd, ['name', 'age'])# 3. Rowfrom pyspark.sql import RowPerson = Row('name', 'age') # 指定模板属性persons = rdd.map(lambda r: Person(*r)) # 把每一行变成Persondf = spark.createDataFrame(persons)df.collect()# 4. 指定类型StructTypefrom pyspark.sql.types import *field_name = StructField('name', StringType(), True) # 名，类型，非空field_age = StructField('age', IntegerType, True)person_type = StructType([field_name, field_age])# 通过链式add也可以# person_type = StructType.add(\"name\", StringType(), True).add(...)df = spark.createDataFrame(rdd, person_type) Row Row是DataFrame中的，它可以定义一些属性，这些属性在DataFrame里面可以被访问。比如：row.key(像属性)和row['key'](像dict) 1234567891011121314151617181920from pyspark import Row# 1. 创建一个模板Person = Row('name', 'age') # &lt;Row(name, age)&gt;'name' in Person # True，有这个属性'sex' in Person # False# 2. 以Person为模板，创建alice, plmalice = Person('Alice', 22) # Row(name='Alice', age=22)plm = Person('PLM', 23)# 访问属性name, age = alice['name'], alice['age']# 返回dictplm.asDict()# &#123;'age': 23, 'name': 'PLM'&#125;# 3. 多个person创建一个DataFramep_list = [alice, plm]p_df = spark.createDataFrame(p_list)p_df.collect()# [Row(name=u'Alice', age=22), Row(name=u'PLM', age=23)] DataFrame的操作 在2.0中，DataFrames只是Scala和Java API中的Rows数据集。它的操作称为非类型转换，与带有强类型Scala和Java数据集的类型转换相反。 Python tips: df.age和df['age']都可以使用，前者在命令行里面方便，但是建议使用后者。 12345df.printSchema()df.select(\"name\").show()df.select(df['name'], df['age'] + 1).show()df.filter(df['age'] &gt; 21).show()df.groupBy(\"age\").count().show() DataFrame的Python Api，DataFrame的函数API 编程方式运行SQL 通过spark.sql()执行，返回一个DataFrame 12345678910111213# 通过df创建一个本地临时视图，与创建这个df的SparkSession同生命周期df.createOrReplaceTempview(\"people\")sqlDF = spark.sql(\"SELECT * FROM people\")sqlDF.show()# 展示为TablesqlDf.collect()# [Row(age=None, name=u'Michael'), Row(age=30, name=u'Andy'), Row(age=19, name=u'Justin')]# 全局临时视图# 在所有session中共享，直到spark application停止df.createGlobalTempView(\"people\")spark.sql(\"SELECT * FROM global_temp.people\").show()spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"Spark SQL","slug":"Spark-SQL","permalink":"http://plmsmile.github.io/tags/Spark-SQL/"}]},{"title":"博客搭建过程及问题","date":"2017-03-13T14:52:41.000Z","path":"2017/03/13/博客搭建过程及问题/","text":"一直都想搭建一个博客，今天终于把博客给初步搭建好了。搭建的过程其实不那么顺利，所以简答记录一下。 搭建过程 根据手把手搭建博客教程这篇文章来进行搭建，其中目前只看了它的第一页 后期在hexo主题里面选择了even主题 依照even-wiki来添加标签、分类和about页面 修改主题目录下的_config.yml的menu，把home、tags等手动改成中文了 遇到的坑 没有看wiki，自己去谷歌建立tags、categories等页面 建立好tags，却在tags页面没有看到相应的标签。是因为没有为tags/index.md设置layout为tags 中文语言，在站点目录下的_config.yml中设置language: zh-cn。 博客重新搭建 配置及源文件 因为经常重装系统，所以博客也需要重新恢复。先配置git相关信息 1234567git config --global user.name \"plmsmile\"git config --global user.email \"pulimingspark@163.com\"ssh-keygen -t rsa -C \"pulimingspark@163.com\"# 去GitHub上添加sshkeycat ~/.ssh/id_rsa.pub# 完成后，进行测试ssh -T git@github.com 然后把之前的PLMBlogs拷贝到D盘，一般目录是d/PLMBlogs 1234567cd PLMBlogs# 安装hexonpm install hexo-cli -g# 安装插件npm install hexo-deployer-git --save# 安装依赖npm install 数学公式渲染 执行完上面的操作后，执行如下 1234hexo cleanhexo generate # 这一步会报错 hexo serverhexo deploy 错误信息如下 123456789101112131415161718192021222324252627282930313233FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: write EPIPE at exports._errnoException (util.js:1020:11) at Socket._writeGeneric (net.js:711:26) at Socket._write (net.js:730:8) at doWrite (_stream_writable.js:331:12) at writeOrBuffer (_stream_writable.js:317:5) at Socket.Writable.write (_stream_writable.js:243:11) at Socket.write (net.js:657:40) at Hexo.pandocRenderer (D:\\PLMBlogs\\node_modules\\hexo-renderer-pandoc\\index.js:64:15) at Hexo.tryCatcher (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\util.js:16:23) at Hexo.ret (eval at makeNodePromisifiedEval (C:\\Users\\PLM\\AppData\\Roaming\\npm\\node_modules\\hexo-cli\\node_modules\\bluebird\\js\\release\\promisify.js:184:12), &lt;anonymous&gt;:13:39) at D:\\PLMBlogs\\node_modules\\hexo\\lib\\hexo\\render.js:61:21 at tryCatcher (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\util.js:16:23) at Promise._settlePromiseFromHandler (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:512:31) at Promise._settlePromise (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:569:18) at Promise._settlePromiseCtx (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:606:10) at Async._drainQueue (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:138:12) at Async._drainQueues (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:143:10) at Immediate.Async.drainQueues (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:17:14) at runCallback (timers.js:672:20) at tryOnImmediate (timers.js:645:5) at processImmediate [as _immediateCallback] (timers.js:617:5)events.js:160 throw er; // Unhandled 'error' event ^Error: spawn pandoc ENOENT at exports._errnoException (util.js:1020:11) at Process.ChildProcess._handle.onexit (internal/child_process.js:193:32) at onErrorNT (internal/child_process.js:367:16) at _combinedTickCallback (internal/process/next_tick.js:80:11) at process._tickCallback (internal/process/next_tick.js:104:9) 原因是有大量的数学公式，所以需要对网页进行渲染。 一般是使用pandoc进行渲染，先去官网下载，然后下一步安装即可。最后，执行下面的命令，安装就好了。 123npm install hexo-renderer-pandoc --save# 再次应该就不会报错了hexo generate 配置git 1234567git config --global user.name plmsmilegit config --global user.email plmspark@163.com# 生成Key，一路回车ssh-keygen -t rsa -C \"plmspark@163.com\" cat ~/.ssh/id_rsa.pub# 测试ssh -T git@github.com 潜在问题 本站没有搜索功能，安装插件失败了 tags页面，标签数量错误 期望 认真学习 好好做笔记 要更新博客 自己常来看看之前的知识点","tags":[{"name":"心得","slug":"心得","permalink":"http://plmsmile.github.io/tags/心得/"}]},{"title":"Spark的键值对RDD","date":"2017-03-13T11:37:06.000Z","path":"2017/03/13/Spark-PairRDD/","text":"PairRDD及其创建 键值对RDD称为PairRDD，通常用来做聚合计算。Spark为Pair RDD提供了许多专有的操作。 1234# 创建pair rdd: map 或者 读取键值对格式自动转成pairrdd# 每行的第一个单词作为key，line作为valuepairs = lines.map(lambda line: (line.split(' ')[0], line)) 转化操作 Pair RDD 的转化操作分为单个和多个RDD的转化操作。 单个Pair RDD转化 reduceByKey(func) 合并含有相同键的值，也称作聚合 123456from operator import addrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])sorted(rdd.reduceByKey(add).collect())# [('a', 2), ('b', 1)]# 这种写法也可以rdd.reduceByKey(lambda x, y: x+y).collect() groupByKey 对具有相同键的值进行分组。会生成hash分区的RDD 12345rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])sorted(rdd.groupByKey().mapValues(len).collect())# [('a', 2), ('b', 1)]sorted(rdd.groupByKey().mapValues(list).collect())[('a', [1, 1]), ('b', [1])] 说明：如果对键进行分组以便对每个键进行聚合（如sum和average），则用reduceByKey和aggregateByKey性能更好 combineByKey 合并具有相同键的值，但是返回不同类型 (K, V) - (K, C)。最常用的聚合操作。 1234x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])def add(a, b): return a + str(b)sorted(x.combineByKey(str, add, add).collect())[('a', '11'), ('b', '1')] 下面是combineByKey的源码和参数说明 123456789101112131415161718def combineByKey[C]( createCombiner: V =&gt; C, // V =&gt; C的转变 / 初始值 / 创建one-element的list mergeValue: (C, V) =&gt; C, // 将原V累加到新的C mergeCombiners: (C, C) =&gt; C, // 两个C合并成一个 partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)] = &#123; //实现略 &#125;// 求平均值val scores = sc.parallelize( List((\"chinese\", 88.0) , (\"chinese\", 90.5) , (\"math\", 60.0), (\"math\", 87.0)))val avg = scores.combineByKey( (v) =&gt; (v, 1), (acc: (Float, Int), V) =&gt; (acc._1 + v, acc._2 + 1), (acc1: (Float, Int), acc2: (Float, Int) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))).map&#123;case (key, value) =&gt; (key, value._1 / value._2.toFloat)&#125; 1234567891011# 求平均值nums = sc.parallelize([('c', 90), ('m', 95), ('c', 80)])sum_count = nums.combineByKey( lambda x: (x, 1), lambda acc, x: (acc[0] + x, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))# [('c', (170, 2)), ('m', (95, 1))]avg_map = sum_count.mapValues(lambda (sum, count): sum/count).collectAsMap()# &#123;'c': 85, 'm': 95&#125;avg_map = sum_count.map(lambda key, s_c: (key, s_c[0]/s_c[1])).collectAsMap() mapValues(f) 对每个pair RDD中的每个Value应用一个func，不改变Key。其实也是对value做map操作。一般我们只想访问pair的值的时候，可以用mapValues。类似于map{case (x, y): (x, func(y))} 1234x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])def f(x): return len(x)x.mapValues(f).collect()[('a', 3), ('b', 1)] mapPartitions(f) 是map的一个变种，都需要传入一个函数f，去处理数据。不同点如下： map: f应用于每一个元素。 mapPartitions: f应用于每一个分区。分区的内容以Iterator[T]传入f，f的输出结果是Iterator[U]。最终RDD的由所有分区经过输入函数处理后的结果得到的。 优点：我们可以为每一个分区做一些初始化操作，而不用为每一个元素做初始化。例如，初始化数据库，次数n。map时：n=元素数量，mapPartitions时：n=分区数量。 1234567891011121314151617181920212223# 1. 每个元素加1def f(iterator): print (\"called f\") return map(lambda x: x + 1, iterator)rdd = sc.parallelize([1, 2, 3, 4, 5], 2)rdd.mapPartitions(f).collect() # 只调用2次f\"\"\"called fcalled f[2, 3, 4, 5, 6]\"\"\"# 2. 分区求和rdd = sc.parallelize([1, 2, 3, 4, 5, 6], 2)def f(iterator): print \"call f\" yield sum(iterator)rdd.mapPartitions(f).collect() # 调用2次f，分区求和\"\"\"call fcall f[6, 15]\"\"\" mapPartitionsWithIndex(f) 和mapPartitions一样，只是多了个partition的index。 12345678910111213rdd = sc.parallelize([\"yellow\",\"red\",\"blue\",\"cyan\",\"black\"], 3)def g(index, item): return \"id-&#123;&#125;, &#123;&#125;\".format(index, item)def f(index, iterator): print 'called f' return map(lambda x: g(index, x), iterator)rdd.mapPartitionsWithIndex(f).collect()\"\"\"called fcalled fcalled f['id-0, yellow', 'id-1, red', 'id-1, blue', 'id-2, cyan', 'id-2, black']\"\"\" repartition(n) 生成新的RDD，分区数目为n。会增加或者减少 RDD的并行度。会对分布式数据集进行shuffle操作，效率低。如果只是想减少分区数，则使用coalesce，不会进行shuffle操作。 1234567&gt;&gt;&gt; rdd = sc.parallelize([1,2,3,4,5,6,7], 4)&gt;&gt;&gt; sorted(rdd.glom().collect())[[1], [2, 3], [4, 5], [6, 7]]&gt;&gt;&gt; len(rdd.repartition(2).glom().collect())2&gt;&gt;&gt; len(rdd.repartition(10).glom().collect())10 coalesce(n) 合并，减少分区数，默认不执行shuffle操作。 1234sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()# [[1], [2, 3], [4, 5]]sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()# [[1, 2, 3, 4, 5]] flatMapValues(f) 打平values，[(&quot;k&quot;, [&quot;v1&quot;, &quot;v2&quot;])] -- [(&quot;k&quot;,&quot;v1&quot;), (&quot;k&quot;, &quot;v2&quot;)] 1234x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])def f(x): return xx.flatMapValues(f).collect()# [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')] keys values sortByKey 返回一个对键进行排序的RDD。会生成范围分区的RDD 123456789101112tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]sc.parallelize(tmp).sortByKey().first()# ('1', 3)sc.parallelize(tmp).sortByKey(True, 1).collect()# [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]sc.parallelize(tmp).sortByKey(True, 2).collect()# [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()# [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)] 两个Pair RDD转化 substract 留下在x中却不在y中的元素 1234x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])y = sc.parallelize([(\"a\", 3), (\"c\", None)])sorted(x.subtract(y).collect())#[('b', 4), ('b', 5)] substractByKey 删掉X中在Y中也存在的Key所包含的所有元素 join 内连接，从x中去和y中一个一个的匹配，(k, v1), (k, v2) -- (k, (v1, v2)) 1234x = sc.parallelize([(\"a\", 1), (\"b\", 4)])y = sc.parallelize([(\"a\", 2), (\"a\", 3)])sorted(x.join(y).collect())# [('a', (1, 2)), ('a', (1, 3))] leftOuterJoin 左边RDD的键都有，右边没有的配None 1234x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])sorted(x.leftOuterJoin(y).collect())# [('a', (1, 2)), ('b', (4, None))] rightOuterJoin 右边RDD的键都有，左边没有的配None 123456x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])sorted(x.rightOuterJoin(y).collect())# [('a', (1, 2))]sorted(y.rightOuterJoin(x).collect())# [('a', (2, 1)), ('b', (None, 4))] cogroup 将两个RDD中拥有相同键的value分组到一起，即使两个RDD的V不一样 123456x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])x.cogroup(y).collect()# 上面显示的是16进制[(x, tuple(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]# [('a', ([1], [2])), ('b', ([4], []))] 行动操作 countByKey 对每个键对应的元素分别计数 123rdd = sc.parallelize([('a', 1), ('b', 1), ('a', 1)])rdd.countByKey().items() # 转换成一个dict，再取所有元素# [('a', 2), ('b', 1)] collectAsMap 返回一个map 123m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()m[1] - 2 # 当做map操作即可m[3] - 4 lookup(key) 返回键的RDD中的值列表。如果RDD具有已知的分区器，则通过仅搜索key映射到的分区来高效地完成该操作。 12345678910l = range(1000) # 1,2,3,...,1000rdd = sc.parallelize(zip(l, l), 10) # 键和值一样，10个数据分片，10个并行度，10个taskrdd.lookup(42) # slow# [42]sorted_rdd = rdd.sortByKey()sorted_rdd.lookup(42) # fast# [42]rdd = sc.parallelize([('a', 'a1'), ('a', 'a2'), ('b', 'b1')])rdd.lookup('a')[0]# 'a1' 聚合操作 当数据是键值对组织的时候，聚合具有相同键的元素是很常见的操作。基础RDD有fold(), combine(), reduce()，Pair RDD有combineByKey()最常用,reduceByKey(), foldByKey()等。 计算均值 1234567891011121314151617181920## 方法一：mapValues和reduceByKeyrdd = sc.parallelize([('a', 1), ('a', 3), ('b', 4)])maprdd = rdd.mapValues(lambda x : (x, 1))# [('a', (1, 1)), ('a', (3, 1)), ('b', (4, 1))]reducerdd = maprdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))# [('a', (4, 2)), ('b', (4, 1))]reducerdd.mapValues(lambda x : x[0]/x[1]).collect()# [('a', 2), ('b', 4)] ## 方法二 combineByKey 最常用的nums = sc.parallelize([('c', 90), ('m', 95), ('c', 80)])sum_count = nums.combineByKey( lambda x: (x, 1), lambda acc, x: (acc[0] + x, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))# [('c', (170, 2)), ('m', (95, 1))]avg_map = sum_count.mapValues(lambda (sum, count): sum/count).collectAsMap()# &#123;'c': 85, 'm': 95&#125;avg_map = sum_count.map(lambda key, s_c: (key, s_c[0]/s_c[1])).collectAsMap() 统计单词计数 1234567rdd = sc.textFile('README.md')words = rdd.flatMap(lambda x: x.split(' '))# 568个result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)# 289result.top(2)# [('your', 1), ('you', 4)] 数据分区 分区说明 在分布式程序中，通信的代价是很大的。因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。Spark程序通过控制RDD分区方式来减少通信开销。使用partitionBy进行分区 不需分区：给定RDD只需要被扫描一次 需要分区：数据集在多次基于键的操作中使用，比如连接操作。partitionBy是转化操作，生成新的RDD，为了多次计算，一般要进行持久化persist Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。Spark不能显示控制具体每个键落在哪一个工作节点上，但是Spark可以确保同一组的键出现在同一个节点上。 Hash分区：将一个RDD分成了100个分区，hashcode(key)%100 相同的，会在同一个节点上面 范围分区：将key在同一个范围区间内的记录放在同一个节点上 一个简单的例子，内存中有一张很大的用户信息表 -- 即(UserId, UserInfo)组成的RDD，UserInfo包含用户订阅了的所有Topics。还有一张(UserId, LinkInfo)存放着过去5分钟用户浏览的Topic。现在要找出用户浏览了但是没有订阅的Topic数量。 123456789101112val sc = new SparkContext(...)val userData = sc.sequenceFile[UserId, UserInfo](\"hdfs://...\").persist()def processNewLog(logFileName: String) &#123; val events = sc.sequenceFile[UserId, LinkInfo](logFileName) val joined = userData.join(events) // (UserId, (UserInfo, LinkInfo)) val offTopicVisits = joined.filter&#123; case (UserId, (UserInfo, LinkInfo)) =&gt; !UserInfo.topics.contains(LinkInfo.topic) &#125;.count() print (\"浏览了且未订阅的数量：\" + offTopicVisits)&#125; 这段代码不够高效。因为每次调用processNewLog都会用join操作，但我们却不知道数据集是如何分区的。 连接操作，会将两个数据集的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在机器上对所有键相同的记录进行操作。 因为userData比events要大的多并且基本不会变化，所以有很多浪费效率的事情：每次调用时都对userData表进行计算hash值计算和跨节点数据混洗。 解决方案：在程序开始的时候，对userData表使用partitionBy()转换操作，将这张表转换为哈希分区 123val userData = sc.sequenceFile[UserId, UserInfo](\"hdfs://...\") .partitionBy(new HashPartioner(100)) // 构造100个分区 .persist() // 持久化当前结果 events是本地变量，并且只使用一次，所以为它指定分区方式没有什么用处。 userData使用了partitionBy()，Spark就知道该RDD是根据键的hash值来分区的。在userData.join(events)时，Spark只会对events进行数据混洗操作。将events中特定UserId的记录发送到userData的对应分区所在的那台机器上。需要网络传输的数据就大大减少了，速度也就显著提升了。 分区相关的操作 Spark的许多操作都有将数据根据跨节点进行混洗的过程。所有这些操作都会从数据分区中获益。类似join这样的二元操作，预先进行数据分区会导致其中至少一个RDD不发生数据混洗。 获取好处的操作：cogroup, groupWith, join, leftOuterJoin , rightOuterJoin, groupByKey, reduceByKey , combineByKey, lookup 为结果设好分区的操作：cogroup, groupWith, join, leftOuterJoin , rightOuterJoin, groupByKey, reduceByKey , combineByKey, partitionBy, sort, （mapValues, flatMapValues, filter 如果父RDD有分区方式的话） 其他所有的操作的结果都不会存在特定的分区方式。对于二元操作，输出数据的分区方式取决于父RDD的分区方式。默认情况结果会采取hash分区。 PageRank PageRank的python版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\" PageRank算法author = PuLiming运行: bin/spark-submit files/pagerank.py data/mllib/pagerank_data.txt 10\"\"\"from __future__ import print_functionimport reimport sysfrom operator import addfrom pyspark import SparkConf, SparkContextdef compute_contribs(urls, rank): \"\"\" 给urls计算 Args: urls: 目标url相邻的urls集合 rank: 目标url的当前rank Returns: url: 相邻urls中的一个url rank: 当前url的新的rank \"\"\" num_urls = len(urls) for url in urls: yield (url, rank / num_urls)def split_url(url_line): \"\"\" 把一行url切分开来 Args: url_line: 一行url，如 1 2 Returns: url, neighbor_url \"\"\" parts = re.split(r'\\s+', url_line) # 正则 return parts[0], parts[1]def compute_pagerank(sc, url_data_file, iterations): \"\"\" 计算各个page的排名 Args: sc: SparkContext url_data_file: 测试数据文件 iterations: 迭代次数 Returns: status: 成功就返回0 \"\"\" # 读取url文件 ['1 2', '1 3', '2 1', '3 1'] lines = sc.textFile(url_data_file).map(lambda line: line.encode('utf8')) # 建立Pair RDD (url, neighbor_urls) [(1,[2,3]), (2,[1]), (3, [1])] links = lines.map(lambda line : split_url(line)).distinct().groupByKey().mapValues(lambda x: list(x)).cache() # 初始化所有url的rank为1 [(1, 1), (2, 1), (3, 1)] ranks = lines.map(lambda line : (line[0], 1)) for i in range(iterations): # (url, [(neighbor_urls), rank]) join neighbor_urls and rank # 把当前url的rank分别contribute到其他相邻的url (url, rank) contribs = links.join(ranks).flatMap( lambda url_urls_rank: compute_contribs(url_urls_rank[1][0], url_urls_rank[1][1]) ) # 把url的所有rank加起来，再赋值新的 ranks = contribs.reduceByKey(add).mapValues(lambda rank : rank * 0.85 + 0.15) for (link, rank) in ranks.collect(): print(\"%s has rank %s.\" % (link, rank)) return 0if __name__ == '__main__': if len(sys.argv) != 3: print(\"Usage: python pagerank.py &lt;data.txt&gt; &lt;iterations&gt;\", file = sys.stderr) sys.exit(-1) # 数据文件和迭代次数 url_data_file = sys.argv[1] iterations = int(sys.argv[2]) # 配置 SparkContext conf = SparkConf().setAppName('PythonPageRank') conf.setMaster('local') sc = SparkContext(conf=conf) ret = compute_pagerank(sc, url_data_file, iterations) sys.exit(ret) PageRank的scala版本 12345678910111213141516val sc = new SparkContext(...)val links = sc.objectFile[(String, Seq[String])](\"links\") .partitionBy(new HashPartitioner(100)) .persist()var ranks = links.mapValues(_ =&gt; 1.0)// 迭代10次for (i &lt;- 0 until 10) &#123; val contributions = links.join(ranks).flatMap &#123; case (pageId, (links, rank)) =&gt; links.map(dest =&gt; (dest, rank / links.size)) &#125; ranks = contributions.reduceByKey(_ + _).mapValues(0.15 + 0.85* _)&#125;ranks.saveAsTextFile(\"ranks\") 当前scala版本的PageRank算法的优点： links每次都会和ranks发生连接操作，所以一开始就对它进行分区partitionBy，就不会通过网络进行数据混洗了，节约了相当可观的网络通信开销 对links进行persist，留在内存中，每次迭代使用 第一次创建ranks，使用mapValues保留了父RDD的分区方式，第一次连接开销就会很小 reduceByKey后已经是分区了，再使用mapValues分区方式，再次和links进行join就会更加高效 所以对分区后的RDD尽量使用mapValues保留父分区方式，而不要用map丢失分区方式。","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"PairRDD","slug":"PairRDD","permalink":"http://plmsmile.github.io/tags/PairRDD/"}]},{"title":"Spark的基础RDD","date":"2017-03-13T11:26:18.000Z","path":"2017/03/13/Spark-BaseRDD/","text":"RDD基础 RDD是Spark中的核心抽象——弹性分布式数据集(Resilient Distributed Dataset)。其实RDD是分布式的元素集合，是一个不可变的分布式对象集合。每个RDD都会被分为多个分区，这些分区运行在不同的节点上。RDD可以包含任意对象。Spark会自动将这些RDD的数据分发到集群上，并将操作并行化执行。 RDD当做我们通过转化操作构建出来的、记录如何计算数据的指令列表。 对数据的所有操作都是创建RDD、转化已有RDD、调用RDD操作进行求值。 RDD主要有2个操作。 转化操作：由一个RDD生成一个新的RDD。惰性求值。 行动操作：会对RDD计算出一个结果或者写到外部系统。会触发实际的计算。 Spark会惰性计算这些RDD，只有第一次在一个行动操作中用到时才会真正计算。 Spark的RDD会在每次行动操作时重新计算。如果想复用，则用persist把RDD持久化缓存下来。 下面是总的大体步骤 从外部数据创建输入RDD。如textFile 使用转化操作得到新的RDD。如map，filter 对重用的中间结果RDD进行持久化。如persist 使用行动操作来触发一次并行计算。如count, first 创建RDD 创建RDD主要有两个方法：读取集合，读取外部数据。 1234# 读取集合words = sc.parallelize([\"hello\", \"spark\", \"good\", \"study\"])# 读取外部数据lines = sc.textFile(\"README.md\") 转化操作 RDD的转化操作会返回新的RDD，是惰性求值的。即只有真正调用这些RDD的行动操作这些RDD才会被计算。许多转化操作是针对各个元素的，即每次只会操作RDD中的一个元素。 通过转化操作，会从已有RDD派生出新的RDD。Spark会使用谱系图(lineage graph)来记录这些不同RDD之间的依赖关系。Spark会利用这些关系按需计算每个RDD，或者恢复所丢失的数据。 最常用的转化操作是map()和filter()。下面说明一下常用的转化操作。 map(f) 对每个元素使用func函数，将返回值构成新的RDD。不会保留父RDD的分区。 1234567rdd = sc.parallelize([\"b\", \"a\", \"c\"])rddnew = rdd.map(lambda x: (x, 1))# [('b', 1), ('a', 1), ('c', 1)]# 可使用sorted()进行排序sorted(rddnew.collect())# [('a', 1), ('b', 1), ('c', 1)] flatMap(f) 对每个元素使用func函数，然后展平结果。通常用来切分单词 1234567lines = sc.textFile(\"README.md\")# 104个words = lines.flatMap(lambda line : line.split(\" \"))# 568个rdd = sc.parallelize([2, 3, 4])rdd2 = rdd.flatMap(lambda x: range(1, x)) # range(1, x) 生成1-x的数，不包括x# [1, 1, 2, 1, 2, 3] filter(f) 元素满足f函数，则放到新的RDD里 123rdd = sc.parallelize([1, 2, 3, 4, 5])rdd.filter(lambda x: x % 2 == 0).collect()# [2, 4] distinct 去重。开销很大，会进行数据混洗。 12sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())# [1, 2, 3] union 合并两个RDD。会包含重复的元素 intersection 求两个RDD的共同的元素。会去掉重复的元素 subtract 留下在自己却不在other里面的元素 cartesian 两个RDD的笛卡尔积 行动操作 行动操作会把最终求得的结果返回到驱动程序，或者写入外部存储系统中。 collect 返回RDD中的所有元素。只适用于数据小的情况，因为会把所有数据加载到驱动程序的内存中。通常只在单元测试中使用 count RDD中元素的个数 countByValue 各元素在RDD中出现的次数，返回一个dictionary。在pair RDD中有countByKey方法 12sc.parallelize([1, 2, 1, 2, 2]).countByValue().items()# [(1, 2), (2, 3)] take(num) 返回RDD中的n个元素。它会首先扫描一个分区，在这个分区里面尽量满足n个元素，不够再去查别的分区。只能用于数据量小的情况下。得到的顺序可能和你预期的不一样 takeOrdered(num, key=None) 获取n个元素，按照升序或者按照传入的key function。只适用于数据小的RDD 1234sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)# [1, 2, 3, 4, 5, 6]sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)# [10, 9, 7, 6, 5, 4] top(num, key=None) 从RDD只获取前N个元素。降序排列。只适用于数据量小的RDD 1234sc.parallelize([2, 3, 4, 5, 6]).top(2)# [6, 5]sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)# [4, 3, 2] reduce(f) 并行整合RDD中的所有数据，得到一个结果。接收一个f函数。目前在本地reduce partitions。返回结果类型不变。 1234from operator import addsc.parallelize([1, 2, 3, 4, 5]).reduce(add)sc.parallelize([1, 2, 3, 4, 5]).reduce(lambda x, y: x+y)# 15 fold(zeroValue, op) 和reduce一样，但是需要提供初始值。op(t1, t2)，t1可以变，t2不能变 123from operator import addsc.parallelize([1, 2, 3, 4, 5]).fold(0, add)# 15 aggregate(zeroValue, seqOp, combOp) 聚合所有分区的元素，然后得到一个结果。和reduce相似，但是通常返回不同的类型。 1234seqOp = (lambda x, y: (x[0] + y, x[1] + 1)) # 累加combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1])) # combine多个sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)# (10, 4) foreach(f) 对rdd中的每个元素使用f函数 123def f(x): print (x)sc.parallelize([1, 2, 3, 4]).foreach(f) glom 将分区中的元素合并到一个list 123rdd = sc.parallelize([1, 2, 3, 4, 5], 2)rdd.glom().collect()# [[1, 2], [3, 4, 5]]","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"RDD","slug":"RDD","permalink":"http://plmsmile.github.io/tags/RDD/"}]},{"title":"HDFS初步学习","date":"2016-12-05T11:59:45.000Z","path":"2016/12/05/HDFS初步学习/","text":"HDFS是Hadoop的一个分布式文件系统 HDFS设计原理 HDFS设计原理 提供了一个抽象访问界面，通过路径访问文件 抽象界面上所展示的文件，存储在很多个服务器上面 抽象路径与实际存储的映射关系，由元数据管理系统来进行管理 为了数据的安全性，数据被存成多个副本 为了负载均衡和吞吐量，这些文件被分隔成为若干个块 元数据存储细节 职责及存储格式 响应客户端请求，维护hdfs目录树 管理元数据，维护映射（HDFS上的文件 --- Blocks --- DataNode fileName, replicas, blockIds, idToHosts /test/a.txt, 3, {blk_1, blk_2}, [{blk1 : [h0, h1, h2]}, {blk2 : [{h1, h2}]}] 元数据存储信息 Meta.data 内存中的元数据 Meta.edits 元数据最新的修改信息，存在磁盘上 Meta.data.image","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://plmsmile.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://plmsmile.github.io/tags/HDFS/"}]}]